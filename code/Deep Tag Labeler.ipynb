{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(\n",
    "    tf.test.is_gpu_available(\n",
    "        cuda_only=False,\n",
    "        min_cuda_compute_capability=None\n",
    "    )\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    tf.compat.v2.config.experimental.list_physical_devices('GPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000 and 10,000 instances that reduced the maximum imbalance from 100-1000:1 to 8:1 using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_tags = json.load(open('data/stackoverflow/08-05-2019/sorted_all_tags.50000.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>label_20</th>\n",
       "      <th>label_21</th>\n",
       "      <th>label_22</th>\n",
       "      <th>label_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C, Mono, Winforms, MessageBox, problem, I, fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Are, NET, data, providers, Oracle, require, O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[How, I, focus, foreign, window, I, applicatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Default, button, hit, windows, forms, trying,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Can, I, avoid, JIT, net, Say, code, always, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  label_0  label_1  \\\n",
       "0  [C, Mono, Winforms, MessageBox, problem, I, fi...        1        0   \n",
       "1  [Are, NET, data, providers, Oracle, require, O...        1        0   \n",
       "2  [How, I, focus, foreign, window, I, applicatio...        1        0   \n",
       "3  [Default, button, hit, windows, forms, trying,...        1        0   \n",
       "4  [Can, I, avoid, JIT, net, Say, code, always, g...        1        0   \n",
       "\n",
       "   label_2  label_3  label_4  label_5  label_6  label_7  label_8  ...  \\\n",
       "0        0        0        0        1        0        0        0  ...   \n",
       "1        0        0        0        0        0        0        0  ...   \n",
       "2        0        0        0        1        0        0        0  ...   \n",
       "3        0        0        0        1        0        0        0  ...   \n",
       "4        0        0        0        0        0        0        0  ...   \n",
       "\n",
       "   label_14  label_15  label_16  label_17  label_18  label_19  label_20  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   label_21  label_22  label_23  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         0         0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/08-05-2019/Questions.Stratified.Final.50000.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,293,018\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,}'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open('data/stackoverflow/08-05-2019/tag_index.50000.json'))\n",
    "index_tag = json.load(open('data/stackoverflow/08-05-2019/index_tag.50000.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = json.load(open('data/stackoverflow/08-05-2019/label_counts.50000.json'))\n",
    "\n",
    "# Sanity check the difference files\n",
    "assert(len(label_counts.keys()) == len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 404 Training Count: 1,292,800\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "# Remove stopwords - now done in Spark, so can remove once that runs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [' '.join(x) for x in posts_text]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "# elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())\n",
    "\n",
    "# def ElmoEmbedding(x):\n",
    "#     return elmo_model(inputs={\n",
    "#                             \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "#                             \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "#                       },\n",
    "#                       signature=\"tokens\",\n",
    "#                       as_dict=True)[\"elmo\"]\n",
    "\n",
    "# text_input = Input(shape=(max_len,), dtype=tf.string)\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Glove Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1292800, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKEN_COUNT)\n",
    "tokenizer.fit_on_texts(documents)\n",
    "# encoded_docs = tokenizer.texts_to_matrix(posts_text, mode='tfidf')\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='pre',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "print(max([len(x) for x in padded_sequences]), min([len(x) for x in padded_sequences]))\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]))\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  71, 3402, 2170, ...,    1,    1,    1],\n",
       "       [  54,  170,   34, ...,    1,    1,    1],\n",
       "       [  21,    2, 1478, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [   2,   13,   35, ...,  956,    1,    1],\n",
       "       [ 119,  462,   32, ...,    1,    1,    1],\n",
       "       [   2,   25,   44, ...,    1,    1,    1]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "# print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     posts_text,\n",
    "#     labels,\n",
    "#     test_size=TEST_SPLIT,\n",
    "#     random_state=1337\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "# from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# # Text model\n",
    "# text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "# text_lstm = LSTM(\n",
    "#     input_shape=(MAX_LEN, 1024,),\n",
    "#     units=512,\n",
    "#     recurrent_dropout=0.2,\n",
    "#     dropout=0.2)(elmo_embedding)\n",
    "\n",
    "# text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "# text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "\n",
    "# text_model = Model(\n",
    "#     inputs=text_input, \n",
    "#     outputs=text_output\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.0005)\n",
    "\n",
    "# text_model.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer=adam,\n",
    "#     metrics=[\n",
    "#         precision_m,\n",
    "#         recall_m,\n",
    "#         f1_m,\n",
    "#         'mae',\n",
    "#         abs_KL_div,\n",
    "#         'accuracy'\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample and Class Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample or class weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.35704997e-03, 4.91477790e-06, 3.46168426e-06, ...,\n",
       "        4.43082862e-06, 1.30453968e-07, 1.30453968e-07]),\n",
       " array([1.30316677e-07, 1.15411547e-04, 1.30316677e-07, ...,\n",
       "        1.30316677e-07, 1.38388975e-04, 1.79070271e-06]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test)\n",
    "\n",
    "train_sample_weights, test_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(16, 1.0),\n",
       "  (5, 1.174390280129519),\n",
       "  (12, 1.3624019987938314),\n",
       "  (17, 1.620059420141379),\n",
       "  (20, 1.8687102644702323),\n",
       "  (10, 1.8711868417938706),\n",
       "  (8, 2.009173379411989),\n",
       "  (9, 2.0102461100376283),\n",
       "  (7, 2.147567699703941),\n",
       "  (22, 2.1867688137843295),\n",
       "  (13, 2.188281855419019),\n",
       "  (14, 2.292860457023547),\n",
       "  (11, 2.338430143144446),\n",
       "  (1, 2.3760254830663823),\n",
       "  (23, 2.4299895506792057),\n",
       "  (6, 2.6281203257437262),\n",
       "  (21, 2.647214410070979),\n",
       "  (2, 2.666767850517724),\n",
       "  (15, 2.9316648127549128),\n",
       "  (0, 3.058921386567626),\n",
       "  (3, 3.0622385747482572),\n",
       "  (18, 3.2606293043012085),\n",
       "  (4, 3.4826674888781217),\n",
       "  (19, 4.0100928133083125)],\n",
       " [(16, 1.0),\n",
       "  (5, 1.1816222042576126),\n",
       "  (12, 1.3517262638717633),\n",
       "  (17, 1.564116283217407),\n",
       "  (10, 1.8412765064035272),\n",
       "  (20, 1.8823781927452243),\n",
       "  (9, 1.9712294897729827),\n",
       "  (8, 1.9954493742889647),\n",
       "  (13, 2.1750992063492065),\n",
       "  (7, 2.1821348594177654),\n",
       "  (22, 2.1897627965043696),\n",
       "  (14, 2.294610151753009),\n",
       "  (23, 2.370911057042444),\n",
       "  (1, 2.379918588873813),\n",
       "  (11, 2.3805646036916395),\n",
       "  (6, 2.5931401537551744),\n",
       "  (21, 2.688534641324341),\n",
       "  (2, 2.7151702786377707),\n",
       "  (15, 3.0199724517906334),\n",
       "  (0, 3.102228510788822),\n",
       "  (3, 3.1867732558139537),\n",
       "  (18, 3.2254505332842958),\n",
       "  (4, 3.428459734167318),\n",
       "  (19, 4.113508442776736)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a Log for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Model using `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), \n",
    "            tf.float32)\n",
    "        return nonzero / y_true.get_shape()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment loop...\n",
      "Number of devices: 4\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
      "<class 'tensorflow.python.distribute.input_lib.DistributedDatasetV1'>\n",
      "<class 'tensorflow.python.distribute.input_lib.DistributedDatasetV1'>\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 50)      500000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 98, 128)      19328       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 97, 128)      25728       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 96, 128)      32128       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 95, 128)      38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 98, 128)      512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 97, 128)      512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 96, 128)      512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 95, 128)      512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 98, 128)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 97, 128)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 96, 128)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 95, 128)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 98, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 97, 128)      0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 96, 128)      0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 95, 128)      0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           16416       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32)           128         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32)           0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 24)           792         dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function binary_crossentropy at 0x7f0a4ed81f28> 0.01 adam selu 1 no_class_weights no_sample_weights\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2b7d0fc91589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mtest_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-2b7d0fc91589>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dropout_ratio, learning_rate, optimizer, activation, epochs, class_weights, sample_weights, test_sample_weights, X_train_dist, X_test_dist)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         epochs=epochs)\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdist_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_distributing_by_cloning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_distribution_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m         training_utils.validate_dataset_input(x, y, sample_weight,\n\u001b[1;32m   2165\u001b[0m                                               validation_split)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model imports\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import ( Input, Embedding, GlobalMaxPooling1D, Conv1D, Dense, Activation, \n",
    "                                      Dropout, Lambda, BatchNormalization, concatenate )\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Fit imports\n",
    "from tensorflow.keras.losses import hinge, mae, binary_crossentropy, kld, Huber, squared_hinge\n",
    "\n",
    "# Hyperparameter/method search space\n",
    "import itertools\n",
    "\n",
    "\n",
    "# For 4 GPUs\n",
    "DIST_BATCH_SIZE = int(BATCH_SIZE/4)\n",
    "\n",
    "\n",
    "print('Starting experiment loop...')\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005, 0.0001]# , 0.00005]\n",
    "losses = [binary_crossentropy, hinge, squared_hinge, mae, kld, Huber, hamming_loss]\n",
    "activations = ['selu']\n",
    "optimizers = ['adam']\n",
    "dropout_ratios = [0.2]\n",
    "filter_lengths = [128]\n",
    "class_weight_set = [None, train_class_weights]\n",
    "sample_weight_set = [None, train_sample_weights]\n",
    "test_sample_weight_set = [None] #, test_sample_weights]\n",
    "\n",
    "args = itertools.product(\n",
    "    learning_rates,\n",
    "    losses,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    dropout_ratios,\n",
    "    filter_lengths,\n",
    "    class_weight_set,\n",
    "    sample_weight_set,\n",
    "    test_sample_weight_set\n",
    ")\n",
    "\n",
    "# tqdm_notebook\n",
    "for learning_rate, loss_function, activation, optimizer, dropout_ratio, filter_length, class_weights, \\\n",
    "    sample_weights, test_sample_weights in args:\n",
    "    \n",
    "    #\n",
    "    # Build ze model...\n",
    "    #\n",
    "    def build_model(\n",
    "        token_count=20000,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        dropout_ratio=0.2,\n",
    "        filter_length=filter_length,\n",
    "        loss_function='binary_crossentropy',\n",
    "        learning_rate=0.001,\n",
    "        optimizer=Adam,\n",
    "        activation='relu'\n",
    "    ):\n",
    "        \"\"\"Build the model using this experiment's parameters\"\"\"\n",
    "        \n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            \n",
    "            print ('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "            \n",
    "            # Distribute the datasets\n",
    "            X_train_dataset = mirrored_strategy.experimental_make_numpy_dataset(X_train).batch(DIST_BATCH_SIZE)\n",
    "            print(type(X_train_dataset))\n",
    "            X_test_dataset = mirrored_strategy.experimental_make_numpy_dataset(X_test).batch(DIST_BATCH_SIZE)\n",
    "            print(type(X_test_dataset))\n",
    "            X_train_dist = mirrored_strategy.experimental_distribute_dataset(X_train_dataset)\n",
    "            print(type(X_train_dist))\n",
    "            X_test_dist = mirrored_strategy.experimental_distribute_dataset(X_test_dataset)\n",
    "            print(type(X_test_dist))\n",
    "        \n",
    "            hashed_input = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "\n",
    "            emb = Embedding(token_count, embedding_dim, weights=[embedding_matrix])(hashed_input)\n",
    "\n",
    "            # Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "            conv1_1 = Conv1D(filters=filter_length, kernel_size=3)(emb)\n",
    "            btch1_1 = BatchNormalization()(conv1_1)\n",
    "            drp1_1  = Dropout(dropout_ratio)(btch1_1)\n",
    "            actv1_1 = Activation(activation)(drp1_1)\n",
    "            glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "            conv1_2 = Conv1D(filters=filter_length, kernel_size=4)(emb)\n",
    "            btch1_2 = BatchNormalization()(conv1_2)\n",
    "            drp1_2  = Dropout(dropout_ratio)(btch1_2)\n",
    "            actv1_2 = Activation(activation)(drp1_2)\n",
    "            glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "            conv1_3 = Conv1D(filters=filter_length, kernel_size=5)(emb)\n",
    "            btch1_3 = BatchNormalization()(conv1_3)\n",
    "            drp1_3  = Dropout(dropout_ratio)(btch1_3)\n",
    "            actv1_3 = Activation(activation)(drp1_3)\n",
    "            glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "            conv1_4 = Conv1D(filters=filter_length, kernel_size=6)(emb)\n",
    "            btch1_4 = BatchNormalization()(conv1_4)\n",
    "            drp1_4  = Dropout(dropout_ratio)(btch1_4)\n",
    "            actv1_4 = Activation(activation)(drp1_4)\n",
    "            glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "            # Gather all convolution layers\n",
    "            cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "            drp1 = Dropout(dropout_ratio)(cnct)\n",
    "\n",
    "            dns1  = Dense(32, activation=activation)(drp1)\n",
    "            btch1 = BatchNormalization()(dns1)\n",
    "            drp2  = Dropout(dropout_ratio)(btch1)\n",
    "\n",
    "            out = Dense(y_train.shape[1], activation='sigmoid')(drp2)\n",
    "\n",
    "            text_model = Model(\n",
    "                inputs=hashed_input, \n",
    "                outputs=out\n",
    "            )\n",
    "\n",
    "            if activation == 'adam':\n",
    "                activation = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "            if activation == 'sgd':\n",
    "                activation = SGD(lr=learning_rate)\n",
    "\n",
    "            text_model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function,\n",
    "                metrics=[\n",
    "                    'categorical_accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.BinaryAccuracy(),\n",
    "                    tf.keras.metrics.Hinge(),\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.Accuracy(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError(),\n",
    "                    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                    tf.keras.metrics.TruePositives(),\n",
    "                    tf.keras.metrics.FalsePositives(),\n",
    "                    tf.keras.metrics.TrueNegatives(),\n",
    "                    tf.keras.metrics.FalseNegatives()\n",
    "                ]\n",
    "            )\n",
    "            text_model.summary()\n",
    "\n",
    "            return text_model, X_train_dist, X_test_dist\n",
    "    \n",
    "    #\n",
    "    # Train ze model...\n",
    "    #\n",
    "    def train_model(\n",
    "        model=None,\n",
    "        dropout_ratio=0.1,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam',\n",
    "        activation='relu',\n",
    "        epochs=5,\n",
    "        class_weights=None,\n",
    "        sample_weights=None,\n",
    "        test_sample_weights=None,\n",
    "        X_train_dist=None,\n",
    "        X_test_dist=None,\n",
    "    ):\n",
    "        \"\"\"Train the model using the current parameters and evaluate performance\"\"\"\n",
    "        \n",
    "        model_name = str(loss_function) + ' ' + str(learning_rate) + ' ' + str(optimizer) + ' ' + \\\n",
    "                     str(activation) + ' ' + str(epochs) + ' ' + \\\n",
    "                     ('class_weights' if isinstance(class_weights, dict) else 'no_class_weights') + ' ' + \\\n",
    "                     ('sample_weights' if isinstance(sample_weights, np.ndarray) else 'no_sample_weights')\n",
    "        print(model_name)\n",
    "        \n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(\n",
    "                patience=1,\n",
    "                verbose=1,\n",
    "                min_delta=0.001,\n",
    "                min_lr=0.0005,\n",
    "            ), \n",
    "            EarlyStopping(\n",
    "                patience=2,\n",
    "                min_delta=0.001,\n",
    "                verbose=1,\n",
    "                restore_best_weights=True\n",
    "            ), \n",
    "            #ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = text_model.fit(\n",
    "            X_train_dist, \n",
    "            y_train,\n",
    "            class_weight=class_weights,\n",
    "            sample_weight=sample_weights,\n",
    "            epochs=epochs,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "        # Evaluate to our log and return a description key and a list of metrics\n",
    "        accr = text_model.evaluate(X_test, y_test)#, sample_weight=test_sample_weights)\n",
    "        f1_score = 2.0 * (accr[1] * accr[2]) / \\\n",
    "                         (accr[1] + accr[2])\n",
    "        return_val = model_name, [i for i in zip([j.item() for j in accr + [f1_score]], text_model.metrics_names + ['val_f1_score'])]\n",
    "\n",
    "        return return_val\n",
    "\n",
    "    #\n",
    "    # main()\n",
    "    #\n",
    "    text_model, X_train_dist, X_test_dist = build_model(\n",
    "        token_count=TOKEN_COUNT,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        filter_length=128,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        dropout_ratio=0.2\n",
    "    )\n",
    "\n",
    "    description_key, accuracies = train_model(\n",
    "        model=text_model,\n",
    "        X_train_dist=X_train_dist,\n",
    "        X_test_dist=X_test_dist,\n",
    "        dropout_ratio=dropout_ratio,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        epochs=1,\n",
    "        class_weights=class_weights,\n",
    "        sample_weights=sample_weights,\n",
    "        test_sample_weights=test_sample_weights,\n",
    "    )\n",
    "\n",
    "    log_record = (description_key, accuracies)\n",
    "    performance_log.append(log_record)\n",
    "\n",
    "    with open('data/performance_log.jsonl', 'w') as f:\n",
    "        for record in performance_log:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "    print(log_record)\n",
    "    print()\n",
    "\n",
    "print('Completed experiment loop!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test) #, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['precision_m'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        scores = []\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            j_score = jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                j_score\n",
    "            ))\n",
    "            scores.append(j_score)\n",
    "        print('')\n",
    "        mean_score = statistics.mean(scores)\n",
    "        if mean_score > max_score:\n",
    "            best_cutoff = cutoff\n",
    "            max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
