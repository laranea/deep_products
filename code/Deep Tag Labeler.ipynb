{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000 and 10,000 instances that reduced the maximum imbalance from 100-1000:1 to 8:1 using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_tags = json.load(open('data/stackoverflow/08-05-2019/sorted_all_tags.50000.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/08-05-2019/Questions.Stratified.Final.50000.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '{:,}'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open('data/stackoverflow/08-05-2019/tag_index.50000.json'))\n",
    "index_tag = json.load(open('data/stackoverflow/08-05-2019/index_tag.50000.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = json.load(open('data/stackoverflow/08-05-2019/label_counts.50000.json'))\n",
    "\n",
    "# Sanity check the difference files\n",
    "assert(len(label_counts.keys()) == len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "# Remove stopwords - now done in Spark, so can remove once that runs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [' '.join(x) for x in posts_text]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "# elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())\n",
    "\n",
    "# def ElmoEmbedding(x):\n",
    "#     return elmo_model(inputs={\n",
    "#                             \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "#                             \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "#                       },\n",
    "#                       signature=\"tokens\",\n",
    "#                       as_dict=True)[\"elmo\"]\n",
    "\n",
    "# text_input = Input(shape=(max_len,), dtype=tf.string)\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Glove Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKEN_COUNT)\n",
    "tokenizer.fit_on_texts(documents)\n",
    "# encoded_docs = tokenizer.texts_to_matrix(posts_text, mode='tfidf')\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='pre',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "print(max([len(x) for x in padded_sequences]), min([len(x) for x in padded_sequences]))\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]))\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(missing_count, too_short_count, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     posts_text,\n",
    "#     labels,\n",
    "#     test_size=TEST_SPLIT,\n",
    "#     random_state=1337\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "#assert(X_train.shape[0] == y_train.shape[0])\n",
    "#assert(X_train.shape[1] == MAX_LEN)\n",
    "#assert(X_test.shape[0] == y_test.shape[0]) \n",
    "#assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "# from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# # Text model\n",
    "# text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "# text_lstm = LSTM(\n",
    "#     input_shape=(MAX_LEN, 1024,),\n",
    "#     units=512,\n",
    "#     recurrent_dropout=0.2,\n",
    "#     dropout=0.2)(elmo_embedding)\n",
    "\n",
    "# text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "# text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "\n",
    "# text_model = Model(\n",
    "#     inputs=text_input, \n",
    "#     outputs=text_output\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.0005)\n",
    "\n",
    "# text_model.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer=adam,\n",
    "#     metrics=[\n",
    "#         precision_m,\n",
    "#         recall_m,\n",
    "#         f1_m,\n",
    "#         'mae',\n",
    "#         abs_KL_div,\n",
    "#         'accuracy'\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample and Class Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample or class weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train).tolist()\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test).tolist()\n",
    "\n",
    "train_sample_weights, test_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a Log for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Model using `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), \n",
    "            tf.float32)\n",
    "        return nonzero / y_true.get_shape()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import ( Input, Embedding, GlobalMaxPooling1D, Conv1D, Dense, Activation, \n",
    "                                      Dropout, Lambda, BatchNormalization, concatenate )\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "# Fit imports\n",
    "from tensorflow.keras.losses import hinge, mae, binary_crossentropy, kld, Huber, squared_hinge\n",
    "# from tensorflow_addons.losses.focal_loss import SigmoidFocalCrossEntropy\n",
    "# from tensorflow_addons.metrics.f_scores import F1Score\n",
    "\n",
    "# Hyperparameter/method search space\n",
    "import itertools\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005, 0.0001, 0.00005]\n",
    "losses = [binary_crossentropy, hinge, squared_hinge, mae, kld, Huber, hamming_loss]\n",
    "activations = ['selu']\n",
    "optimizers = ['adam']\n",
    "dropout_ratios = [0.2]\n",
    "filter_lengths = [128]\n",
    "class_weight_set = [None, train_class_weights]\n",
    "sample_weight_set = [None, train_sample_weights]\n",
    "test_sample_weight_set = []#None, test_sample_weights]\n",
    "\n",
    "args = itertools.product(\n",
    "    learning_rates,\n",
    "    losses,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    dropout_ratios,\n",
    "    filter_lengths,\n",
    "    class_weight_set,\n",
    "    sample_weight_set,\n",
    "    test_sample_weights\n",
    ")\n",
    "\n",
    "performance_log = {}\n",
    "for learning_rate, loss_function, activation, optimizer, dropout_ratio, filter_length, class_weights, \\\n",
    "    sample_weights, test_sample_weights in args:\n",
    "    \n",
    "    #\n",
    "    # Build ze model...\n",
    "    #\n",
    "    def build_model(\n",
    "        token_count=20000,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        dropout_ratio=0.2,\n",
    "        filter_length=filter_length,\n",
    "        loss_function='binary_crossentropy',\n",
    "        learning_rate=0.001,\n",
    "        optimizer=Adam,\n",
    "        activation='relu'\n",
    "    ):\n",
    "        \"\"\"Build the model using this experiment's parameters\"\"\"\n",
    "        \n",
    "        hashed_input = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "        \n",
    "        emb = Embedding(token_count, embedding_dim, weights=[embedding_matrix])(hashed_input)\n",
    "\n",
    "        # Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "        conv1_1 = Conv1D(filters=filter_length, kernel_size=3)(emb)\n",
    "        btch1_1 = BatchNormalization()(conv1_1)\n",
    "        drp1_1  = Dropout(dropout_ratio)(btch1_1)\n",
    "        actv1_1 = Activation('relu')(drp1_1)\n",
    "        glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "        conv1_2 = Conv1D(filters=filter_length, kernel_size=4)(emb)\n",
    "        btch1_2 = BatchNormalization()(conv1_2)\n",
    "        drp1_2  = Dropout(dropout_ratio)(btch1_2)\n",
    "        actv1_2 = Activation('relu')(drp1_2)\n",
    "        glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "        conv1_3 = Conv1D(filters=filter_length, kernel_size=5)(emb)\n",
    "        btch1_3 = BatchNormalization()(conv1_3)\n",
    "        drp1_3  = Dropout(dropout_ratio)(btch1_3)\n",
    "        actv1_3 = Activation('relu')(drp1_3)\n",
    "        glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "        conv1_4 = Conv1D(filters=filter_length, kernel_size=6)(emb)\n",
    "        btch1_4 = BatchNormalization()(conv1_4)\n",
    "        drp1_4  = Dropout(dropout_ratio)(btch1_4)\n",
    "        actv1_4 = Activation('relu')(drp1_4)\n",
    "        glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "        # Gather all convolution layers\n",
    "        cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "        drp1 = Dropout(dropout_ratio)(cnct)\n",
    "\n",
    "        dns1  = Dense(32, activation='relu')(drp1)\n",
    "        btch1 = BatchNormalization()(dns1)\n",
    "        drp2  = Dropout(dropout_ratio)(btch1)\n",
    "\n",
    "        out = Dense(y_train.shape[1], activation='sigmoid')(drp2)\n",
    "\n",
    "        text_model = Model(\n",
    "            inputs=hashed_input, \n",
    "            outputs=out\n",
    "        )\n",
    "        \n",
    "        if activation == 'adam':\n",
    "            activation = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        if activation == 'sgd':\n",
    "            activation = SGD(lr=learning_rate)\n",
    "\n",
    "        text_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function,\n",
    "            metrics=[\n",
    "                'categorical_accuracy',\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.BinaryAccuracy(),\n",
    "                tf.keras.metrics.Hinge(),\n",
    "                tf.keras.metrics.AUC(),\n",
    "                tf.keras.metrics.Accuracy(),\n",
    "                tf.keras.metrics.MeanAbsoluteError(),\n",
    "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                tf.keras.metrics.TruePositives(),\n",
    "                tf.keras.metrics.FalsePositives(),\n",
    "                tf.keras.metrics.TrueNegatives(),\n",
    "                tf.keras.metrics.FalseNegatives()\n",
    "            ]\n",
    "        )\n",
    "        text_model.summary()\n",
    "\n",
    "        return text_model\n",
    "    \n",
    "    #\n",
    "    # Train ze model...\n",
    "    #\n",
    "    def train_model(\n",
    "        model=None,\n",
    "        dropout_ratio=0.1,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam',\n",
    "        activation='relu',\n",
    "        epochs=5,\n",
    "        class_weights=None,\n",
    "        sample_weights=None,\n",
    "        test_sample_weights=None\n",
    "    ):\n",
    "        \"\"\"Train the model using the current parameters and evaluate performance\"\"\"\n",
    "        \n",
    "        model_name = str(loss_function) + ' ' + str(learning_rate) + ' ' + str(optimizer) + ' ' + \\\n",
    "                     str(activation) + ' ' + str(epochs) + ' ' + \\\n",
    "                     ('class_weights' if isinstance(class_weights, dict) else 'no_class_weights') + ' ' + \\\n",
    "                     ('sample_weights' if isinstance(sample_weights, np.ndarray) else 'no_sample_weights')\n",
    "        print(model_name)\n",
    "        \n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(), \n",
    "            EarlyStopping(patience=1), \n",
    "            #ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = text_model.fit(\n",
    "            X_train, \n",
    "            y_train,\n",
    "            class_weight=class_weights,\n",
    "            sample_weight=sample_weights,\n",
    "            epochs=epochs,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "        # Evaluate to our log and return a description key and a list of metrics\n",
    "        accr = text_model.evaluate(X_test, y_test)#, sample_weight=test_sample_weights)\n",
    "        f1_score = 2.0 * (accr[1] * accr[2]) / \\\n",
    "                         (accr[1] + accr[2])\n",
    "        return_val = model_name, [i for i in zip(accr + [f1_score], text_model.metrics_names + ['val_f1_score'])]\n",
    "\n",
    "        return return_val\n",
    "\n",
    "    #\n",
    "    # main()\n",
    "    #\n",
    "    text_model = build_model(\n",
    "        token_count=TOKEN_COUNT,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        filter_length=128,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        dropout_ratio=0.2\n",
    "    )\n",
    "\n",
    "    description_key, accuracies = train_model(\n",
    "        model=text_model,\n",
    "        dropout_ratio=dropout_ratio,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        epochs=1,\n",
    "        class_weights=class_weights,\n",
    "        sample_weights=sample_weights,\n",
    "        test_sample_weights=test_sample_weights,\n",
    "    )\n",
    "    performance_log[description_key] = accuracies\n",
    "    with open('data/performance_log.jsonl', 'w') as f:\n",
    "        for key, value in performance_log.items():\n",
    "            value.update({'name': key})\n",
    "            f.write(json.dumps(value) + '\\n')\n",
    "    \n",
    "    print(description_key, accuracies)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test) #, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['precision_m'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        scores = []\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            j_score = jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                j_score\n",
    "            ))\n",
    "            scores.append(j_score)\n",
    "        print('')\n",
    "        mean_score = statistics.mean(scores)\n",
    "        if mean_score > max_score:\n",
    "            best_cutoff = cutoff\n",
    "            max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
