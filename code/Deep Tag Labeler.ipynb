{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 14 Million Answered Questions with Tags from Stack Overflow\n",
    "\n",
    "We load all answered questions from Stack Overflow. This data was converted from XML to JSON and then sampled using Spark on a single `r5.12xlarge` machine cluster with [code/stackoverflow/sample_json.spark.py](stackoverflow/sample_json.spark.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I want to use a track-bar to change a form'...</td>\n",
       "      <td>&lt;c#&gt;&lt;floating-point&gt;&lt;type-conversion&gt;&lt;double&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have an absolutely positioned &lt;code&gt;div&lt;/...</td>\n",
       "      <td>&lt;html&gt;&lt;css&gt;&lt;css3&gt;&lt;internet-explorer-7&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Given a &lt;code&gt;DateTime&lt;/code&gt; representing ...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;&lt;datetime&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Given a specific &lt;code&gt;DateTime&lt;/code&gt; valu...</td>\n",
       "      <td>&lt;c#&gt;&lt;datetime&gt;&lt;time&gt;&lt;datediff&gt;&lt;relative-time-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Is there a standard way for a web server to...</td>\n",
       "      <td>&lt;html&gt;&lt;browser&gt;&lt;timezone&gt;&lt;user-agent&gt;&lt;timezone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>I want to use a track-bar to change a form'...   \n",
       "1  <p>I have an absolutely positioned <code>div</...   \n",
       "2  <p>Given a <code>DateTime</code> representing ...   \n",
       "3  <p>Given a specific <code>DateTime</code> valu...   \n",
       "4  <p>Is there a standard way for a web server to...   \n",
       "\n",
       "                                               _Tags  \n",
       "0  <c#><floating-point><type-conversion><double><...  \n",
       "1             <html><css><css3><internet-explorer-7>  \n",
       "2                               <c#><.net><datetime>  \n",
       "3  <c#><datetime><time><datediff><relative-time-s...  \n",
       "4  <html><browser><timezone><user-agent><timezone...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/parquet/Questions.Answered.parquet',\n",
    "    columns=['_Body', '_Tags'],\n",
    "    filters=[('_Tags','!=',None),],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I want to use a track-bar to change a form'...</td>\n",
       "      <td>&lt;c#&gt;&lt;floating-point&gt;&lt;type-conversion&gt;&lt;double&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have an absolutely positioned &lt;code&gt;div&lt;/...</td>\n",
       "      <td>&lt;html&gt;&lt;css&gt;&lt;css3&gt;&lt;internet-explorer-7&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Given a &lt;code&gt;DateTime&lt;/code&gt; representing ...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;&lt;datetime&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Given a specific &lt;code&gt;DateTime&lt;/code&gt; valu...</td>\n",
       "      <td>&lt;c#&gt;&lt;datetime&gt;&lt;time&gt;&lt;datediff&gt;&lt;relative-time-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Is there a standard way for a web server to...</td>\n",
       "      <td>&lt;html&gt;&lt;browser&gt;&lt;timezone&gt;&lt;user-agent&gt;&lt;timezone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>I want to use a track-bar to change a form'...   \n",
       "1  <p>I have an absolutely positioned <code>div</...   \n",
       "2  <p>Given a <code>DateTime</code> representing ...   \n",
       "3  <p>Given a specific <code>DateTime</code> valu...   \n",
       "4  <p>Is there a standard way for a web server to...   \n",
       "\n",
       "                                               _Tags  \n",
       "0  <c#><floating-point><type-conversion><double><...  \n",
       "1             <html><css><css3><internet-explorer-7>  \n",
       "2                               <c#><.net><datetime>  \n",
       "3  <c#><datetime><time><datediff><relative-time-s...  \n",
       "4  <html><browser><timezone><user-agent><timezone...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = posts_df.head(1000000)\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unlabeled Posts\n",
    "\n",
    "Note: these have already been filtered to remove untagged questions, so there are from 1-5 labels per post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts w/ tags: 1,000,000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I want to use a track-bar to change a form'...</td>\n",
       "      <td>&lt;c#&gt;&lt;floating-point&gt;&lt;type-conversion&gt;&lt;double&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have an absolutely positioned &lt;code&gt;div&lt;/...</td>\n",
       "      <td>&lt;html&gt;&lt;css&gt;&lt;css3&gt;&lt;internet-explorer-7&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Given a &lt;code&gt;DateTime&lt;/code&gt; representing ...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;&lt;datetime&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Given a specific &lt;code&gt;DateTime&lt;/code&gt; valu...</td>\n",
       "      <td>&lt;c#&gt;&lt;datetime&gt;&lt;time&gt;&lt;datediff&gt;&lt;relative-time-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Is there a standard way for a web server to...</td>\n",
       "      <td>&lt;html&gt;&lt;browser&gt;&lt;timezone&gt;&lt;user-agent&gt;&lt;timezone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>I want to use a track-bar to change a form'...   \n",
       "1  <p>I have an absolutely positioned <code>div</...   \n",
       "2  <p>Given a <code>DateTime</code> representing ...   \n",
       "3  <p>Given a specific <code>DateTime</code> valu...   \n",
       "4  <p>Is there a standard way for a web server to...   \n",
       "\n",
       "                                               _Tags  \n",
       "0  <c#><floating-point><type-conversion><double><...  \n",
       "1             <html><css><css3><internet-explorer-7>  \n",
       "2                               <c#><.net><datetime>  \n",
       "3  <c#><datetime><time><datediff><relative-time-s...  \n",
       "4  <html><browser><timezone><user-agent><timezone...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_posts = posts_df.dropna(axis=0, subset=['_Tags'])\n",
    "print('Posts w/ tags: {:,}'.format(len(tag_posts.index)))\n",
    "tag_posts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Tags from their XML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_posts['_Tag_List'] = tag_posts['_Tags'].apply(lambda x: re.findall('\\<(.+?)\\>', x))\n",
    "\n",
    "flat_tags = tag_posts.apply(lambda x: pd.Series(x['_Tag_List']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "flat_tags.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_counts = flat_tags.groupby(flat_tags).count().sort_values(ascending=False)\n",
    "# print(tag_counts[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Distribution of Tag Counts\n",
    "\n",
    "This is split between two charts for clarity, tag counts <100 and >=100. The data is highly skewed, which is an issue we must address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# tag_counts[tag_counts <= 100].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_counts[tag_counts > 100].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Thresholds for Filtering Tags by Frequency\n",
    "\n",
    "The higher the threshold, the fewer classes, the less sparse the data, the easier the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23,795 tags with more than 0 count\n",
      "There are 9,706 tags with more than 10 count\n",
      "There are 6,660 tags with more than 20 count\n",
      "There are 3,901 tags with more than 50 count\n",
      "There are 2,508 tags with more than 100 count\n",
      "There are 416 tags with more than 1,000 count\n",
      "There are 70 tags with more than 5,000 count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for row in tag_posts['_Tag_List']:\n",
    "    for tag in row:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "for i in [0, 10, 20, 50, 100, 1000, 5000]:\n",
    "    filtered_tags = list(filter(lambda x: x > i, tag_counts.values()))\n",
    "    print('There are {:,} tags with more than {:,} count'.format(len(filtered_tags), i))\n",
    "\n",
    "MIN_TAGS = 5000\n",
    "\n",
    "record_count = len([i for i in filter(lambda x: x > MIN_TAGS, tag_counts.values())])\n",
    "record_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags with 5,000 occurrences: 70\n"
     ]
    }
   ],
   "source": [
    "all_tags = set()\n",
    "for row in tag_posts['_Tag_List']:\n",
    "    for tag in row:\n",
    "        if tag_counts[tag] > MIN_TAGS:\n",
    "            all_tags.add(tag)\n",
    "\n",
    "print('Total unique tags with {:,} occurrences: {:,}'.format(MIN_TAGS, len(all_tags)))\n",
    "sorted_all_tags = sorted(all_tags)\n",
    "\n",
    "tag_to_id = {val:i for i, val in enumerate(sorted_all_tags)}\n",
    "id_to_tag = {i:val for i, val in enumerate(sorted_all_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Tag Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 70, 70)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "tag_list = tag_posts['_Tag_List'].tolist()\n",
    "\n",
    "# Loop through every post...\n",
    "for i, tag_set in enumerate(tag_posts['_Tag_List'].tolist()):\n",
    "    # Then build a record_count element wide list for each tag present\n",
    "    label_row = []\n",
    "    for tag in sorted_all_tags:\n",
    "        if tag in tag_list[i]:\n",
    "            label_row.append(1)\n",
    "        else:\n",
    "            label_row.append(0)\n",
    "    labels.append(label_row)\n",
    "    \n",
    "tag_labels = [id_to_tag[key_id] for key_id in sorted(id_to_tag.keys()) if tag_counts[id_to_tag[key_id]] > MIN_TAGS]\n",
    "\n",
    "len(labels), len(labels[0]), len(tag_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract/Tokenize Non-Code Text from Posts\n",
    "\n",
    "We leave posts' source code out for now because it will need a different embedding and thus multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [i, want, to, use, a, track-bar, to, change, a...\n",
       "1    [i, have, an, absolutely, positioned, containi...\n",
       "2    [given, a, representing, a, person's, birthday...\n",
       "3    [given, a, specific, value,, how, do, i, displ...\n",
       "4    [is, there, a, standard, way, for, a, web, ser...\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import  Pool\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def parallelize_series(s, func, n_cores=4):\n",
    "    s_split = np.array_split(s, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    s = pd.concat(pool.map(func, s_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return s\n",
    "\n",
    "N_CORES = 12\n",
    "MAX_LEN = 100\n",
    "PAD_TOKEN = '__PAD__'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def extract_text(s):\n",
    "    \"\"\"Extract non-code text from posts (questions/answers)\"\"\"\n",
    "    text = []\n",
    "    for body in s:\n",
    "        doc = BeautifulSoup(body)\n",
    "        codes = doc.find_all('code')\n",
    "        [code.extract() if code else None for code in codes]\n",
    "        tokens = doc.text.split()\n",
    "        padded_tokens = [tokens[i].lower() if len(tokens) > i else PAD_TOKEN for i in range(0,MAX_LEN)]\n",
    "        text.append(padded_tokens)\n",
    "    return pd.Series(text)\n",
    "\n",
    "post_text = parallelize_series(tag_posts._Body, extract_text, n_cores=N_CORES).reset_index(drop=True)\n",
    "#post_text = tag_posts._Body.apply(extract_text).reset_index(drop=True)\n",
    "post_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 100, 1000000, 70)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_text.index), len(post_text.iloc[0]), len(labels), len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 1,000,000 example posts\n"
     ]
    }
   ],
   "source": [
    "# Validate the posts match the labels\n",
    "assert(len(post_text.index) == len(labels))\n",
    "print('We are left with {:,} example posts'.format(len(post_text.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d7312d37609d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Filter the posts to match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpost_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpost_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_zero_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpost_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel_mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Filter label rows that don't have any positive labels\n",
    "label_mx = np.array(labels)\n",
    "max_per_row = label_mx.max(axis=1)\n",
    "non_zero_index = np.nonzero(max_per_row)[0]\n",
    "\n",
    "label_mx = label_mx[non_zero_index]\n",
    "\n",
    "# Filter the posts to match\n",
    "post_text = post_text[post_text.index.isin(non_zero_index)]\n",
    "post_text = post_text.to\n",
    "\n",
    "assert(post_text.shape[0] == label_mx.shape[0])\n",
    "print('Unfiltered Counts: {:,} {:,}'.format(post_text.shape[0], label_mx.shape[0]))\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(post_text.shape[0] / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "label_mx = label_mx[0:training_count]\n",
    "post_text = post_text[0:training_count]\n",
    "\n",
    "assert(post_text.shape[0] == label_mx.shape[0])\n",
    "print('Final Counts: {:,} {:,}'.format(post_text.shape[0], label_mx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['this', 'is', 'what', ..., '__PAD__', '__PAD__', '__PAD__'],\n",
       "       ['hi', 'this', 'script', ..., '__PAD__', '__PAD__', '__PAD__'],\n",
       "       [\"i'm\", 'creating', 'an', ..., 'figure', 'out', 'how'],\n",
       "       ...,\n",
       "       ['i', 'want', 'to', ..., '__PAD__', '__PAD__', '__PAD__'],\n",
       "       ['assertjson', 'fails', 'even', ..., '__PAD__', '__PAD__',\n",
       "        '__PAD__'],\n",
       "       ['i', 'am', 'doing', ..., '__PAD__', '__PAD__', '__PAD__']],\n",
       "      dtype='<U1356')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 15:34:57.517402 140412342794048 module_wrapper.py:136] From /home/rjurney/anaconda/envs/deep/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59840, 100), (10560, 100), (59840, 33), (10560, 33))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    post_text,\n",
    "    label_mx,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=34\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "def abs_KL_div(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), None)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), None)\n",
    "    return K.sum(K.abs( (y_true - y_pred) * (K.log(y_true / y_pred))), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "# from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# # Text model\n",
    "# text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "# text_lstm = LSTM(\n",
    "#     input_shape=(MAX_LEN, 1024,),\n",
    "#     units=512,\n",
    "#     recurrent_dropout=0.2,\n",
    "#     dropout=0.2)(elmo_embedding)\n",
    "\n",
    "# text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "# text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "\n",
    "# text_model = Model(\n",
    "#     inputs=text_input, \n",
    "#     outputs=text_output\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.0005)\n",
    "\n",
    "# text_model.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer=adam,\n",
    "#     metrics=[\n",
    "#         precision_m,\n",
    "#         recall_m,\n",
    "#         f1_m,\n",
    "#         'mae',\n",
    "#         abs_KL_div,\n",
    "#         'accuracy'\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample and Class Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample or class weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 44.24890006285355,\n",
       " 1: 61.80860403863038,\n",
       " 2: 11.001719018596656,\n",
       " 3: 51.76470588235294,\n",
       " 4: 40.36697247706422,\n",
       " 5: 35.84521384928717,\n",
       " 6: 40.43653072946582,\n",
       " 7: 9.547057228098724,\n",
       " 8: 19.670298966191673,\n",
       " 9: 20.72416838386812,\n",
       " 10: 64.0582347588717,\n",
       " 11: 64.46886446886447,\n",
       " 12: 14.614905542869005,\n",
       " 13: 21.43727161997564,\n",
       " 14: 50.79365079365079,\n",
       " 15: 8.007279344858963,\n",
       " 16: 6.984126984126984,\n",
       " 17: 12.867848656552733,\n",
       " 18: 46.591661151555265,\n",
       " 19: 69.42800788954635,\n",
       " 20: 22.229239027470793,\n",
       " 21: 48.65238424326192,\n",
       " 22: 40.41331802525833,\n",
       " 23: 9.639874024373546,\n",
       " 24: 10.880989180834622,\n",
       " 25: 45.86319218241042,\n",
       " 26: 57.65765765765766,\n",
       " 27: 55.34591194968554,\n",
       " 28: 38.745184369840395,\n",
       " 29: 23.56091030789826,\n",
       " 30: 46.31578947368421,\n",
       " 31: 58.91213389121339,\n",
       " 32: 66.10328638497653}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = {}\n",
    "for i, tag in enumerate(sorted_all_tags):\n",
    "    class_weights[i] = label_mx.shape[0] / tag_counts[tag]\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a Log for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Model using `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Embedding, GlobalMaxPool1D, Conv1D, Dense, Activation, Dropout, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "\n",
    "def build_model(max_len, label_count, dropout_ratio=0.1, filter_length=50):\n",
    "    \n",
    "    text_input = Input(shape=(max_len,), dtype=tf.string)\n",
    "\n",
    "    elmo_embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(text_input)\n",
    "\n",
    "    dropout = Dropout(dropout_ratio)(elmo_embedding)\n",
    "\n",
    "    conv1d = Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1)(dropout)\n",
    "\n",
    "    global_1d = GlobalMaxPool1D()(conv1d)\n",
    "\n",
    "    dense = Dense(label_count, activation='sigmoid')(global_1d)\n",
    "\n",
    "    text_model = Model(\n",
    "        inputs=text_input, \n",
    "        outputs=dense\n",
    "    )\n",
    "\n",
    "    text_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=abs_KL_div,#'binary_crossentropy',\n",
    "        metrics=[\n",
    "            'categorical_accuracy',\n",
    "            precision_m,\n",
    "            recall_m,\n",
    "            f1_m,\n",
    "            'mae',\n",
    "            abs_KL_div,\n",
    "            true_positive(),\n",
    "            false_positive(),\n",
    "            true_negative(),\n",
    "            false_negative(),\n",
    "            'accuracy',\n",
    "        ]\n",
    "    )\n",
    "    text_model.summary()\n",
    "    \n",
    "    return text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 98, 50)            153650    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 33)                1683      \n",
      "=================================================================\n",
      "Total params: 155,333\n",
      "Trainable params: 155,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 59840 samples, validate on 10560 samples\n",
      "Epoch 1/20\n",
      "59840/59840 [==============================] - 494s 8ms/step - loss: 229.5814 - categorical_accuracy: 0.1351 - precision_m: 0.0576 - recall_m: 0.0017 - f1_m: 0.0019 - mean_absolute_error: 0.0478 - abs_KL_div: 9.5175 - true_positive: 4.0000 - false_positive: 166.0000 - true_negative: 58324.0000 - false_negative: 1346.0000 - acc: 0.1351 - val_loss: 8.0077 - val_categorical_accuracy: 0.2410 - val_precision_m: 0.3697 - val_recall_m: 0.0056 - val_f1_m: 0.0110 - val_mean_absolute_error: 0.0458 - val_abs_KL_div: 8.0077 - val_true_positive: 0.0000e+00 - val_false_positive: 0.0000e+00 - val_true_negative: 10352.0000 - val_false_negative: 208.0000 - val_acc: 0.2410\n",
      "Epoch 2/20\n",
      "59840/59840 [==============================] - 490s 8ms/step - loss: 178.5241 - categorical_accuracy: 0.2956 - precision_m: 0.7117 - recall_m: 0.0203 - f1_m: 0.0390 - mean_absolute_error: 0.0445 - abs_KL_div: 7.5648 - true_positive: 1.0000 - false_positive: 2.0000 - true_negative: 58488.0000 - false_negative: 1349.0000 - acc: 0.2956 - val_loss: 7.2832 - val_categorical_accuracy: 0.3336 - val_precision_m: 0.9164 - val_recall_m: 0.0485 - val_f1_m: 0.0912 - val_mean_absolute_error: 0.0438 - val_abs_KL_div: 7.2832 - val_true_positive: 0.0000e+00 - val_false_positive: 0.0000e+00 - val_true_negative: 10352.0000 - val_false_negative: 208.0000 - val_acc: 0.3336\n",
      "Epoch 3/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 161.1733 - categorical_accuracy: 0.3645 - precision_m: 0.9236 - recall_m: 0.0561 - f1_m: 0.1046 - mean_absolute_error: 0.0426 - abs_KL_div: 6.9820 - true_positive: 5.0000 - false_positive: 3.0000 - true_negative: 58487.0000 - false_negative: 1345.0000 - acc: 0.3645 - val_loss: 7.0050 - val_categorical_accuracy: 0.3652 - val_precision_m: 0.9226 - val_recall_m: 0.0779 - val_f1_m: 0.1424 - val_mean_absolute_error: 0.0423 - val_abs_KL_div: 7.0050 - val_true_positive: 5.0000 - val_false_positive: 0.0000e+00 - val_true_negative: 10352.0000 - val_false_negative: 203.0000 - val_acc: 0.3652\n",
      "Epoch 4/20\n",
      "59840/59840 [==============================] - 490s 8ms/step - loss: 151.7417 - categorical_accuracy: 0.3934 - precision_m: 0.9362 - recall_m: 0.0874 - f1_m: 0.1584 - mean_absolute_error: 0.0412 - abs_KL_div: 6.6484 - true_positive: 3.0000 - false_positive: 4.0000 - true_negative: 58486.0000 - false_negative: 1347.0000 - acc: 0.3934 - val_loss: 6.7941 - val_categorical_accuracy: 0.3874 - val_precision_m: 0.9036 - val_recall_m: 0.1098 - val_f1_m: 0.1944 - val_mean_absolute_error: 0.0412 - val_abs_KL_div: 6.7941 - val_true_positive: 2.0000 - val_false_positive: 1.0000 - val_true_negative: 10351.0000 - val_false_negative: 206.0000 - val_acc: 0.3874\n",
      "Epoch 5/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 145.6828 - categorical_accuracy: 0.4133 - precision_m: 0.9373 - recall_m: 0.1108 - f1_m: 0.1965 - mean_absolute_error: 0.0402 - abs_KL_div: 6.4481 - true_positive: 18.0000 - false_positive: 7.0000 - true_negative: 58483.0000 - false_negative: 1332.0000 - acc: 0.4133 - val_loss: 6.7125 - val_categorical_accuracy: 0.4117 - val_precision_m: 0.9069 - val_recall_m: 0.1373 - val_f1_m: 0.2367 - val_mean_absolute_error: 0.0406 - val_abs_KL_div: 6.7125 - val_true_positive: 3.0000 - val_false_positive: 3.0000 - val_true_negative: 10349.0000 - val_false_negative: 205.0000 - val_acc: 0.4117\n",
      "Epoch 6/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 141.5802 - categorical_accuracy: 0.4271 - precision_m: 0.9398 - recall_m: 0.1263 - f1_m: 0.2210 - mean_absolute_error: 0.0396 - abs_KL_div: 6.3088 - true_positive: 17.0000 - false_positive: 8.0000 - true_negative: 58482.0000 - false_negative: 1333.0000 - acc: 0.4271 - val_loss: 6.7545 - val_categorical_accuracy: 0.4009 - val_precision_m: 0.8997 - val_recall_m: 0.1356 - val_f1_m: 0.2342 - val_mean_absolute_error: 0.0402 - val_abs_KL_div: 6.7545 - val_true_positive: 5.0000 - val_false_positive: 7.0000 - val_true_negative: 10345.0000 - val_false_negative: 203.0000 - val_acc: 0.4009\n",
      "Epoch 7/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 138.3622 - categorical_accuracy: 0.4347 - precision_m: 0.9411 - recall_m: 0.1377 - f1_m: 0.2384 - mean_absolute_error: 0.0391 - abs_KL_div: 6.2186 - true_positive: 27.0000 - false_positive: 14.0000 - true_negative: 58476.0000 - false_negative: 1323.0000 - acc: 0.4347 - val_loss: 6.7301 - val_categorical_accuracy: 0.4188 - val_precision_m: 0.8862 - val_recall_m: 0.1625 - val_f1_m: 0.2730 - val_mean_absolute_error: 0.0397 - val_abs_KL_div: 6.7301 - val_true_positive: 3.0000 - val_false_positive: 1.0000 - val_true_negative: 10351.0000 - val_false_negative: 205.0000 - val_acc: 0.4188\n",
      "Epoch 8/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 135.7968 - categorical_accuracy: 0.4400 - precision_m: 0.9423 - recall_m: 0.1489 - f1_m: 0.2553 - mean_absolute_error: 0.0387 - abs_KL_div: 6.1407 - true_positive: 30.0000 - false_positive: 18.0000 - true_negative: 58472.0000 - false_negative: 1320.0000 - acc: 0.4400 - val_loss: 6.6879 - val_categorical_accuracy: 0.4147 - val_precision_m: 0.9010 - val_recall_m: 0.1610 - val_f1_m: 0.2715 - val_mean_absolute_error: 0.0395 - val_abs_KL_div: 6.6879 - val_true_positive: 1.0000 - val_false_positive: 1.0000 - val_true_negative: 10351.0000 - val_false_negative: 207.0000 - val_acc: 0.4147\n",
      "Epoch 9/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 133.7937 - categorical_accuracy: 0.4445 - precision_m: 0.9453 - recall_m: 0.1559 - f1_m: 0.2657 - mean_absolute_error: 0.0384 - abs_KL_div: 6.0909 - true_positive: 48.0000 - false_positive: 14.0000 - true_negative: 58476.0000 - false_negative: 1302.0000 - acc: 0.4445 - val_loss: 6.6882 - val_categorical_accuracy: 0.4234 - val_precision_m: 0.8913 - val_recall_m: 0.1691 - val_f1_m: 0.2824 - val_mean_absolute_error: 0.0392 - val_abs_KL_div: 6.6882 - val_true_positive: 11.0000 - val_false_positive: 8.0000 - val_true_negative: 10344.0000 - val_false_negative: 197.0000 - val_acc: 0.4234\n",
      "Epoch 10/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 131.9987 - categorical_accuracy: 0.4496 - precision_m: 0.9450 - recall_m: 0.1626 - f1_m: 0.2754 - mean_absolute_error: 0.0381 - abs_KL_div: 6.0468 - true_positive: 47.0000 - false_positive: 18.0000 - true_negative: 58472.0000 - false_negative: 1303.0000 - acc: 0.4496 - val_loss: 6.5495 - val_categorical_accuracy: 0.4253 - val_precision_m: 0.8980 - val_recall_m: 0.1776 - val_f1_m: 0.2948 - val_mean_absolute_error: 0.0389 - val_abs_KL_div: 6.5495 - val_true_positive: 5.0000 - val_false_positive: 2.0000 - val_true_negative: 10350.0000 - val_false_negative: 203.0000 - val_acc: 0.4253\n",
      "Epoch 11/20\n",
      "59840/59840 [==============================] - 492s 8ms/step - loss: 130.4137 - categorical_accuracy: 0.4520 - precision_m: 0.9438 - recall_m: 0.1673 - f1_m: 0.2821 - mean_absolute_error: 0.0379 - abs_KL_div: 6.0034 - true_positive: 52.0000 - false_positive: 15.0000 - true_negative: 58475.0000 - false_negative: 1298.0000 - acc: 0.4520 - val_loss: 6.4955 - val_categorical_accuracy: 0.4295 - val_precision_m: 0.9007 - val_recall_m: 0.1760 - val_f1_m: 0.2924 - val_mean_absolute_error: 0.0385 - val_abs_KL_div: 6.4955 - val_true_positive: 5.0000 - val_false_positive: 4.0000 - val_true_negative: 10348.0000 - val_false_negative: 203.0000 - val_acc: 0.4295\n",
      "Epoch 12/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 129.0353 - categorical_accuracy: 0.4560 - precision_m: 0.9470 - recall_m: 0.1708 - f1_m: 0.2873 - mean_absolute_error: 0.0377 - abs_KL_div: 5.9670 - true_positive: 59.0000 - false_positive: 18.0000 - true_negative: 58472.0000 - false_negative: 1291.0000 - acc: 0.4560 - val_loss: 6.7882 - val_categorical_accuracy: 0.4203 - val_precision_m: 0.8877 - val_recall_m: 0.1933 - val_f1_m: 0.3158 - val_mean_absolute_error: 0.0389 - val_abs_KL_div: 6.7882 - val_true_positive: 4.0000 - val_false_positive: 0.0000e+00 - val_true_negative: 10352.0000 - val_false_negative: 204.0000 - val_acc: 0.4203\n",
      "Epoch 13/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 127.6901 - categorical_accuracy: 0.4597 - precision_m: 0.9497 - recall_m: 0.1755 - f1_m: 0.2941 - mean_absolute_error: 0.0375 - abs_KL_div: 5.9247 - true_positive: 62.0000 - false_positive: 26.0000 - true_negative: 58464.0000 - false_negative: 1288.0000 - acc: 0.4597 - val_loss: 6.7174 - val_categorical_accuracy: 0.4236 - val_precision_m: 0.8866 - val_recall_m: 0.1918 - val_f1_m: 0.3137 - val_mean_absolute_error: 0.0388 - val_abs_KL_div: 6.7174 - val_true_positive: 8.0000 - val_false_positive: 10.0000 - val_true_negative: 10342.0000 - val_false_negative: 200.0000 - val_acc: 0.4236\n",
      "Epoch 14/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 126.5381 - categorical_accuracy: 0.4632 - precision_m: 0.9508 - recall_m: 0.1796 - f1_m: 0.3001 - mean_absolute_error: 0.0373 - abs_KL_div: 5.8949 - true_positive: 62.0000 - false_positive: 22.0000 - true_negative: 58468.0000 - false_negative: 1288.0000 - acc: 0.4632 - val_loss: 6.7881 - val_categorical_accuracy: 0.4254 - val_precision_m: 0.8787 - val_recall_m: 0.1936 - val_f1_m: 0.3154 - val_mean_absolute_error: 0.0390 - val_abs_KL_div: 6.7881 - val_true_positive: 6.0000 - val_false_positive: 5.0000 - val_true_negative: 10347.0000 - val_false_negative: 202.0000 - val_acc: 0.4254\n",
      "Epoch 15/20\n",
      "59840/59840 [==============================] - 491s 8ms/step - loss: 125.4938 - categorical_accuracy: 0.4646 - precision_m: 0.9470 - recall_m: 0.1824 - f1_m: 0.3038 - mean_absolute_error: 0.0372 - abs_KL_div: 5.8774 - true_positive: 79.0000 - false_positive: 24.0000 - true_negative: 58466.0000 - false_negative: 1271.0000 - acc: 0.4646 - val_loss: 6.6506 - val_categorical_accuracy: 0.4251 - val_precision_m: 0.8866 - val_recall_m: 0.1875 - val_f1_m: 0.3076 - val_mean_absolute_error: 0.0385 - val_abs_KL_div: 6.6506 - val_true_positive: 5.0000 - val_false_positive: 7.0000 - val_true_negative: 10345.0000 - val_false_negative: 203.0000 - val_acc: 0.4251\n"
     ]
    }
   ],
   "source": [
    "text_model = build_model(MAX_LEN, y_train.shape[1])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=4), \n",
    "    # ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = text_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    class_weight=class_weights,\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['precision_m'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        scores = []\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            j_score = jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                j_score\n",
    "            ))\n",
    "            scores.append(j_score)\n",
    "        print('')\n",
    "        mean_score = statistics.mean(scores)\n",
    "        if mean_score > max_score:\n",
    "            best_cutoff = cutoff\n",
    "            max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
