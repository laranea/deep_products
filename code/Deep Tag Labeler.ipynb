{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(\n",
    "    tf.test.is_gpu_available(\n",
    "        cuda_only=False,\n",
    "        min_cuda_compute_capability=None\n",
    "    )\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    tf.compat.v2.config.experimental.list_physical_devices('GPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000 and 10,000 instances that reduced the maximum imbalance from 100-1000:1 to 8:1 using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_tags = json.load(open('data/stackoverflow/08-05-2019/sorted_all_tags.50000.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>label_20</th>\n",
       "      <th>label_21</th>\n",
       "      <th>label_22</th>\n",
       "      <th>label_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C, Mono, Winforms, MessageBox, problem, I, fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Are, NET, data, providers, Oracle, require, O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[How, I, focus, foreign, window, I, applicatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Default, button, hit, windows, forms, trying,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Can, I, avoid, JIT, net, Say, code, always, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  label_0  label_1  \\\n",
       "0  [C, Mono, Winforms, MessageBox, problem, I, fi...        1        0   \n",
       "1  [Are, NET, data, providers, Oracle, require, O...        1        0   \n",
       "2  [How, I, focus, foreign, window, I, applicatio...        1        0   \n",
       "3  [Default, button, hit, windows, forms, trying,...        1        0   \n",
       "4  [Can, I, avoid, JIT, net, Say, code, always, g...        1        0   \n",
       "\n",
       "   label_2  label_3  label_4  label_5  label_6  label_7  label_8  ...  \\\n",
       "0        0        0        0        1        0        0        0  ...   \n",
       "1        0        0        0        0        0        0        0  ...   \n",
       "2        0        0        0        1        0        0        0  ...   \n",
       "3        0        0        0        1        0        0        0  ...   \n",
       "4        0        0        0        0        0        0        0  ...   \n",
       "\n",
       "   label_14  label_15  label_16  label_17  label_18  label_19  label_20  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   label_21  label_22  label_23  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         0         0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/08-05-2019/Questions.Stratified.Final.50000.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,293,018\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,}'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open('data/stackoverflow/08-05-2019/tag_index.50000.json'))\n",
    "index_tag = json.load(open('data/stackoverflow/08-05-2019/index_tag.50000.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = json.load(open('data/stackoverflow/08-05-2019/label_counts.50000.json'))\n",
    "\n",
    "# Sanity check the difference files\n",
    "assert(len(label_counts.keys()) == len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 12 Training Count: 1,228,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "MAX_LEN = 100\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "# Remove stopwords - now done in Spark, so can remove once that runs\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [' '.join(x) for x in posts_text]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "# elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())\n",
    "\n",
    "# def ElmoEmbedding(x):\n",
    "#     return elmo_model(inputs={\n",
    "#                             \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "#                             \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "#                       },\n",
    "#                       signature=\"tokens\",\n",
    "#                       as_dict=True)[\"elmo\"]\n",
    "\n",
    "# text_input = Input(shape=(max_len,), dtype=tf.string)\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Glove Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1228800, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKEN_COUNT)\n",
    "tokenizer.fit_on_texts(documents)\n",
    "# encoded_docs = tokenizer.texts_to_matrix(posts_text, mode='tfidf')\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='pre',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "print(max([len(x) for x in padded_sequences]), min([len(x) for x in padded_sequences]))\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]))\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  70, 3348, 2215, ...,    1,    1,    1],\n",
       "       [  56,  170,   34, ...,    1,    1,    1],\n",
       "       [  21,    2, 1487, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [   2,   60,   14, ...,    1,    1,    1],\n",
       "       [   2,   25,   67, ...,    1,    1,    1],\n",
       "       [  25,   44,    4, ...,    1,    1,    1]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/deep/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "# print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     posts_text,\n",
    "#     labels,\n",
    "#     test_size=TEST_SPLIT,\n",
    "#     random_state=1337\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:100000]\n",
    "y_train = y_train[:100000]\n",
    "X_test  = X_test[:10000]\n",
    "y_test  = y_test[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "# from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# # Text model\n",
    "# text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "# text_lstm = LSTM(\n",
    "#     input_shape=(MAX_LEN, 1024,),\n",
    "#     units=512,\n",
    "#     recurrent_dropout=0.2,\n",
    "#     dropout=0.2)(elmo_embedding)\n",
    "\n",
    "# text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "# text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "\n",
    "# text_model = Model(\n",
    "#     inputs=text_input, \n",
    "#     outputs=text_output\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.0005)\n",
    "\n",
    "# text_model.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer=adam,\n",
    "#     metrics=[\n",
    "#         precision_m,\n",
    "#         recall_m,\n",
    "#         f1_m,\n",
    "#         'mae',\n",
    "#         abs_KL_div,\n",
    "#         'accuracy'\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample and Class Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample or class weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.33963712e-07, 1.33963712e-07, 4.93465951e-06, ...,\n",
       "        7.46035101e-06, 1.33963712e-07, 5.51851358e-06]),\n",
       " array([4.44153539e-05, 6.64109379e-06, 1.43860647e-04, ...,\n",
       "        1.34149824e-07, 1.34149824e-07, 1.34149824e-07]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test)\n",
    "\n",
    "train_sample_weights, test_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(16, 1.0),\n",
       "  (5, 1.1167699782208076),\n",
       "  (12, 1.2632177373507676),\n",
       "  (17, 1.5067811934900543),\n",
       "  (10, 1.7714589423332447),\n",
       "  (8, 1.8521811614337316),\n",
       "  (20, 1.8547579298831385),\n",
       "  (9, 1.900769888793841),\n",
       "  (13, 2.007831325301205),\n",
       "  (7, 2.058042605742513),\n",
       "  (22, 2.0850797622771347),\n",
       "  (14, 2.1551891367604266),\n",
       "  (11, 2.2727582679849982),\n",
       "  (23, 2.275085324232082),\n",
       "  (1, 2.3291404612159328),\n",
       "  (6, 2.4516366311143805),\n",
       "  (21, 2.5221339387060158),\n",
       "  (2, 2.5618754803996926),\n",
       "  (15, 2.812658227848101),\n",
       "  (3, 2.832979175520612),\n",
       "  (0, 2.836595744680851),\n",
       "  (18, 3.099023709902371),\n",
       "  (4, 3.126641651031895),\n",
       "  (19, 3.7789115646258504)],\n",
       " [(16, 1.0),\n",
       "  (5, 1.1710296684118673),\n",
       "  (12, 1.2244525547445255),\n",
       "  (17, 1.5354691075514875),\n",
       "  (10, 1.6650124069478909),\n",
       "  (9, 1.7845744680851063),\n",
       "  (20, 1.7941176470588236),\n",
       "  (8, 1.8383561643835618),\n",
       "  (7, 1.8795518207282913),\n",
       "  (22, 1.9226361031518624),\n",
       "  (13, 2.185667752442997),\n",
       "  (14, 2.185667752442997),\n",
       "  (23, 2.2745762711864406),\n",
       "  (6, 2.329861111111111),\n",
       "  (11, 2.3461538461538463),\n",
       "  (1, 2.5225563909774436),\n",
       "  (21, 2.727642276422764),\n",
       "  (0, 2.807531380753138),\n",
       "  (2, 2.8553191489361702),\n",
       "  (18, 2.982222222222222),\n",
       "  (3, 3.008968609865471),\n",
       "  (15, 3.1650943396226414),\n",
       "  (4, 3.388888888888889),\n",
       "  (19, 3.769662921348315)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a Log for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Model using `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), \n",
    "            tf.float32)\n",
    "        return nonzero / y_true.get_shape()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment loop...\n",
      "\n",
      "16 total iterations...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "## Model imports\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import ( Input, Embedding, GlobalMaxPooling1D, Conv1D, Dense, Activation, \n",
    "                                      Dropout, Lambda, BatchNormalization, concatenate )\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Fit imports\n",
    "from tensorflow.keras.losses import hinge, mae, binary_crossentropy, kld, Huber, squared_hinge\n",
    "\n",
    "# Hyperparameter/method search space\n",
    "import itertools\n",
    "\n",
    "# For 4 GPUs\n",
    "DIST_BATCH_SIZE = int(BATCH_SIZE/4)\n",
    "EPOCHS = 4\n",
    "\n",
    "\n",
    "print('Starting experiment loop...')\n",
    "\n",
    "EXPERIMENT_NAME = 'kld dense dims x filter_lengths x adam/sgd'\n",
    "learning_rates = [0.005]\n",
    "losses = [kld] #binary_crossentropy, kld, hinge, mae]\n",
    "activations = ['selu'] # 'selu'\n",
    "optimizers = ['adam', 'sgd']\n",
    "dropout_ratios = [0.2]\n",
    "filter_lengths = [64, 128]\n",
    "class_weight_set = [train_class_weights]\n",
    "sample_weight_set = [train_sample_weights]\n",
    "test_sample_weight_set = [None] #, test_sample_weights]\n",
    "dense_dims = [16, 32, 64, 128]\n",
    "\n",
    "args = list(itertools.product(\n",
    "    learning_rates,\n",
    "    losses,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    dropout_ratios,\n",
    "    filter_lengths,\n",
    "    class_weight_set,\n",
    "    sample_weight_set,\n",
    "    test_sample_weight_set,\n",
    "    dense_dims\n",
    "))\n",
    "print()\n",
    "print(f'{len(args):,} total iterations...')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/rjurney/weakly-supervised-learning/runs/mfy8r0qb?apiKey=0ac3e7e868cd69f2f6097199b31256021b9af259\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "# tqdm_notebook\n",
    "for learning_rate, loss_function, activation, optimizer, dropout_ratio, filter_length, class_weights, \\\n",
    "    sample_weights, test_sample_weights, dense_dim in args:\n",
    "    \n",
    "    cw_label  = 'class_weights' if isinstance(class_weights, dict) else 'no_class_weights'\n",
    "    sw_label  = 'sample_weights' if isinstance(sample_weights, np.ndarray) else 'no_sample_weights'\n",
    "    tsw_label = 'test_sample_weights' if isinstance(test_sample_weights, list) else 'no_test_sample_weights'\n",
    "    \n",
    "    model_name = str(loss_function.__name__) + ' ' + str(learning_rate) + ' ' + str(optimizer) + ' ' + \\\n",
    "                 str(activation) + ' ' + str(EPOCHS) + ' ' + cw_label + ' ' + sw_label + ' ' + tsw_label + ' ' + \\\n",
    "                 str(dense_dim)\n",
    "    \n",
    "    # Log wandb config\n",
    "    config.update(\n",
    "        {\n",
    "            'class_weights': cw_label,\n",
    "            'sample_weights': sw_label,\n",
    "            'test_sample_weights': tsw_label,\n",
    "        },\n",
    "        allow_val_change=True\n",
    "    )\n",
    "    \n",
    "    print(model_name)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #\n",
    "    # Build ze model...\n",
    "    #\n",
    "    def build_model(\n",
    "        token_count=None,\n",
    "        max_words=None,\n",
    "        embedding_dim=None,\n",
    "        label_count=None,\n",
    "        dropout_ratio=None,\n",
    "        filter_length=None,\n",
    "        loss_function=None,\n",
    "        learning_rate=None,\n",
    "        optimizer=None,\n",
    "        activation=None,\n",
    "        dense_dim=None,\n",
    "    ):\n",
    "        \"\"\"Build the model using this experiment's parameters\"\"\"\n",
    "        \n",
    "        # Store config in wandb\n",
    "        config.update(\n",
    "            {\n",
    "                'token_count': token_count,\n",
    "                'max_words': max_words,\n",
    "                'embedding_dim': embedding_dim,\n",
    "                'label_count': label_count,\n",
    "                'dropout_ratio': dropout_ratio,\n",
    "                'filter_length': filter_length,\n",
    "                'loss_function': loss_function.__name__,\n",
    "                'learning_rate': learning_rate,\n",
    "                'optimizer': optimizer,\n",
    "                'activation': activation,\n",
    "                'dense_dim': dense_dim,\n",
    "            },\n",
    "            allow_val_change=True\n",
    "        )\n",
    "        \n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\"])\n",
    "        with mirrored_strategy.scope():\n",
    "            \n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        \n",
    "            hashed_input = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "\n",
    "            emb = Embedding(token_count, embedding_dim, weights=[embedding_matrix])(hashed_input)\n",
    "\n",
    "            # Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "            conv1_1 = Conv1D(filters=filter_length, kernel_size=3)(emb)\n",
    "            btch1_1 = BatchNormalization()(conv1_1)\n",
    "            drp1_1  = Dropout(dropout_ratio)(btch1_1)\n",
    "            actv1_1 = Activation(activation)(drp1_1)\n",
    "            glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "            conv1_2 = Conv1D(filters=filter_length, kernel_size=4)(emb)\n",
    "            btch1_2 = BatchNormalization()(conv1_2)\n",
    "            drp1_2  = Dropout(dropout_ratio)(btch1_2)\n",
    "            actv1_2 = Activation(activation)(drp1_2)\n",
    "            glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "            conv1_3 = Conv1D(filters=filter_length, kernel_size=5)(emb)\n",
    "            btch1_3 = BatchNormalization()(conv1_3)\n",
    "            drp1_3  = Dropout(dropout_ratio)(btch1_3)\n",
    "            actv1_3 = Activation(activation)(drp1_3)\n",
    "            glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "            conv1_4 = Conv1D(filters=filter_length, kernel_size=6)(emb)\n",
    "            btch1_4 = BatchNormalization()(conv1_4)\n",
    "            drp1_4  = Dropout(dropout_ratio)(btch1_4)\n",
    "            actv1_4 = Activation(activation)(drp1_4)\n",
    "            glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "            # Gather all convolution layers\n",
    "            cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "            drp1 = Dropout(dropout_ratio)(cnct)\n",
    "\n",
    "            dns1  = Dense(dense_dim, activation=activation)(drp1)\n",
    "            btch1 = BatchNormalization()(dns1)\n",
    "            drp2  = Dropout(dropout_ratio)(btch1)\n",
    "\n",
    "            out = Dense(y_train.shape[1], activation='sigmoid')(drp2)\n",
    "\n",
    "            text_model = Model(\n",
    "                inputs=hashed_input, \n",
    "                outputs=out\n",
    "            )\n",
    "\n",
    "            if activation == 'adam':\n",
    "                activation = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "            if activation == 'sgd':\n",
    "                activation = SGD(lr=learning_rate)\n",
    "\n",
    "            text_model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function,\n",
    "                metrics=[\n",
    "                    'categorical_accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.BinaryAccuracy(),\n",
    "                    tf.keras.metrics.Hinge(),\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.Accuracy(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError(),\n",
    "                    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                    tf.keras.metrics.TruePositives(),\n",
    "                    tf.keras.metrics.FalsePositives(),\n",
    "                    tf.keras.metrics.TrueNegatives(),\n",
    "                    tf.keras.metrics.FalseNegatives()\n",
    "                ]\n",
    "            )\n",
    "            #text_model.summary()\n",
    "\n",
    "            return text_model\n",
    "\n",
    "    #\n",
    "    # Train ze model...\n",
    "    #\n",
    "    def train_model(\n",
    "        model=None,\n",
    "        X_train=None,\n",
    "        X_test=None,\n",
    "        learning_rate=None,\n",
    "        loss_function=None,\n",
    "        optimizer=None,\n",
    "        activation=None,\n",
    "        epochs=None,\n",
    "        class_weights=None,\n",
    "        sample_weights=None,\n",
    "        test_sample_weights=None,\n",
    "    ):\n",
    "        \"\"\"Train the model using the current parameters and evaluate performance\"\"\"\n",
    "        \n",
    "        # Log wandb config\n",
    "        config.update(\n",
    "            { 'epochs': epochs },\n",
    "            allow_val_change=True,\n",
    "        )\n",
    "        \n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(\n",
    "                patience=1,\n",
    "                verbose=1,\n",
    "                min_delta=0.001,\n",
    "                min_lr=0.0005,\n",
    "            ), \n",
    "            EarlyStopping(\n",
    "                patience=2,\n",
    "                min_delta=0.001,\n",
    "                verbose=1,\n",
    "                restore_best_weights=True\n",
    "            ), \n",
    "            WandbCallback()\n",
    "            #ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = text_model.fit(\n",
    "            X_train, \n",
    "            y_train,\n",
    "            class_weight=class_weights,\n",
    "            sample_weight=sample_weights,\n",
    "            epochs=epochs,\n",
    "            batch_size=DIST_BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "        # Evaluate to our log and return a description key and a list of metrics\n",
    "        accr = text_model.evaluate(X_test, y_test, sample_weight=test_sample_weights)\n",
    "        f1_score = 2.0 * (accr[1] * accr[2]) / \\\n",
    "                         (accr[1] + accr[2])\n",
    "        return_val = [i for i in zip([j.item() for j in accr + [f1_score]], text_model.metrics_names + ['val_f1_score'])]\n",
    "\n",
    "        return return_val\n",
    "\n",
    "    #\n",
    "    # main()\n",
    "    #\n",
    "    text_model = build_model(\n",
    "        token_count=TOKEN_COUNT,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        filter_length=128,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        dropout_ratio=dropout_ratio\n",
    "    )\n",
    "    try:\n",
    "        accuracies = train_model(\n",
    "            model=text_model,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            activation=activation,\n",
    "            epochs=EPOCHS,\n",
    "            class_weights=class_weights,\n",
    "            sample_weights=sample_weights,\n",
    "            test_sample_weights=test_sample_weights,\n",
    "        )\n",
    "\n",
    "        log_record = (model_name, accuracies)\n",
    "        performance_log.append(log_record)\n",
    "\n",
    "        with open('data/performance_log.jsonl', 'w') as f:\n",
    "            for record in performance_log:\n",
    "                f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "        print(log_record)\n",
    "        sys.stdout.flush()\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Aboring training run!')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print('Completed experiment loop!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test) #, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['precision_m'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        scores = []\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            j_score = jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                j_score\n",
    "            ))\n",
    "            scores.append(j_score)\n",
    "        print('')\n",
    "        mean_score = statistics.mean(scores)\n",
    "        if mean_score > max_score:\n",
    "            best_cutoff = cutoff\n",
    "            max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
