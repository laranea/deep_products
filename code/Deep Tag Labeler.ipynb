{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000 and 10,000 instances that reduced the maximum imbalance from 100-1000:1 to 8:1 using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_tags = json.load(open('data/stackoverflow/08-05-2019/sorted_all_tags.50000.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>label_20</th>\n",
       "      <th>label_21</th>\n",
       "      <th>label_22</th>\n",
       "      <th>label_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C, Mono, Winforms, MessageBox, problem, I, fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Are, NET, data, providers, Oracle, require, O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[How, I, focus, foreign, window, I, applicatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Default, button, hit, windows, forms, trying,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Can, I, avoid, JIT, net, Say, code, always, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  label_0  label_1  \\\n",
       "0  [C, Mono, Winforms, MessageBox, problem, I, fi...        1        0   \n",
       "1  [Are, NET, data, providers, Oracle, require, O...        1        0   \n",
       "2  [How, I, focus, foreign, window, I, applicatio...        1        0   \n",
       "3  [Default, button, hit, windows, forms, trying,...        1        0   \n",
       "4  [Can, I, avoid, JIT, net, Say, code, always, g...        1        0   \n",
       "\n",
       "   label_2  label_3  label_4  label_5  label_6  label_7  label_8  ...  \\\n",
       "0        0        0        0        1        0        0        0  ...   \n",
       "1        0        0        0        0        0        0        0  ...   \n",
       "2        0        0        0        1        0        0        0  ...   \n",
       "3        0        0        0        1        0        0        0  ...   \n",
       "4        0        0        0        0        0        0        0  ...   \n",
       "\n",
       "   label_14  label_15  label_16  label_17  label_18  label_19  label_20  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   label_21  label_22  label_23  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         0         0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/08-05-2019/Questions.Stratified.Final.50000.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,293,018\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,}'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open('data/stackoverflow/08-05-2019/tag_index.50000.json'))\n",
    "index_tag = json.load(open('data/stackoverflow/08-05-2019/index_tag.50000.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = json.load(open('data/stackoverflow/08-05-2019/label_counts.50000.json'))\n",
    "\n",
    "# Sanity check the difference files\n",
    "assert(len(label_counts.keys()) == len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 404 Training Count: 1,292,800\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "# Remove stopwords - now done in Spark, so can remove once that runs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [' '.join(x) for x in posts_text]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "# elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())\n",
    "\n",
    "# def ElmoEmbedding(x):\n",
    "#     return elmo_model(inputs={\n",
    "#                             \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "#                             \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "#                       },\n",
    "#                       signature=\"tokens\",\n",
    "#                       as_dict=True)[\"elmo\"]\n",
    "\n",
    "# text_input = Input(shape=(max_len,), dtype=tf.string)\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Glove Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1292800, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKEN_COUNT)\n",
    "tokenizer.fit_on_texts(documents)\n",
    "# encoded_docs = tokenizer.texts_to_matrix(posts_text, mode='tfidf')\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='pre',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "print(max([len(x) for x in padded_sequences]), min([len(x) for x in padded_sequences]))\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]))\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  71, 3402, 2170, ...,    1,    1,    1],\n",
       "       [  54,  170,   34, ...,    1,    1,    1],\n",
       "       [  21,    2, 1478, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [   2,   13,   35, ...,  956,    1,    1],\n",
       "       [ 119,  462,   32, ...,    1,    1,    1],\n",
       "       [   2,   25,   44, ...,    1,    1,    1]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     posts_text,\n",
    "#     labels,\n",
    "#     test_size=TEST_SPLIT,\n",
    "#     random_state=1337\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "# from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# # Text model\n",
    "# text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "# elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "# text_lstm = LSTM(\n",
    "#     input_shape=(MAX_LEN, 1024,),\n",
    "#     units=512,\n",
    "#     recurrent_dropout=0.2,\n",
    "#     dropout=0.2)(elmo_embedding)\n",
    "\n",
    "# text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "# text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "\n",
    "# text_model = Model(\n",
    "#     inputs=text_input, \n",
    "#     outputs=text_output\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.0005)\n",
    "\n",
    "# text_model.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer=adam,\n",
    "#     metrics=[\n",
    "#         precision_m,\n",
    "#         recall_m,\n",
    "#         f1_m,\n",
    "#         'mae',\n",
    "#         abs_KL_div,\n",
    "#         'accuracy'\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample and Class Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample or class weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.35704997e-03, 4.91477790e-06, 3.46168426e-06, ...,\n",
       "        4.43082862e-06, 1.30453968e-07, 1.30453968e-07]),\n",
       " array([1.30316677e-07, 1.15411547e-04, 1.30316677e-07, ...,\n",
       "        1.30316677e-07, 1.38388975e-04, 1.79070271e-06]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test)\n",
    "\n",
    "train_sample_weights, test_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(16, 1.0),\n",
       "  (5, 1.174390280129519),\n",
       "  (12, 1.3624019987938314),\n",
       "  (17, 1.620059420141379),\n",
       "  (20, 1.8687102644702323),\n",
       "  (10, 1.8711868417938706),\n",
       "  (8, 2.009173379411989),\n",
       "  (9, 2.0102461100376283),\n",
       "  (7, 2.147567699703941),\n",
       "  (22, 2.1867688137843295),\n",
       "  (13, 2.188281855419019),\n",
       "  (14, 2.292860457023547),\n",
       "  (11, 2.338430143144446),\n",
       "  (1, 2.3760254830663823),\n",
       "  (23, 2.4299895506792057),\n",
       "  (6, 2.6281203257437262),\n",
       "  (21, 2.647214410070979),\n",
       "  (2, 2.666767850517724),\n",
       "  (15, 2.9316648127549128),\n",
       "  (0, 3.058921386567626),\n",
       "  (3, 3.0622385747482572),\n",
       "  (18, 3.2606293043012085),\n",
       "  (4, 3.4826674888781217),\n",
       "  (19, 4.0100928133083125)],\n",
       " [(16, 1.0),\n",
       "  (5, 1.1816222042576126),\n",
       "  (12, 1.3517262638717633),\n",
       "  (17, 1.564116283217407),\n",
       "  (10, 1.8412765064035272),\n",
       "  (20, 1.8823781927452243),\n",
       "  (9, 1.9712294897729827),\n",
       "  (8, 1.9954493742889647),\n",
       "  (13, 2.1750992063492065),\n",
       "  (7, 2.1821348594177654),\n",
       "  (22, 2.1897627965043696),\n",
       "  (14, 2.294610151753009),\n",
       "  (23, 2.370911057042444),\n",
       "  (1, 2.379918588873813),\n",
       "  (11, 2.3805646036916395),\n",
       "  (6, 2.5931401537551744),\n",
       "  (21, 2.688534641324341),\n",
       "  (2, 2.7151702786377707),\n",
       "  (15, 3.0199724517906334),\n",
       "  (0, 3.102228510788822),\n",
       "  (3, 3.1867732558139537),\n",
       "  (18, 3.2254505332842958),\n",
       "  (4, 3.428459734167318),\n",
       "  (19, 4.113508442776736)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a Log for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Model using `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), \n",
    "            tf.float32)\n",
    "        return nonzero / y_true.get_shape()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment loop...\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 100, 50)      500000      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 98, 128)      19328       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 97, 128)      25728       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 96, 128)      32128       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 95, 128)      38528       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 98, 128)      512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 97, 128)      512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 96, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 95, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 98, 128)      0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 97, 128)      0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 96, 128)      0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 95, 128)      0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 98, 128)      0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 97, 128)      0           dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 96, 128)      0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 95, 128)      0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 128)          0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 128)          0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 128)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 512)          0           global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "                                                                 global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 512)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           16416       dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32)           128         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 32)           0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 24)           792         dropout_47[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights\n",
      "Train on 1163520 samples, validate on 129280 samples\n",
      "1163520/1163520 [==============================] - 624s 537us/sample - loss: 0.0789 - categorical_accuracy: 0.2552 - precision_7: 0.6610 - recall_7: 0.3788 - binary_accuracy: 0.9739 - hinge: 1.0099 - auc_7: 0.9334 - accuracy: 7.0404e-05 - mean_absolute_error: 0.0420 - mean_absolute_percentage_error: 22749130.0000 - true_positives_7: 338907.0000 - false_positives_7: 173812.0000 - true_negatives_7: 26856064.0000 - false_negatives_7: 555703.0000 - val_loss: 0.0584 - val_categorical_accuracy: 0.3065 - val_precision_7: 0.8611 - val_recall_7: 0.3936 - val_binary_accuracy: 0.9786 - val_hinge: 0.9992 - val_auc_7: 0.9690 - val_accuracy: 0.0000e+00 - val_mean_absolute_error: 0.0312 - val_mean_absolute_percentage_error: 12390827.0000 - val_true_positives_7: 39070.0000 - val_false_positives_7: 6300.0000 - val_true_negatives_7: 2997155.0000 - val_false_negatives_7: 60195.0000\n",
      "129280/129280 [==============================] - 21s 164us/sample - loss: 0.0584 - categorical_accuracy: 0.3065 - precision_7: 0.8611 - recall_7: 0.3936 - binary_accuracy: 0.9786 - hinge: 0.9992 - auc_7: 0.9690 - accuracy: 0.0000e+00 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 12390799.0000 - true_positives_7: 39070.0000 - false_positives_7: 6300.0000 - true_negatives_7: 2997155.0000 - false_negatives_7: 60195.0000\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights [(0.05836321952715885, 'loss'), (0.30645111203193665, 'categorical_accuracy'), (0.8611417412757874, 'precision_7'), (0.393592894077301, 'recall_7'), (0.9785681366920471, 'binary_accuracy'), (0.9991918802261353, 'hinge'), (0.9690455794334412, 'auc_7'), (0.0, 'accuracy'), (0.031184330582618713, 'mean_absolute_error'), (12390799.0, 'mean_absolute_percentage_error'), (39070.0, 'true_positives_7'), (6300.0, 'false_positives_7'), (2997155.0, 'true_negatives_7'), (60195.0, 'false_negatives_7'), (0.4520374182230622, 'val_f1_score'), ('name', '<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights')]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 100, 50)      500000      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 98, 128)      19328       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 97, 128)      25728       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 96, 128)      32128       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 95, 128)      38528       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 98, 128)      512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 97, 128)      512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 96, 128)      512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 95, 128)      512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 98, 128)      0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 97, 128)      0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 96, 128)      0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 95, 128)      0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 98, 128)      0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 97, 128)      0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 96, 128)      0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 95, 128)      0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 128)          0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 128)          0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 128)          0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 128)          0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512)          0           global_max_pooling1d_32[0][0]    \n",
      "                                                                 global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 512)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 32)           16416       dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32)           128         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 32)           0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 24)           792         dropout_53[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights sample_weights\n",
      "Train on 1163520 samples, validate on 129280 samples\n",
      "1163520/1163520 [==============================] - 628s 540us/sample - loss: 3.5176e-04 - categorical_accuracy: 0.0351 - precision_8: 0.0683 - recall_8: 0.2295 - binary_accuracy: 0.8751 - hinge: 1.1789 - auc_8: 0.5924 - accuracy: 0.0000e+00 - mean_absolute_error: 0.2109 - mean_absolute_percentage_error: 188381680.0000 - true_positives_8: 205274.0000 - false_positives_8: 2799298.0000 - true_negatives_8: 24230608.0000 - false_negatives_8: 689336.0000 - val_loss: 0.3105 - val_categorical_accuracy: 0.0307 - val_precision_8: 0.0604 - val_recall_8: 0.2295 - val_binary_accuracy: 0.8611 - val_hinge: 1.1485 - val_auc_8: 0.6255 - val_accuracy: 7.7351e-06 - val_mean_absolute_error: 0.1805 - val_mean_absolute_percentage_error: 157518880.0000 - val_true_positives_8: 22782.0000 - val_false_positives_8: 354367.0000 - val_true_negatives_8: 2649088.0000 - val_false_negatives_8: 76483.0000\n",
      "129280/129280 [==============================] - 21s 164us/sample - loss: 0.3105 - categorical_accuracy: 0.0307 - precision_8: 0.0604 - recall_8: 0.2295 - binary_accuracy: 0.8611 - hinge: 1.1485 - auc_8: 0.6255 - accuracy: 7.7351e-06 - mean_absolute_error: 0.1805 - mean_absolute_percentage_error: 157518512.0000 - true_positives_8: 22782.0000 - false_positives_8: 354367.0000 - true_negatives_8: 2649088.0000 - false_negatives_8: 76483.0000\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights sample_weights [(0.31047432633245936, 'loss'), (0.03072400949895382, 'categorical_accuracy'), (0.06040583550930023, 'precision_8'), (0.22950688004493713, 'recall_8'), (0.8611388206481934, 'binary_accuracy'), (1.1485443115234375, 'hinge'), (0.6254979372024536, 'auc_8'), (7.735148756182753e-06, 'accuracy'), (0.1805381029844284, 'mean_absolute_error'), (157518512.0, 'mean_absolute_percentage_error'), (22782.0, 'true_positives_8'), (354367.0, 'false_positives_8'), (2649088.0, 'true_negatives_8'), (76483.0, 'false_negatives_8'), (0.040731100486859836, 'val_f1_score'), ('name', '<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights')]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 100, 50)      500000      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 98, 128)      19328       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 97, 128)      25728       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 96, 128)      32128       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 95, 128)      38528       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 98, 128)      512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 97, 128)      512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 96, 128)      512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 95, 128)      512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 98, 128)      0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 97, 128)      0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 96, 128)      0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 95, 128)      0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 98, 128)      0           dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 97, 128)      0           dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 96, 128)      0           dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 95, 128)      0           dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_36 (Global (None, 128)          0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_37 (Global (None, 128)          0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_38 (Global (None, 128)          0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_39 (Global (None, 128)          0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_36[0][0]    \n",
      "                                                                 global_max_pooling1d_37[0][0]    \n",
      "                                                                 global_max_pooling1d_38[0][0]    \n",
      "                                                                 global_max_pooling1d_39[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 32)           16416       dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32)           128         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 32)           0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 24)           792         dropout_59[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 class_weights no_sample_weights\n",
      "Train on 1163520 samples, validate on 129280 samples\n",
      "1163520/1163520 [==============================] - 632s 543us/sample - loss: 0.1787 - categorical_accuracy: 0.2543 - precision_9: 0.6586 - recall_9: 0.3672 - binary_accuracy: 0.9736 - hinge: 1.0086 - auc_9: 0.9303 - accuracy: 1.4339e-04 - mean_absolute_error: 0.0407 - mean_absolute_percentage_error: 21150342.0000 - true_positives_9: 328470.0000 - false_positives_9: 170257.0000 - true_negatives_9: 26859602.0000 - false_negatives_9: 566140.0000 - val_loss: 0.0581 - val_categorical_accuracy: 0.3149 - val_precision_9: 0.8599 - val_recall_9: 0.3934 - val_binary_accuracy: 0.9785 - val_hinge: 0.9981 - val_auc_9: 0.9683 - val_accuracy: 3.2230e-07 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 11602047.0000 - val_true_positives_9: 39050.0000 - val_false_positives_9: 6362.0000 - val_true_negatives_9: 2997093.0000 - val_false_negatives_9: 60215.0000\n",
      "129280/129280 [==============================] - 21s 165us/sample - loss: 0.0581 - categorical_accuracy: 0.3149 - precision_9: 0.8599 - recall_9: 0.3934 - binary_accuracy: 0.9785 - hinge: 0.9981 - auc_9: 0.9683 - accuracy: 3.2230e-07 - mean_absolute_error: 0.0301 - mean_absolute_percentage_error: 11602076.0000 - true_positives_9: 39050.0000 - false_positives_9: 6362.0000 - true_negatives_9: 2997093.0000 - false_negatives_9: 60215.0000\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 class_weights no_sample_weights [(0.058069217937196244, 'loss'), (0.3149442970752716, 'categorical_accuracy'), (0.8599048852920532, 'precision_9'), (0.3933914303779602, 'recall_9'), (0.9785423278808594, 'binary_accuracy'), (0.9981111884117126, 'hinge'), (0.968305230140686, 'auc_9'), (3.22297864840948e-07, 'accuracy'), (0.03010517917573452, 'mean_absolute_error'), (11602076.0, 'mean_absolute_percentage_error'), (39050.0, 'true_positives_9'), (6362.0, 'false_positives_9'), (2997093.0, 'true_negatives_9'), (60215.0, 'false_negatives_9'), (0.4610330390126215, 'val_f1_score'), ('name', '<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights')]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 100, 50)      500000      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 98, 128)      19328       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 97, 128)      25728       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 96, 128)      32128       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 95, 128)      38528       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 98, 128)      512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 97, 128)      512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 96, 128)      512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 95, 128)      512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 98, 128)      0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 97, 128)      0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 96, 128)      0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 95, 128)      0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 98, 128)      0           dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 97, 128)      0           dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 96, 128)      0           dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 95, 128)      0           dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_40 (Global (None, 128)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_41 (Global (None, 128)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_42 (Global (None, 128)          0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_43 (Global (None, 128)          0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_40[0][0]    \n",
      "                                                                 global_max_pooling1d_41[0][0]    \n",
      "                                                                 global_max_pooling1d_42[0][0]    \n",
      "                                                                 global_max_pooling1d_43[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           16416       dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32)           128         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 32)           0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 24)           792         dropout_65[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 class_weights sample_weights\n",
      "Train on 1163520 samples, validate on 129280 samples\n",
      "1163520/1163520 [==============================] - 636s 547us/sample - loss: 9.5493e-04 - categorical_accuracy: 0.0344 - precision_10: 0.0620 - recall_10: 0.2217 - binary_accuracy: 0.8676 - hinge: 1.2018 - auc_10: 0.5720 - accuracy: 0.0000e+00 - mean_absolute_error: 0.2338 - mean_absolute_percentage_error: 211495984.0000 - true_positives_10: 198331.0000 - false_positives_10: 3000301.0000 - true_negatives_10: 24029548.0000 - false_negatives_10: 696279.0000 - val_loss: 0.2858 - val_categorical_accuracy: 0.0381 - val_precision_10: 0.1032 - val_recall_10: 0.2521 - val_binary_accuracy: 0.9060 - val_hinge: 1.1582 - val_auc_10: 0.6391 - val_accuracy: 0.0000e+00 - val_mean_absolute_error: 0.1902 - val_mean_absolute_percentage_error: 168526816.0000 - val_true_positives_10: 25025.0000 - val_false_positives_10: 217407.0000 - val_true_negatives_10: 2786048.0000 - val_false_negatives_10: 74240.0000\n",
      "129280/129280 [==============================] - 21s 165us/sample - loss: 0.2858 - categorical_accuracy: 0.0381 - precision_10: 0.1032 - recall_10: 0.2521 - binary_accuracy: 0.9060 - hinge: 1.1582 - auc_10: 0.6391 - accuracy: 0.0000e+00 - mean_absolute_error: 0.1902 - mean_absolute_percentage_error: 168526336.0000 - true_positives_10: 25025.0000 - false_positives_10: 217407.0000 - true_negatives_10: 2786048.0000 - false_negatives_10: 74240.0000\n",
      "<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 class_weights sample_weights [(0.28580092568651283, 'loss'), (0.03812654688954353, 'categorical_accuracy'), (0.10322482138872147, 'precision_10'), (0.25210297107696533, 'recall_10'), (0.906003475189209, 'binary_accuracy'), (1.1581953763961792, 'hinge'), (0.6391046047210693, 'auc_10'), (0.0, 'accuracy'), (0.19018904864788055, 'mean_absolute_error'), (168526336.0, 'mean_absolute_percentage_error'), (25025.0, 'true_positives_10'), (217407.0, 'false_positives_10'), (2786048.0, 'true_negatives_10'), (74240.0, 'false_negatives_10'), (0.055685433584266385, 'val_f1_score'), ('name', '<function binary_crossentropy at 0x7f053562eea0> 0.01 adam selu 1 no_class_weights no_sample_weights')]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 100, 50)      500000      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 98, 128)      19328       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 97, 128)      25728       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 96, 128)      32128       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 95, 128)      38528       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 98, 128)      512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 97, 128)      512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 96, 128)      512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 95, 128)      512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 98, 128)      0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 97, 128)      0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 96, 128)      0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 95, 128)      0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 98, 128)      0           dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 97, 128)      0           dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 96, 128)      0           dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 95, 128)      0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_44 (Global (None, 128)          0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_45 (Global (None, 128)          0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_46 (Global (None, 128)          0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_47 (Global (None, 128)          0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_44[0][0]    \n",
      "                                                                 global_max_pooling1d_45[0][0]    \n",
      "                                                                 global_max_pooling1d_46[0][0]    \n",
      "                                                                 global_max_pooling1d_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           16416       dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32)           128         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 32)           0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 24)           792         dropout_71[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 635,096\n",
      "Trainable params: 634,008\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "<function hinge at 0x7f053562eb70> 0.01 adam selu 1 no_class_weights no_sample_weights\n",
      "Train on 1163520 samples, validate on 129280 samples\n",
      " 394624/1163520 [=========>....................] - ETA: 6:41 - loss: 1.0069 - categorical_accuracy: 0.0795 - precision_11: 0.0432 - recall_11: 0.0061 - binary_accuracy: 0.9638 - hinge: 1.0069 - auc_11: 0.5010 - accuracy: 0.0076 - mean_absolute_error: 0.0390 - mean_absolute_percentage_error: 7148244.0000 - true_positives_11: 1860.0000 - false_positives_11: 41179.0000 - true_negatives_11: 9125829.0000 - false_negatives_11: 302108.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c242436cf9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtest_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m     \u001b[0mperformance_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdescription_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-c242436cf9ce>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dropout_ratio, learning_rate, optimizer, activation, epochs, class_weights, sample_weights, test_sample_weights)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model imports\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import ( Input, Embedding, GlobalMaxPooling1D, Conv1D, Dense, Activation, \n",
    "                                      Dropout, Lambda, BatchNormalization, concatenate )\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "# Fit imports\n",
    "from tensorflow.keras.losses import hinge, mae, binary_crossentropy, kld, Huber, squared_hinge\n",
    "# from tensorflow_addons.losses.focal_loss import SigmoidFocalCrossEntropy\n",
    "# from tensorflow_addons.metrics.f_scores import F1Score\n",
    "\n",
    "# Hyperparameter/method search space\n",
    "import itertools\n",
    "\n",
    "print('Starting experiment loop...')\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005, 0.0001]# , 0.00005]\n",
    "losses = [binary_crossentropy, hinge, squared_hinge, mae, kld, Huber, hamming_loss]\n",
    "activations = ['selu']\n",
    "optimizers = ['adam']\n",
    "dropout_ratios = [0.2]\n",
    "filter_lengths = [128]\n",
    "class_weight_set = [None, train_class_weights]\n",
    "sample_weight_set = [None, train_sample_weights]\n",
    "test_sample_weight_set = [None] #, test_sample_weights]\n",
    "\n",
    "args = itertools.product(\n",
    "    learning_rates,\n",
    "    losses,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    dropout_ratios,\n",
    "    filter_lengths,\n",
    "    class_weight_set,\n",
    "    sample_weight_set,\n",
    "    test_sample_weight_set\n",
    ")\n",
    "\n",
    "performance_log = {}\n",
    "for learning_rate, loss_function, activation, optimizer, dropout_ratio, filter_length, class_weights, \\\n",
    "    sample_weights, test_sample_weights in args:\n",
    "    \n",
    "    #\n",
    "    # Build ze model...\n",
    "    #\n",
    "    def build_model(\n",
    "        token_count=20000,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        dropout_ratio=0.2,\n",
    "        filter_length=filter_length,\n",
    "        loss_function='binary_crossentropy',\n",
    "        learning_rate=0.001,\n",
    "        optimizer=Adam,\n",
    "        activation='relu'\n",
    "    ):\n",
    "        \"\"\"Build the model using this experiment's parameters\"\"\"\n",
    "        \n",
    "        hashed_input = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "        \n",
    "        emb = Embedding(token_count, embedding_dim, weights=[embedding_matrix])(hashed_input)\n",
    "\n",
    "        # Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "        conv1_1 = Conv1D(filters=filter_length, kernel_size=3)(emb)\n",
    "        btch1_1 = BatchNormalization()(conv1_1)\n",
    "        drp1_1  = Dropout(dropout_ratio)(btch1_1)\n",
    "        actv1_1 = Activation('relu')(drp1_1)\n",
    "        glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "        conv1_2 = Conv1D(filters=filter_length, kernel_size=4)(emb)\n",
    "        btch1_2 = BatchNormalization()(conv1_2)\n",
    "        drp1_2  = Dropout(dropout_ratio)(btch1_2)\n",
    "        actv1_2 = Activation('relu')(drp1_2)\n",
    "        glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "        conv1_3 = Conv1D(filters=filter_length, kernel_size=5)(emb)\n",
    "        btch1_3 = BatchNormalization()(conv1_3)\n",
    "        drp1_3  = Dropout(dropout_ratio)(btch1_3)\n",
    "        actv1_3 = Activation('relu')(drp1_3)\n",
    "        glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "        conv1_4 = Conv1D(filters=filter_length, kernel_size=6)(emb)\n",
    "        btch1_4 = BatchNormalization()(conv1_4)\n",
    "        drp1_4  = Dropout(dropout_ratio)(btch1_4)\n",
    "        actv1_4 = Activation('relu')(drp1_4)\n",
    "        glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "        # Gather all convolution layers\n",
    "        cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "        drp1 = Dropout(dropout_ratio)(cnct)\n",
    "\n",
    "        dns1  = Dense(32, activation='relu')(drp1)\n",
    "        btch1 = BatchNormalization()(dns1)\n",
    "        drp2  = Dropout(dropout_ratio)(btch1)\n",
    "\n",
    "        out = Dense(y_train.shape[1], activation='sigmoid')(drp2)\n",
    "\n",
    "        text_model = Model(\n",
    "            inputs=hashed_input, \n",
    "            outputs=out\n",
    "        )\n",
    "        \n",
    "        if activation == 'adam':\n",
    "            activation = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        if activation == 'sgd':\n",
    "            activation = SGD(lr=learning_rate)\n",
    "\n",
    "        text_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function,\n",
    "            metrics=[\n",
    "                'categorical_accuracy',\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.BinaryAccuracy(),\n",
    "                tf.keras.metrics.Hinge(),\n",
    "                tf.keras.metrics.AUC(),\n",
    "                tf.keras.metrics.Accuracy(),\n",
    "                tf.keras.metrics.MeanAbsoluteError(),\n",
    "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                tf.keras.metrics.TruePositives(),\n",
    "                tf.keras.metrics.FalsePositives(),\n",
    "                tf.keras.metrics.TrueNegatives(),\n",
    "                tf.keras.metrics.FalseNegatives()\n",
    "            ]\n",
    "        )\n",
    "        text_model.summary()\n",
    "\n",
    "        return text_model\n",
    "    \n",
    "    #\n",
    "    # Train ze model...\n",
    "    #\n",
    "    def train_model(\n",
    "        model=None,\n",
    "        dropout_ratio=0.1,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam',\n",
    "        activation='relu',\n",
    "        epochs=5,\n",
    "        class_weights=None,\n",
    "        sample_weights=None,\n",
    "        test_sample_weights=None\n",
    "    ):\n",
    "        \"\"\"Train the model using the current parameters and evaluate performance\"\"\"\n",
    "        \n",
    "        model_name = str(loss_function) + ' ' + str(learning_rate) + ' ' + str(optimizer) + ' ' + \\\n",
    "                     str(activation) + ' ' + str(epochs) + ' ' + \\\n",
    "                     ('class_weights' if isinstance(class_weights, dict) else 'no_class_weights') + ' ' + \\\n",
    "                     ('sample_weights' if isinstance(sample_weights, np.ndarray) else 'no_sample_weights')\n",
    "        print(model_name)\n",
    "        \n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(\n",
    "                patience=1,\n",
    "                verbose=1,\n",
    "                min_delta=0.001,\n",
    "                min_lr=0.0005,\n",
    "            ), \n",
    "            EarlyStopping(\n",
    "                patience=2,\n",
    "                min_delta=0.001,\n",
    "                verbose=1,\n",
    "                restore_best_weights=True\n",
    "            ), \n",
    "            #ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = text_model.fit(\n",
    "            X_train, \n",
    "            y_train,\n",
    "            class_weight=class_weights,\n",
    "            sample_weight=sample_weights,\n",
    "            epochs=epochs,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "        # Evaluate to our log and return a description key and a list of metrics\n",
    "        accr = text_model.evaluate(X_test, y_test)#, sample_weight=test_sample_weights)\n",
    "        f1_score = 2.0 * (accr[1] * accr[2]) / \\\n",
    "                         (accr[1] + accr[2])\n",
    "        return_val = model_name, [i for i in zip([j.item() for j in accr + [f1_score]], text_model.metrics_names + ['val_f1_score'])]\n",
    "\n",
    "        return return_val\n",
    "\n",
    "    #\n",
    "    # main()\n",
    "    #\n",
    "    text_model = build_model(\n",
    "        token_count=TOKEN_COUNT,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        filter_length=128,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        dropout_ratio=0.2\n",
    "    )\n",
    "\n",
    "    description_key, accuracies = train_model(\n",
    "        model=text_model,\n",
    "        dropout_ratio=dropout_ratio,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        epochs=1,\n",
    "        class_weights=class_weights,\n",
    "        sample_weights=sample_weights,\n",
    "        test_sample_weights=test_sample_weights,\n",
    "    )\n",
    "    performance_log[description_key] = accuracies\n",
    "    with open('data/performance_log.jsonl', 'w') as f:\n",
    "        for name, log in performance_log.items():\n",
    "            log.append(('name', key))\n",
    "            f.write(json.dumps(log) + '\\n')\n",
    "    \n",
    "    print(description_key, accuracies)\n",
    "    print()\n",
    "\n",
    "print('Completed experiment loop!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test) #, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['precision_m'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        scores = []\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            j_score = jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                j_score\n",
    "            ))\n",
    "            scores.append(j_score)\n",
    "        print('')\n",
    "        mean_score = statistics.mean(scores)\n",
    "        if mean_score > max_score:\n",
    "            best_cutoff = cutoff\n",
    "            max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
