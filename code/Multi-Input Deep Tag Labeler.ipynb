{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Products: Deep Tag Labeler\n",
    "\n",
    "This is the first project for the book Deep Products, about using NLP and weakly supervised learning to build complete machine learning products. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier using LSTMs and Emlo embeddings.\n",
    "\n",
    "## Multi-Input Strategy\n",
    "\n",
    "This is a second attempt using multiple balanced data inputs on multiple models that combine into the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 14 Million Answered Questions from Stack Overflow\n",
    "\n",
    "We load all answered questions from Stack Overflow. This data was converted from XML to JSON and then sampled using Spark on a single `r5.12xlarge` machine cluster with [code/stackoverflow/sample_json.spark.py](stackoverflow/sample_json.spark.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I want to use a track-bar to change a form'...</td>\n",
       "      <td>&lt;c#&gt;&lt;floating-point&gt;&lt;type-conversion&gt;&lt;double&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have an absolutely positioned &lt;code&gt;div&lt;/...</td>\n",
       "      <td>&lt;html&gt;&lt;css&gt;&lt;css3&gt;&lt;internet-explorer-7&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Given a &lt;code&gt;DateTime&lt;/code&gt; representing ...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;&lt;datetime&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Given a specific &lt;code&gt;DateTime&lt;/code&gt; valu...</td>\n",
       "      <td>&lt;c#&gt;&lt;datetime&gt;&lt;time&gt;&lt;datediff&gt;&lt;relative-time-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Is there a standard way for a web server to...</td>\n",
       "      <td>&lt;html&gt;&lt;browser&gt;&lt;timezone&gt;&lt;user-agent&gt;&lt;timezone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>I want to use a track-bar to change a form'...   \n",
       "1  <p>I have an absolutely positioned <code>div</...   \n",
       "2  <p>Given a <code>DateTime</code> representing ...   \n",
       "3  <p>Given a specific <code>DateTime</code> valu...   \n",
       "4  <p>Is there a standard way for a web server to...   \n",
       "\n",
       "                                               _Tags  \n",
       "0  <c#><floating-point><type-conversion><double><...  \n",
       "1             <html><css><css3><internet-explorer-7>  \n",
       "2                               <c#><.net><datetime>  \n",
       "3  <c#><datetime><time><datediff><relative-time-s...  \n",
       "4  <html><browser><timezone><user-agent><timezone...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/parquet/Questions.Answered.parquet',\n",
    "    columns=['_Body', '_Tags'],\n",
    "    filters=[('_Tags','!=',None),],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I want to use a track-bar to change a form'...</td>\n",
       "      <td>&lt;c#&gt;&lt;floating-point&gt;&lt;type-conversion&gt;&lt;double&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have an absolutely positioned &lt;code&gt;div&lt;/...</td>\n",
       "      <td>&lt;html&gt;&lt;css&gt;&lt;css3&gt;&lt;internet-explorer-7&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Given a &lt;code&gt;DateTime&lt;/code&gt; representing ...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;&lt;datetime&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Given a specific &lt;code&gt;DateTime&lt;/code&gt; valu...</td>\n",
       "      <td>&lt;c#&gt;&lt;datetime&gt;&lt;time&gt;&lt;datediff&gt;&lt;relative-time-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Is there a standard way for a web server to...</td>\n",
       "      <td>&lt;html&gt;&lt;browser&gt;&lt;timezone&gt;&lt;user-agent&gt;&lt;timezone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>I want to use a track-bar to change a form'...   \n",
       "1  <p>I have an absolutely positioned <code>div</...   \n",
       "2  <p>Given a <code>DateTime</code> representing ...   \n",
       "3  <p>Given a specific <code>DateTime</code> valu...   \n",
       "4  <p>Is there a standard way for a web server to...   \n",
       "\n",
       "                                               _Tags  \n",
       "0  <c#><floating-point><type-conversion><double><...  \n",
       "1             <html><css><css3><internet-explorer-7>  \n",
       "2                               <c#><.net><datetime>  \n",
       "3  <c#><datetime><time><datediff><relative-time-s...  \n",
       "4  <html><browser><timezone><user-agent><timezone...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = posts_df.head(1000000)\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unlabeled Posts\n",
    "\n",
    "Note: these have already been filtered to remove untagged questions, so there are from 1-5 labels per post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_posts = posts_df.dropna(axis=0, subset=['_Tags'])\n",
    "print('Posts w/ tags: {:,}'.format(len(tag_posts.index)))\n",
    "tag_posts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Tags from their XML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_posts['_Tag_List'] = tag_posts['_Tags'].apply(lambda x: re.findall('\\<(.+?)\\>', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Thresholds for Filtering Tags by Frequency\n",
    "\n",
    "The higher the threshold, the fewer classes, the less sparse the data, the easier the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23,795 tags with more than 0 count\n",
      "There are 9,706 tags with more than 10 count\n",
      "There are 6,660 tags with more than 20 count\n",
      "There are 3,901 tags with more than 50 count\n",
      "There are 2,508 tags with more than 100 count\n",
      "There are 416 tags with more than 1,000 count\n",
      "There are 70 tags with more than 5,000 count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for row in tag_posts['_Tag_List']:\n",
    "    for tag in row:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "for i in [0, 10, 20, 50, 100, 1000, 5000]:\n",
    "    filtered_tags = list(filter(lambda x: x > i, tag_counts.values()))\n",
    "    print('There are {:,} tags with more than {:,} count'.format(len(filtered_tags), i))\n",
    "\n",
    "MIN_TAGS = 5000\n",
    "\n",
    "record_count = len([i for i in filter(lambda x: x > MIN_TAGS, tag_counts.values())])\n",
    "record_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags with 5,000 occurrences: 70\n"
     ]
    }
   ],
   "source": [
    "all_tags = set()\n",
    "for row in tag_posts['_Tag_List']:\n",
    "    for tag in row:\n",
    "        if tag_counts[tag] > MIN_TAGS:\n",
    "            all_tags.add(tag)\n",
    "\n",
    "print('Total unique tags with {:,} occurrences: {:,}'.format(MIN_TAGS, len(all_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Tag Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 70)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "print(len(tag_posts.index))\n",
    "# Loop through every post...\n",
    "for i, tag_set in enumerate(tag_posts['_Tag_List'].tolist()):\n",
    "    # Then build a record_count element wide list for each tag present\n",
    "    label_row = []\n",
    "    for tag in tag_set:\n",
    "        if tag in all_tags:\n",
    "            label_row.append(tag)\n",
    "    labels.append(label_row)\n",
    "    \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "one_hot_labels = mlb.fit_transform(labels)\n",
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Rows Up Into Samples Per Label\n",
    "\n",
    "Create a dataset defined by an *index* of the `DataFrame` for each and every label column with balanced 0/1 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of the number of examples/tags and the total positives for each label\n",
    "length = one_hot_labels.shape[0]\n",
    "width = one_hot_labels.shape[1]\n",
    "total_positives = one_hot_labels.sum(axis=0)\n",
    "\n",
    "# Build indexes of positive and negative values for each label\n",
    "positive_indexes = []\n",
    "negative_indexes = []\n",
    "for column in one_hot_labels.T:\n",
    "    positive_indexes.append([i for i, val in enumerate(column) if val == 1])\n",
    "    negative_indexes.append([i for i, val in enumerate(column) if val == 0])   \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "per_class_training_dfs = []\n",
    "for i in range(0, width):\n",
    "    # Get the training rows for the positive indexes and then an equal number from the negative indexes\n",
    "    pos_example_count = len(positive_indexes[i])\n",
    "    sample_negative_indexes = negative_indexes[i][0:pos_example_count]\n",
    "    \n",
    "    combined_index = sorted(positive_indexes[i] + sample_negative_indexes)\n",
    "    combined_examples = tag_posts.loc[combined_index]\n",
    "\n",
    "    bin_label_series = pd.Series(one_hot_labels.T[i][combined_index], index=combined_index)\n",
    "    print(tag_posts.index)\n",
    "    break\n",
    "    #combined_examples['_Bin_Labels'] = bin_label_series\n",
    "    \n",
    "    # Now get the labels that correspond to the combined examples\n",
    "    # combined_examples['_Bin_Labels'] one_hot_labels.T[i][0]\n",
    "    #labels = one_hot_labels[combined_index]\n",
    "    #combined_examples['_Bin_Labels'] = labels\n",
    "    \n",
    "    #print(combined_examples[['_Tag_List','_Bin_Labels']])\n",
    "\n",
    "    #break\n",
    "    \n",
    "#     # Now reset indexes on both examples and labels\n",
    "#     combined_examples.reset_index(inplace=True)\n",
    "#     #print(combined_examples['_Tags'])\n",
    "#     #print(labels)\n",
    "#     print(positive_examples['_Tags'])\n",
    "    \n",
    "#     #per_class_bin_labels.append(labels)\n",
    "        \n",
    "    #per_label_training_dfs.append(combined_examples)\n",
    "    #print(one_hot_labels.T[i][positive_indexes[i]])\n",
    "\n",
    "#per_label_training_dfs[0]\n",
    "bin_label_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract/Tokenize Non-Code Text from Posts\n",
    "\n",
    "We leave posts' source code out for now because it will need a different embedding and thus multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_LEN = 150\n",
    "PAD_TOKEN = '__PAD__'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def extract_text(x):\n",
    "    \"\"\"Extract non-code text from posts (questions/answers)\"\"\"\n",
    "    doc = BeautifulSoup(x)\n",
    "    codes = doc.find_all('code')\n",
    "    [code.extract() if code else None for code in codes]\n",
    "    tokens = doc.text.split()\n",
    "    padded_tokens = [tokens[i] if len(tokens) > i else PAD_TOKEN for i in range(0,MAX_LEN)]\n",
    "    return padded_tokens\n",
    "\n",
    "post_text = tag_posts._Body.apply(extract_text).reset_index(drop=True)\n",
    "post_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_text.index), len(post_text.iloc[0]), len(labels), len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the posts match the labels\n",
    "assert(len(post_text.index) == len(labels))\n",
    "print('We are left with {:,} example posts'.format(len(post_text.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "The Elmo embedding requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Filter label rows that don't have any positive labels\n",
    "label_mx = np.array(labels)\n",
    "max_per_row = label_mx.max(axis=1)\n",
    "non_zero_index = np.nonzero(max_per_row)[0]\n",
    "\n",
    "label_mx = label_mx[non_zero_index]\n",
    "\n",
    "# Filter the posts to match\n",
    "post_text = post_text[post_text.index.isin(non_zero_index)]\n",
    "post_text = np.array(post_text.tolist())\n",
    "\n",
    "assert(post_text.shape[0] == label_mx.shape[0])\n",
    "print('Unfiltered Counts: {:,} {:,}'.format(post_text.shape[0], label_mx.shape[0]))\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(post_text.shape[0] / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "label_mx = label_mx[0:training_count]\n",
    "post_text = post_text[0:training_count]\n",
    "\n",
    "assert(post_text.shape[0] == label_mx.shape[0])\n",
    "print('Final Counts: {:,} {:,}'.format(post_text.shape[0], label_mx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elmo Embedding Layer using Tensorflow Hub\n",
    "\n",
    "Note that this layer takes a padded two-dimensional array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "We `train_test_split` rather than k-fold cross validate because it is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    post_text,\n",
    "    label_mx,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=34\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM Model to Classify Posts into Tags\n",
    "\n",
    "We use the padded/tokenized posts as input, an Elmo embedding feeding an Long-Short-Term-Memory (LSTM) layer followed by a Dense layer with the same number of output neurons as our tag list.\n",
    "\n",
    "We use focal loss as a loss function, which is used in appliations like object detection, because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, concatenate, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Lambda, Bidirectional\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras_metrics import precision, f1_score, false_negative, true_positive, false_positive, true_negative\n",
    "\n",
    "# Text model\n",
    "text_input = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
    "\n",
    "elmo_embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(text_input)\n",
    "\n",
    "text_lstm = LSTM(\n",
    "    input_shape=(MAX_LEN, 1024,),\n",
    "    units=512,\n",
    "    recurrent_dropout=0.2,\n",
    "    dropout=0.2)(elmo_embedding)\n",
    "\n",
    "text_dense = Dense(200, activation='relu')(text_lstm)\n",
    "\n",
    "text_output = Dense(record_count, activation='sigmoid')(text_dense)\n",
    "text_batch = BatchNormalization()(text_output)\n",
    "\n",
    "text_model = Model(\n",
    "    inputs=text_input, \n",
    "    outputs=text_batch\n",
    ")\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "def abs_KL_div(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), None)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), None)\n",
    "    return K.sum(K.abs( (y_true - y_pred) * (K.log(y_true / y_pred))), axis=-1)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.0005)\n",
    "\n",
    "text_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam,\n",
    "    metrics=[\n",
    "        precision_m,\n",
    "        recall_m,\n",
    "        f1_m,\n",
    "        'mae',\n",
    "        abs_KL_div,\n",
    "        #'accuracy'\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample Weights\n",
    "\n",
    "Because we have skewed classes and multiple classes per example, we employ sample weights which weight the importance of each row according to the relative frequency of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "for i, tag in enumerate(sorted_all_tags):\n",
    "    class_weights[i] = label_mx.shape[0] / tag_counts[tag]\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "history = text_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "        EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "    ],\n",
    "    sample_weight=train_sample_weights,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_abs_KL_div'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "y_pred = text_model.predict(X_test)\n",
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "        y_pred_bin = K.greater(y_pred, cutoff).eval()\n",
    "        print('Cutoff: {:,}'.format(cutoff))\n",
    "        print('Hamming loss: {:,}'.format(\n",
    "            hamming_loss(y_test, y_pred_bin)\n",
    "        ))\n",
    "        for j_type in ['micro', 'macro', 'weighted']:\n",
    "            print('Jaccard {} score: {:,}'.format(\n",
    "                j_type,\n",
    "                jaccard_score(y_test, y_pred_bin, average=j_type)\n",
    "            ))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python\n",
    "def entropy(labels, base=None):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    value,counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "\n",
    "    ent = 0.\n",
    "\n",
    "    # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "entropy(y_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
