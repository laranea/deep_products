{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakly Supervised Learning: Stack Overflow Tag Labeler\n",
    "\n",
    "This is the first project for the book Weakly Supervised Learning, about using natural language processing (NLP) and weakly supervised learning (WSL) to build better models with less data. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier based on 1D Convlutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rjurney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always use a random seed for reproducible results\n",
    "\n",
    "Using random seeds throughout makes our work reproducible - so we get the same tokenization consistently, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPU Support for Tensorflow/Keras\n",
    "\n",
    "The first thing to do is to verify that our JupyterLab Python environment on our Data Science Engine EC2 instance is properly configured to work with its onboard GPU. We use `tensorflow.test.is_gpu_available` and `tensorflow.compat.v2.config.experimental.list_physical_devices` to verify the GPUs are working with Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 or more GPUs is available: True\n",
      "GPUs on tap: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "print(f'1 or more GPUs is available: {gpu_avail}')\n",
    "\n",
    "avail_gpus = tf.compat.v2.config.experimental.list_physical_devices('GPU')\n",
    "print(f'GPUs on tap: {avail_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widen the Max Width of Pandas Columns\n",
    "\n",
    "We want to be able to see the text of the questions in our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Application\n",
    "\n",
    "Setup the batch size, document size, token count for the tokenizer/embedding, the embedding dimensions and the test/train split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 64\n",
    "MAX_LEN     = 200\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE  = 50\n",
    "TEST_SPLIT  = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000, 10,000, 5,000 and 2,000 instances that reduced the maximum imbalance via oversampling using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive. The latter parts of the run could be computed on my own 6 core/64GB machine with NVM drives for data/Spark caching.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_776</th>\n",
       "      <th>label_777</th>\n",
       "      <th>label_778</th>\n",
       "      <th>label_779</th>\n",
       "      <th>label_780</th>\n",
       "      <th>label_781</th>\n",
       "      <th>label_782</th>\n",
       "      <th>label_783</th>\n",
       "      <th>label_784</th>\n",
       "      <th>label_785</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                             _Body  \\\n",
       "0  [How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...   \n",
       "1  [Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...   \n",
       "\n",
       "   label_0  label_1  label_2  label_3  label_4  label_5  label_6  label_7  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   label_8  ...  label_776  label_777  label_778  label_779  label_780  \\\n",
       "0        0  ...          0          0          0          0          0   \n",
       "1        0  ...          0          0          0          0          0   \n",
       "\n",
       "   label_781  label_782  label_783  label_784  label_785  \n",
       "0          0          0          0          0          0  \n",
       "1          0          0          0          0          0  \n",
       "\n",
       "[2 rows x 787 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag limit defines which dataset to load - those with tags having at least 50K, 20K, 10K, 5K or 2K instances\n",
    "TAG_LIMIT = 2000\n",
    "\n",
    "# Pre-computed sorted list of tag/index pairs\n",
    "sorted_all_tags = json.load(open(f'data/stackoverflow/08-05-2019/sorted_all_tags.{TAG_LIMIT}.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1\n",
    "\n",
    "# Load the parquet file using pyarrow for this tag limit, using the sorted tag index to specify the columns\n",
    "posts_df = pd.read_parquet(\n",
    "    f'data/stackoverflow/08-05-2019/Questions.Stratified.Final.{TAG_LIMIT}.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,554,788 Stackoverflow questions with a tag having at least 2,000 occurrences\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,} Stackoverflow questions with a tag having at least 2,000 occurrences'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check the Labels\n",
    "\n",
    "There shouldn't be any rows that are all zeros - it throws off the metrics computed during training. There shouldn't be more than six tags for one question, there is a hard limit in the Stack Overflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero rows: 1,554,788, Total rows: 1,554,788, Non-zero ratio: 1.0, Least tags: 1, Most tags: 6\n"
     ]
    }
   ],
   "source": [
    "test_matrix = posts_df[[f'label_{i}' for i in range(0, max_index)]].as_matrix()\n",
    "\n",
    "tests = np.count_nonzero(test_matrix.sum(axis=1)), \\\n",
    "        test_matrix.sum(axis=1).shape[0], \\\n",
    "        test_matrix.sum(axis=1).min(), \\\n",
    "        test_matrix.sum(axis=1).max()\n",
    "\n",
    "print(f'Non-zero rows: {tests[0]:,}, Total rows: {tests[1]:,}, Non-zero ratio: {tests[0]/tests[1]:,}, Least tags: {tests[2]:,}, Most tags: {tests[3]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Precomputed Indexes for Converting Between Tag Indexes and Tags\n",
    "\n",
    "At the end, when we visualize the predictions of our model versus the test data, we will use these indexes to convert from numeric predictions corresponding to tag indexes in a matrix to actual text tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open(f'data/stackoverflow/08-05-2019/tag_index.{TAG_LIMIT}.json'))\n",
    "index_tag = json.load(open(f'data/stackoverflow/08-05-2019/index_tag.{TAG_LIMIT}.json'))\n",
    "\n",
    "# Sanity check the different files\n",
    "assert( len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "We need to join the previously tokenized text back into a string for use in a Tokenizer, which provides useful properties. In addition, making the number of documents a multiple of batch size is a requirement for Tensorflow/Keras to split work among multiple GPUs and to use certain models such as Elmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 121 Training Count: 1,548,800\n"
     ]
    }
   ],
   "source": [
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# Training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Conserve RAM\n",
    "del posts_df\n",
    "gc.collect()\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the Sequences\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a numpy matrix below.\n",
    "\n",
    "Note that the string `__PAD__` has been used previously to pad the documents, so we use it here for words missing in the top `TOKEN_COUNT` words, in our `Tokenizer`.\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a *numpy* matrix below. \n",
    "\n",
    "In addition to converting the text to numeric sequences with a key, Kerasâ€™ *Tokenizer* class is handy for producing the final results of the model via the [*keras.preprocessing.text.Tokenizer.sequences_to_texts*](https://keras.io/preprocessing/text/#tokenizer) method. Then we use Kerasâ€™ [*keras.preprocessing.sequence.pad_sequences*](https://keras.io/preprocessing/sequence/#pad_sequences) method and check the output to ensure the sequences are all 200 items long or they wonâ€™t convert properly into a matrix. The string `__PAD__` has been used previously to pad the documents, so we reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1548800, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=TOKEN_COUNT,\n",
    "    oov_token='__PAD__'\n",
    ")\n",
    "tokenizer.fit_on_texts(documents)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del documents\n",
    "del sequences\n",
    "gc.collect()\n",
    "\n",
    "# Verify that all padded documents are now the same length\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]) )\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embeddings\n",
    "\n",
    "Stanford defines [GloVe Embeddings](https://nlp.stanford.edu/projects/glove/) as:\n",
    "\n",
    "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "We'll try them out to see if they can beat our own embedding, specific to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 50))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Train Datasets\n",
    "\n",
    "We need one dataset to train with and one separate dataset to test and validate our model with.  The oft used [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) makes it so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del padded_sequences\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Class Weights to Balance Uneven Label Counts\n",
    "\n",
    "Although there has already been filtering and up-sampling of the data to restrict it to a sample of questions with at least one tag that occurs more than 2,000 times, there are still as much as a 8000:1 ratio between common and uncommon labels. Without class weights, the most common label will be much more likely to be predicted than the least common. Class weights will make the loss function consider uncommon classes more than frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(356, 1.0),\n",
       "  (90, 1.09143374198603),\n",
       "  (361, 1.3325384636111335),\n",
       "  (546, 1.3808898878914375),\n",
       "  (17, 1.6404179451032423),\n",
       "  (516, 1.7734526285591337),\n",
       "  (92, 1.81591025923478),\n",
       "  (343, 2.411238420282789),\n",
       "  (371, 3.1597290994473375),\n",
       "  (1, 3.705567211149408),\n",
       "  (309, 3.935423546731389),\n",
       "  (89, 4.0107457118527226),\n",
       "  (150, 4.613292910447761),\n",
       "  (628, 4.779597951096936),\n",
       "  (483, 5.11316170388751),\n",
       "  (350, 5.191255970188422),\n",
       "  (53, 5.266719914802982),\n",
       "  (581, 5.4809376039011415),\n",
       "  (459, 5.682465816385155),\n",
       "  (471, 6.025158382066277),\n",
       "  (552, 6.3403205128205125),\n",
       "  (405, 6.612448188260463),\n",
       "  (629, 7.392854473428508),\n",
       "  (183, 7.418360459011475),\n",
       "  (580, 7.432296363089871),\n",
       "  (653, 7.646617703904136),\n",
       "  (51, 7.8994489258046485),\n",
       "  (34, 8.29425576519916),\n",
       "  (753, 8.520761543762923),\n",
       "  (56, 8.58659605868565),\n",
       "  (766, 8.66406797477225),\n",
       "  (378, 8.737544169611308),\n",
       "  (622, 9.112677353970886),\n",
       "  (457, 9.586063190540802),\n",
       "  (590, 9.830931318954379),\n",
       "  (647, 10.178964701039416),\n",
       "  (773, 10.341802593057299),\n",
       "  (776, 10.436741584889733),\n",
       "  (513, 11.143420459666517),\n",
       "  (76, 11.387174763988027),\n",
       "  (706, 11.408189158016148),\n",
       "  (161, 11.944088878154812),\n",
       "  (729, 11.981708055723804),\n",
       "  (566, 12.062073170731708),\n",
       "  (13, 12.158451137062077),\n",
       "  (204, 12.539173427991887),\n",
       "  (394, 12.700179763739087),\n",
       "  (419, 12.934353341179548),\n",
       "  (11, 13.09185969556585),\n",
       "  (272, 13.278158142032488),\n",
       "  (448, 13.893664840567496),\n",
       "  (311, 14.097633979475484),\n",
       "  (227, 14.61205495641897),\n",
       "  (408, 14.764741006120316),\n",
       "  (548, 14.929660377358491),\n",
       "  (558, 15.11445599022005),\n",
       "  (494, 15.31573242489935),\n",
       "  (32, 15.325224666873257),\n",
       "  (218, 15.349006828057107),\n",
       "  (721, 15.440056197315018),\n",
       "  (93, 15.6551123773346),\n",
       "  (658, 15.904325454253096),\n",
       "  (575, 16.741536899119836),\n",
       "  (40, 16.84991482112436),\n",
       "  (322, 17.029786501377412),\n",
       "  (16, 17.16872070821038),\n",
       "  (312, 17.549503193754436),\n",
       "  (611, 17.89884183858125),\n",
       "  (42, 17.94104843098132),\n",
       "  (502, 17.944303338171263),\n",
       "  (550, 18.022776967930028),\n",
       "  (603, 18.32326787699148),\n",
       "  (528, 18.374326583689392),\n",
       "  (719, 18.394829830760646),\n",
       "  (402, 18.40167441860465),\n",
       "  (600, 18.525753886495597),\n",
       "  (306, 18.86856161770317),\n",
       "  (429, 18.915471409447314),\n",
       "  (772, 18.919089517980108),\n",
       "  (280, 19.0686331212647),\n",
       "  (264, 19.27299298519096),\n",
       "  (547, 19.27299298519096),\n",
       "  (171, 19.325713169206722),\n",
       "  (237, 19.33326817826427),\n",
       "  (514, 19.520228932307084),\n",
       "  (260, 19.535650799920994),\n",
       "  (152, 19.901207243460764),\n",
       "  (234, 20.2474923234391),\n",
       "  (731, 20.36841021416804),\n",
       "  (191, 20.372605561277034),\n",
       "  (478, 20.439966935317216),\n",
       "  (666, 20.593170934832397),\n",
       "  (426, 20.649060542797496),\n",
       "  (485, 20.93312169312169),\n",
       "  (110, 20.946421008047437),\n",
       "  (582, 21.01763705907352),\n",
       "  (44, 21.20235798499464),\n",
       "  (765, 21.216001716001717),\n",
       "  (256, 21.408874458874458),\n",
       "  (693, 21.47394702561876),\n",
       "  (669, 21.728690685413007),\n",
       "  (761, 21.810143329658214),\n",
       "  (689, 21.834216335540837),\n",
       "  (768, 21.863174182139698),\n",
       "  (0, 21.882522123893807),\n",
       "  (639, 21.91646354974518),\n",
       "  (116, 21.935905965846086),\n",
       "  (303, 22.37760180995475),\n",
       "  (708, 22.587120347111213),\n",
       "  (168, 22.592279579716767),\n",
       "  (744, 22.769106813996316),\n",
       "  (717, 22.790092165898617),\n",
       "  (685, 22.932761418965917),\n",
       "  (67, 23.055710955710957),\n",
       "  (174, 23.25628967787444),\n",
       "  (267, 23.27270588235294),\n",
       "  (170, 23.617239732569246),\n",
       "  (617, 23.764776549735704),\n",
       "  (418, 23.908387720570463),\n",
       "  (62, 24.036208991494533),\n",
       "  (415, 24.11238420282789),\n",
       "  (620, 24.11826383808827),\n",
       "  (739, 24.183129584352077),\n",
       "  (300, 24.39186189889026),\n",
       "  (142, 24.421975308641976),\n",
       "  (701, 24.470311726867887),\n",
       "  (529, 24.51883986117997),\n",
       "  (242, 24.60422885572139),\n",
       "  (376, 24.677894211576845),\n",
       "  (357, 24.708718461154135),\n",
       "  (625, 24.78301177649712),\n",
       "  (694, 24.914105793450883),\n",
       "  (525, 24.964411913175162),\n",
       "  (713, 25.091070522577372),\n",
       "  (58, 25.097437198680538),\n",
       "  (431, 25.52490322580645),\n",
       "  (30, 25.590944372574384),\n",
       "  (631, 25.73744470465782),\n",
       "  (178, 25.744143675169184),\n",
       "  (486, 25.885632033499085),\n",
       "  (229, 26.00815145937418),\n",
       "  (635, 26.00815145937418),\n",
       "  (328, 26.049249407426917),\n",
       "  (212, 26.124933967247756),\n",
       "  (752, 26.18025410269984),\n",
       "  (54, 26.201059602649007),\n",
       "  (228, 26.221898197242844),\n",
       "  (425, 26.291600212652845),\n",
       "  (493, 26.488752008569897),\n",
       "  (349, 26.5171581769437),\n",
       "  (64, 26.538502817279312),\n",
       "  (289, 26.782832385594368),\n",
       "  (226, 26.862846279196088),\n",
       "  (623, 26.884751291111716),\n",
       "  (163, 26.892060902664493),\n",
       "  (391, 27.187740516767455),\n",
       "  (181, 27.33803206191266),\n",
       "  (284, 27.46709247431269),\n",
       "  (320, 27.666853146853146),\n",
       "  (506, 27.67459429210968),\n",
       "  (681, 27.713365088260016),\n",
       "  (115, 27.79123349255409),\n",
       "  (96, 28.011611441517985),\n",
       "  (77, 28.067253121452893),\n",
       "  (338, 28.29204805491991),\n",
       "  (667, 28.29204805491991),\n",
       "  (433, 28.52047289504037),\n",
       "  (153, 28.694226863939658),\n",
       "  (274, 28.719221835075494),\n",
       "  (370, 28.937682855471035),\n",
       "  (73, 28.988569753810083),\n",
       "  (728, 28.997068308413954),\n",
       "  (323, 29.082328726845045),\n",
       "  (277, 29.133726067746686),\n",
       "  (244, 29.21116361488482),\n",
       "  (692, 29.35856337192045),\n",
       "  (608, 29.402199762187873),\n",
       "  (231, 29.43720238095238),\n",
       "  (482, 29.52507462686567),\n",
       "  (710, 29.57805023923445),\n",
       "  (656, 29.60460939838372),\n",
       "  (366, 29.675667566756676),\n",
       "  (560, 29.675667566756676),\n",
       "  (159, 29.693485439807866),\n",
       "  (358, 29.89993954050786),\n",
       "  (380, 29.963344441078462),\n",
       "  (413, 29.99060036385688),\n",
       "  (335, 30.03613726085636),\n",
       "  (604, 30.43353846153846),\n",
       "  (261, 30.508636644046884),\n",
       "  (559, 30.59356634704609),\n",
       "  (7, 30.967125860989356),\n",
       "  (217, 31.03514276749294),\n",
       "  (466, 31.33006018371872),\n",
       "  (87, 31.349920760697305),\n",
       "  (252, 31.39968253968254),\n",
       "  (409, 31.419631512071156),\n",
       "  (134, 31.459605597964376),\n",
       "  (599, 31.47963080840229),\n",
       "  (407, 31.539859693877553),\n",
       "  (705, 31.68129404228059),\n",
       "  (59, 31.86501288659794),\n",
       "  (26, 31.99902944031058),\n",
       "  (605, 32.0612641815235),\n",
       "  (613, 32.0612641815235),\n",
       "  (382, 32.0820629257217),\n",
       "  (444, 32.14462138446539),\n",
       "  (121, 32.25994781474233),\n",
       "  (723, 32.39731411726171),\n",
       "  (23, 32.41855129465749),\n",
       "  (125, 32.58945634266886),\n",
       "  (375, 32.7404832836809),\n",
       "  (652, 32.7404832836809),\n",
       "  (213, 32.75132450331126),\n",
       "  (245, 32.87105350614822),\n",
       "  (527, 32.88198138297872),\n",
       "  (117, 33.00266933600267),\n",
       "  (714, 33.1020749665328),\n",
       "  (601, 33.14644772117963),\n",
       "  (15, 33.24672268907563),\n",
       "  (468, 33.28028263795424),\n",
       "  (36, 33.39264010803511),\n",
       "  (273, 33.46041948579161),\n",
       "  (71, 33.49441246190315),\n",
       "  (520, 33.528474576271186),\n",
       "  (458, 33.56260604004072),\n",
       "  (305, 33.65396393331065),\n",
       "  (85, 33.71131561008862),\n",
       "  (396, 33.73431105047749),\n",
       "  (511, 33.745820539065164),\n",
       "  (446, 33.757337883959046),\n",
       "  (732, 33.838179952104),\n",
       "  (737, 33.896161754626455),\n",
       "  (725, 33.966002747252745),\n",
       "  (428, 33.9776709034696),\n",
       "  (155, 34.047848537005166),\n",
       "  (496, 34.10655172413793),\n",
       "  (643, 34.13008971704624),\n",
       "  (427, 34.141870900932),\n",
       "  (141, 34.26013162452373),\n",
       "  (61, 34.571478504019574),\n",
       "  (216, 34.58356643356643),\n",
       "  (607, 34.668419207851386),\n",
       "  (314, 34.76590509666081),\n",
       "  (177, 34.82711267605634),\n",
       "  (565, 35.04925584691708),\n",
       "  (327, 35.13641207815275),\n",
       "  (447, 35.21146315414738),\n",
       "  (377, 35.274251069900146),\n",
       "  (86, 35.34989278055754),\n",
       "  (290, 35.36253128351805),\n",
       "  (724, 35.36253128351805),\n",
       "  (233, 35.55319913731129),\n",
       "  (651, 35.56598345918734),\n",
       "  (341, 35.7072202166065),\n",
       "  (438, 35.84958318231243),\n",
       "  (158, 35.914669571532315),\n",
       "  (602, 36.01930080116533),\n",
       "  (741, 36.01930080116533),\n",
       "  (111, 36.045553935860056),\n",
       "  (135, 36.07184536834428),\n",
       "  (295, 36.11135450894487),\n",
       "  (467, 36.1245434623813),\n",
       "  (103, 36.137742053343075),\n",
       "  (318, 36.17739575713241),\n",
       "  (555, 36.23040293040293),\n",
       "  (503, 36.24367900329791),\n",
       "  (730, 36.363602941176474),\n",
       "  (189, 36.41715758468336),\n",
       "  (504, 36.68731454005935),\n",
       "  (670, 36.728184181210544),\n",
       "  (186, 36.74182763744428),\n",
       "  (644, 36.796502976190474),\n",
       "  (740, 36.86507640700708),\n",
       "  (220, 36.89257739649385),\n",
       "  (385, 36.89257739649385),\n",
       "  (487, 37.01684131736527),\n",
       "  (778, 37.01684131736527),\n",
       "  (301, 37.03070011231748),\n",
       "  (480, 37.05844885724991),\n",
       "  (641, 37.05844885724991),\n",
       "  (420, 37.072338830584705),\n",
       "  (176, 37.08623922009749),\n",
       "  (88, 37.10015003750938),\n",
       "  (195, 37.114071294559096),\n",
       "  (91, 37.169860954528374),\n",
       "  (474, 37.22581859239744),\n",
       "  (203, 37.2398343373494),\n",
       "  (347, 37.2398343373494),\n",
       "  (660, 37.352341389728096),\n",
       "  (530, 37.36645258783528),\n",
       "  (129, 37.53662239089184),\n",
       "  (442, 37.579407294832826),\n",
       "  (743, 37.579407294832826),\n",
       "  (583, 37.607984790874525),\n",
       "  (432, 37.69397865853659),\n",
       "  (522, 37.72273073989321),\n",
       "  (554, 37.72273073989321),\n",
       "  (648, 37.76594119893089),\n",
       "  (172, 37.80925076452599),\n",
       "  (194, 37.80925076452599),\n",
       "  (46, 37.823709369024854),\n",
       "  (675, 37.823709369024854),\n",
       "  (771, 37.823709369024854),\n",
       "  (336, 37.881654538491),\n",
       "  (441, 37.925230061349694),\n",
       "  (157, 38.01268255188317),\n",
       "  (14, 38.07120862201693),\n",
       "  (734, 38.129915188897456),\n",
       "  (239, 38.30712625871418),\n",
       "  (770, 38.36656322730799),\n",
       "  (247, 38.38145129996119),\n",
       "  (702, 38.41126213592233),\n",
       "  (538, 38.47102294826916),\n",
       "  (369, 38.576053042121686),\n",
       "  (637, 38.59110417479516),\n",
       "  (767, 38.59110417479516),\n",
       "  (398, 38.621241702459976),\n",
       "  (128, 38.69679186228482),\n",
       "  (330, 38.72709475332811),\n",
       "  (324, 38.74226400313357),\n",
       "  (606, 38.879323899371066),\n",
       "  (138, 39.00197160883281),\n",
       "  (524, 39.09446640316205),\n",
       "  (19, 39.2029330162505),\n",
       "  (727, 39.26518459706232),\n",
       "  (66, 39.39028275587415),\n",
       "  (392, 39.40597609561753),\n",
       "  (80, 39.437400318979265),\n",
       "  (345, 39.45313123254886),\n",
       "  (238, 39.468874700718274),\n",
       "  (230, 39.54778088764494),\n",
       "  (733, 39.54778088764494),\n",
       "  (9, 39.579431772709086),\n",
       "  (223, 39.627003205128204),\n",
       "  (406, 39.69060995184591),\n",
       "  (114, 39.73844917637606),\n",
       "  (241, 39.77040611178126),\n",
       "  (365, 39.786403861625104),\n",
       "  (508, 39.80241448692153),\n",
       "  (564, 39.80241448692153),\n",
       "  (292, 39.834474426097465),\n",
       "  (703, 39.89874949576442),\n",
       "  (556, 40.060348319157555),\n",
       "  (291, 40.17424857839155),\n",
       "  (626, 40.256003256003254),\n",
       "  (278, 40.30521597392013),\n",
       "  (395, 40.32164696290257),\n",
       "  (594, 40.45357873210634),\n",
       "  (445, 40.55309553095531),\n",
       "  (348, 40.73682042833608),\n",
       "  (533, 40.73682042833608),\n",
       "  (726, 40.77040395713108),\n",
       "  (401, 40.87148760330579),\n",
       "  (359, 40.92221762515515),\n",
       "  (28, 40.93915562913907),\n",
       "  (25, 40.99005387484459),\n",
       "  (464, 40.99005387484459),\n",
       "  (749, 41.024056408129404),\n",
       "  (122, 41.07516611295681),\n",
       "  (755, 41.194918783840066),\n",
       "  (664, 41.280884808013354),\n",
       "  (748, 41.280884808013354),\n",
       "  (757, 41.33263685750104),\n",
       "  (246, 41.38451882845188),\n",
       "  (101, 41.41917922948074),\n",
       "  (126, 41.45389773679799),\n",
       "  (762, 41.45389773679799),\n",
       "  (540, 41.47127882599581),\n",
       "  (410, 41.48867449664429),\n",
       "  (587, 41.55840336134454),\n",
       "  (422, 41.57587221521648),\n",
       "  (436, 41.68099452170249),\n",
       "  (630, 41.80431107354184),\n",
       "  (596, 41.92835947435354),\n",
       "  (754, 41.92835947435354),\n",
       "  (352, 42.03527411814704),\n",
       "  (319, 42.071033602722245),\n",
       "  (3, 42.1068539804172),\n",
       "  (668, 42.1247870528109),\n",
       "  (567, 42.26880341880342),\n",
       "  (383, 42.450214592274676),\n",
       "  (4, 42.48668384879725),\n",
       "  (106, 42.48668384879725),\n",
       "  (769, 42.48668384879725),\n",
       "  (476, 42.55981067125646),\n",
       "  (545, 42.55981067125646),\n",
       "  (130, 42.59646856158484),\n",
       "  (143, 42.65157395429064),\n",
       "  (340, 42.66997411561691),\n",
       "  (263, 42.688390159689256),\n",
       "  (481, 42.76221357544315),\n",
       "  (249, 42.780709342560556),\n",
       "  (523, 42.780709342560556),\n",
       "  (646, 42.89202081526453),\n",
       "  (43, 42.94789405123752),\n",
       "  (140, 42.94789405123752),\n",
       "  (423, 42.96655082536925),\n",
       "  (615, 43.022618529795565),\n",
       "  (672, 43.07883275261324),\n",
       "  (208, 43.11639058413252),\n",
       "  (354, 43.11639058413252),\n",
       "  (39, 43.154013961605585),\n",
       "  (293, 43.1728502837189),\n",
       "  (5, 43.21057230231542),\n",
       "  (74, 43.26727909011374),\n",
       "  (162, 43.30516637478109),\n",
       "  (389, 43.34312007011393),\n",
       "  (113, 43.36212187637001),\n",
       "  (597, 43.36212187637001),\n",
       "  (682, 43.36212187637001),\n",
       "  (251, 43.419227392449514),\n",
       "  (175, 43.45738137082601),\n",
       "  (206, 43.553060325847646),\n",
       "  (332, 43.726348364279396),\n",
       "  (166, 43.74568774878372),\n",
       "  (461, 43.90102086107412),\n",
       "  (784, 43.95955555555555),\n",
       "  (156, 43.97910182303246),\n",
       "  (321, 44.03784505788067),\n",
       "  (337, 44.03784505788067),\n",
       "  (452, 44.03784505788067),\n",
       "  (711, 44.03784505788067),\n",
       "  (722, 44.0770944741533),\n",
       "  (593, 44.09674543022737),\n",
       "  (299, 44.13609995537706),\n",
       "  (450, 44.15580357142857),\n",
       "  (579, 44.15580357142857),\n",
       "  (414, 44.23479427549195),\n",
       "  (50, 44.274395702775294),\n",
       "  (197, 44.314068100358426),\n",
       "  (578, 44.39362657091562),\n",
       "  (317, 44.45348314606741),\n",
       "  (97, 44.47347122302158),\n",
       "  (780, 44.47347122302158),\n",
       "  (108, 44.49347728295097),\n",
       "  (99, 44.53354344889689),\n",
       "  (532, 44.53354344889689),\n",
       "  (235, 44.593778178539225),\n",
       "  (618, 44.613892647722146),\n",
       "  (700, 44.613892647722146),\n",
       "  (421, 44.6743450767841),\n",
       "  (515, 44.81603987313095),\n",
       "  (416, 44.83635539437897),\n",
       "  (68, 44.87704174228675),\n",
       "  (271, 44.938209904588824),\n",
       "  (489, 44.958636363636366),\n",
       "  (70, 45.10214318285454),\n",
       "  (144, 45.12271897810219),\n",
       "  (750, 45.12271897810219),\n",
       "  (360, 45.16392694063927),\n",
       "  (69, 45.20521023765996),\n",
       "  (346, 45.20521023765996),\n",
       "  (688, 45.20521023765996),\n",
       "  (691, 45.20521023765996),\n",
       "  (585, 45.22588020118884),\n",
       "  (707, 45.26727688787185),\n",
       "  (756, 45.308749427393494),\n",
       "  (329, 45.3295142071494),\n",
       "  (304, 45.350298028427325),\n",
       "  (490, 45.4127640036731),\n",
       "  (561, 45.580184331797234),\n",
       "  (355, 45.601198709082524),\n",
       "  (521, 45.622232472324725),\n",
       "  (139, 45.72769301895516),\n",
       "  (49, 45.770013882461825),\n",
       "  (107, 45.81241315423807),\n",
       "  (588, 45.81241315423807),\n",
       "  (718, 45.85489105238757),\n",
       "  (253, 45.91875580315692),\n",
       "  (683, 45.91875580315692),\n",
       "  (393, 45.9614312267658),\n",
       "  (133, 45.98279869827987),\n",
       "  (136, 46.04702048417132),\n",
       "  (262, 46.04702048417132),\n",
       "  (539, 46.13292910447761),\n",
       "  (437, 46.17600373482726),\n",
       "  (211, 46.240766713417486),\n",
       "  (296, 46.30571161048689),\n",
       "  (451, 46.34910965323336),\n",
       "  (123, 46.39258911819888),\n",
       "  (222, 46.41435945565462),\n",
       "  (276, 46.41435945565462),\n",
       "  (149, 46.47979323308271),\n",
       "  (472, 46.47979323308271),\n",
       "  (698, 46.47979323308271),\n",
       "  (577, 46.50164551010813),\n",
       "  (287, 46.54541176470588),\n",
       "  (65, 46.58926048045219),\n",
       "  (294, 46.61121583411875),\n",
       "  (404, 46.61121583411875),\n",
       "  (609, 46.61121583411875),\n",
       "  (79, 46.655188679245285),\n",
       "  (214, 46.655188679245285),\n",
       "  (362, 46.699244570349386),\n",
       "  (463, 46.699244570349386),\n",
       "  (313, 46.7213037316958),\n",
       "  (704, 46.7876064333018),\n",
       "  (759, 46.7876064333018),\n",
       "  (258, 46.85409758408337),\n",
       "  (344, 46.9207779886148),\n",
       "  (665, 46.9207779886148),\n",
       "  (315, 47.09952380952381),\n",
       "  (745, 47.09952380952381),\n",
       "  (37, 47.121962839447356),\n",
       "  (118, 47.144423260247855),\n",
       "  (285, 47.18940839694657),\n",
       "  (424, 47.21193317422434),\n",
       "  (673, 47.21193317422434),\n",
       "  (659, 47.347534705600765),\n",
       "  (616, 47.37021072796935),\n",
       "  (475, 47.39290848107331),\n",
       "  (75, 47.43836930455635),\n",
       "  (758, 47.46113243761996),\n",
       "  (6, 47.50672430355427),\n",
       "  (655, 47.50672430355427),\n",
       "  (255, 47.52955309947141),\n",
       "  (219, 47.62108810784786),\n",
       "  (243, 47.68997107039537),\n",
       "  (589, 47.73600386100386),\n",
       "  (526, 47.759053597295996),\n",
       "  (55, 47.82833655705996),\n",
       "  (363, 47.82833655705996),\n",
       "  (679, 47.82833655705996),\n",
       "  (124, 47.8514755684567),\n",
       "  (469, 47.87463697967086),\n",
       "  (94, 47.92102713178294),\n",
       "  (570, 47.94425593795444),\n",
       "  (507, 47.967507274490785),\n",
       "  (709, 47.967507274490785),\n",
       "  (302, 48.037396794560465),\n",
       "  (164, 48.084103062712686),\n",
       "  (270, 48.130900243309),\n",
       "  (200, 48.24829268292683),\n",
       "  (281, 48.24829268292683),\n",
       "  (334, 48.27183992191313),\n",
       "  (417, 48.29541015625),\n",
       "  (517, 48.29541015625),\n",
       "  (372, 48.3190034196385),\n",
       "  (52, 48.366259168704154),\n",
       "  (537, 48.366259168704154),\n",
       "  (591, 48.366259168704154),\n",
       "  (621, 48.41360744003916),\n",
       "  (24, 48.461048505634494),\n",
       "  (342, 48.461048505634494),\n",
       "  (411, 48.461048505634494),\n",
       "  (576, 48.508582638548305),\n",
       "  (185, 48.651746187899654),\n",
       "  (495, 48.651746187899654),\n",
       "  (193, 48.675688976377955),\n",
       "  (173, 48.69965534219596),\n",
       "  (368, 48.77169625246548),\n",
       "  (236, 48.79575727676369),\n",
       "  (562, 48.84395061728395),\n",
       "  (102, 48.86808300395257),\n",
       "  (387, 48.892239248640635),\n",
       "  (509, 48.892239248640635),\n",
       "  (592, 48.892239248640635),\n",
       "  (145, 48.916419386745794),\n",
       "  (308, 48.940623453735775),\n",
       "  (569, 49.03767972235994),\n",
       "  (573, 49.03767972235994),\n",
       "  (182, 49.11072492552135),\n",
       "  (738, 49.1351217088922),\n",
       "  (82, 49.159542743538765),\n",
       "  (109, 49.20845771144278),\n",
       "  (333, 49.20845771144278),\n",
       "  (120, 49.25747011952191),\n",
       "  (386, 49.306580259222336),\n",
       "  (779, 49.306580259222336),\n",
       "  (179, 49.35578842315369),\n",
       "  (500, 49.35578842315369),\n",
       "  (250, 49.504004004004),\n",
       "  (199, 49.60330992978937),\n",
       "  (48, 49.653112449799195),\n",
       "  (221, 49.653112449799195),\n",
       "  (781, 49.653112449799195),\n",
       "  (2, 49.67805123053742),\n",
       "  (95, 49.67805123053742),\n",
       "  (137, 49.67805123053742),\n",
       "  (209, 49.67805123053742),\n",
       "  (353, 49.67805123053742),\n",
       "  (326, 49.70301507537688),\n",
       "  (636, 49.70301507537688),\n",
       "  (735, 49.70301507537688),\n",
       "  (367, 49.753018108651915),\n",
       "  (127, 49.778057372924),\n",
       "  (501, 49.853326612903224),\n",
       "  (269, 49.87846696923853),\n",
       "  (488, 49.87846696923853),\n",
       "  (551, 49.87846696923853),\n",
       "  (699, 49.87846696923853),\n",
       "  (783, 49.90363269424824),\n",
       "  (477, 49.928823826350325),\n",
       "  (645, 49.928823826350325),\n",
       "  (232, 49.954040404040406),\n",
       "  (112, 49.97928246589186),\n",
       "  (364, 49.97928246589186),\n",
       "  (225, 50.00455005055612),\n",
       "  (190, 50.02984319676277),\n",
       "  (747, 50.02984319676277),\n",
       "  (254, 50.05516194331984),\n",
       "  (492, 50.08050632911392),\n",
       "  (684, 50.105876393110435),\n",
       "  (78, 50.20761421319797),\n",
       "  (265, 50.28418912048805),\n",
       "  (676, 50.28418912048805),\n",
       "  (146, 50.335368956743004),\n",
       "  (148, 50.38665308201732),\n",
       "  (633, 50.38665308201732),\n",
       "  (640, 50.46377551020408),\n",
       "  (777, 50.48953547728433),\n",
       "  (775, 50.51532175689479),\n",
       "  (595, 50.541134389371486),\n",
       "  (192, 50.56697341513292),\n",
       "  (649, 50.56697341513292),\n",
       "  (661, 50.696565863659664),\n",
       "  (439, 50.7225641025641),\n",
       "  (443, 50.748589020010265),\n",
       "  (598, 50.800719054956346),\n",
       "  (782, 50.800719054956346),\n",
       "  (491, 50.82682425488181),\n",
       "  (696, 50.85295629820052),\n",
       "  (240, 50.87911522633745),\n",
       "  (677, 50.87911522633745),\n",
       "  (720, 51.01031459515214),\n",
       "  (715, 51.0629839958699),\n",
       "  (563, 51.08935950413223),\n",
       "  (465, 51.11576227390181),\n",
       "  (512, 51.168649767201245),\n",
       "  (430, 51.274753758424055),\n",
       "  (642, 51.30134854771784),\n",
       "  (690, 51.30134854771784),\n",
       "  (100, 51.32797093928386),\n",
       "  (751, 51.3546209761163),\n",
       "  (473, 51.43473738949558),\n",
       "  (198, 51.46149843912591),\n",
       "  (435, 51.46149843912591),\n",
       "  (268, 51.48828735033837),\n",
       "  (84, 51.54194893173528),\n",
       "  (610, 51.59572248304643),\n",
       "  (379, 51.64960835509138),\n",
       "  (638, 51.64960835509138),\n",
       "  (104, 51.67659352142111),\n",
       "  (316, 51.67659352142111),\n",
       "  (498, 51.67659352142111),\n",
       "  (634, 51.70360690015682),\n",
       "  (310, 51.73064853556485),\n",
       "  (105, 51.81194342587742),\n",
       "  (584, 51.81194342587742),\n",
       "  (390, 51.89349422875131),\n",
       "  (188, 51.97530215449291),\n",
       "  (298, 51.97530215449291),\n",
       "  (180, 52.02998421883219),\n",
       "  (456, 52.05736842105263),\n",
       "  (259, 52.1396942540854),\n",
       "  (331, 52.1396942540854),\n",
       "  (785, 52.1396942540854),\n",
       "  (132, 52.19472295514512),\n",
       "  (518, 52.24986793449551),\n",
       "  (60, 52.36050820539968),\n",
       "  (81, 52.38824152542373),\n",
       "  (98, 52.41600423953365),\n",
       "  (470, 52.44379639448569),\n",
       "  (519, 52.527349973446626),\n",
       "  (479, 52.611170212765956),\n",
       "  (484, 52.611170212765956),\n",
       "  (671, 52.611170212765956),\n",
       "  (678, 52.611170212765956),\n",
       "  (147, 52.667199148029816),\n",
       "  (282, 52.667199148029816),\n",
       "  (742, 52.667199148029816),\n",
       "  (572, 52.69525839104955),\n",
       "  (388, 52.72334754797441),\n",
       "  (297, 52.751466666666666),\n",
       "  (695, 52.751466666666666),\n",
       "  (154, 52.864243719935864),\n",
       "  (151, 52.892513368983956),\n",
       "  (248, 52.892513368983956),\n",
       "  (400, 52.892513368983956),\n",
       "  (35, 52.94914346895075),\n",
       "  (440, 52.94914346895075),\n",
       "  (736, 53.0058949624866),\n",
       "  (202, 53.0343163538874),\n",
       "  (568, 53.0343163538874),\n",
       "  (614, 53.09125067096082),\n",
       "  (169, 53.14830736163353),\n",
       "  (680, 53.14830736163353),\n",
       "  (686, 53.14830736163353),\n",
       "  (434, 53.32021563342318),\n",
       "  (663, 53.34897518878101),\n",
       "  (712, 53.34897518878101),\n",
       "  (184, 53.37776578521317),\n",
       "  (165, 53.40658747300216),\n",
       "  (10, 53.49323958896701),\n",
       "  (31, 53.49323958896701),\n",
       "  (760, 53.49323958896701),\n",
       "  (697, 53.58017334777898),\n",
       "  (657, 53.609214092140924),\n",
       "  (266, 53.725692558392176),\n",
       "  (57, 53.75489130434783),\n",
       "  (29, 53.813384113166485),\n",
       "  (131, 53.813384113166485),\n",
       "  (215, 53.842678279804026),\n",
       "  (224, 53.842678279804026),\n",
       "  (83, 53.96017457719585),\n",
       "  (632, 53.96017457719585),\n",
       "  (746, 53.9896288209607),\n",
       "  (27, 54.04863387978142),\n",
       "  (497, 54.10776805251641),\n",
       "  (72, 54.13738368910783),\n",
       "  (207, 54.167031763417306),\n",
       "  (119, 54.19671232876712),\n",
       "  (462, 54.19671232876712),\n",
       "  (196, 54.256171146461874),\n",
       "  (574, 54.28594950603732),\n",
       "  (187, 54.37548103353491),\n",
       "  (535, 54.405390539053904),\n",
       "  (624, 54.405390539053904),\n",
       "  (275, 54.55543298400441),\n",
       "  (571, 54.5855408388521),\n",
       "  (654, 54.5855408388521),\n",
       "  (716, 54.5855408388521),\n",
       "  (283, 54.61568194367753),\n",
       "  (534, 54.61568194367753),\n",
       "  (160, 54.645856353591164),\n",
       "  (339, 54.82760532150776),\n",
       "  (627, 54.85801442041043),\n",
       "  (257, 54.88845726970033),\n",
       "  (505, 54.91893392559689),\n",
       "  (544, 55.01056729699666),\n",
       "  (201, 55.194754464285715),\n",
       "  (373, 55.25642458100559),\n",
       "  (774, 55.34918858421936),\n",
       "  (12, 55.411204481792716),\n",
       "  (403, 55.44226457399103),\n",
       "  (541, 55.4733595064498),\n",
       "  (612, 55.4733595064498),\n",
       "  (662, 55.4733595064498),\n",
       "  (543, 55.50448933782267),\n",
       "  (531, 55.56685393258427),\n",
       "  (205, 55.62935883014623),\n",
       "  (412, 55.62935883014623),\n",
       "  (45, 55.75479143179256),\n",
       "  (8, 55.880790960451975),\n",
       "  (167, 55.9756649688738),\n",
       "  (381, 56.03909348441926),\n",
       "  (325, 56.19829545454545),\n",
       "  (21, 56.262229806598405),\n",
       "  (764, 56.2942515651679),\n",
       "  (288, 56.32630979498861),\n",
       "  (351, 56.48715019988578),\n",
       "  (557, 56.51942857142857),\n",
       "  (453, 56.58409610983982),\n",
       "  (22, 56.61648540354894),\n",
       "  (460, 56.61648540354894),\n",
       "  (397, 56.681375358166186),\n",
       "  (399, 56.778989667049366),\n",
       "  (18, 56.81160252728317),\n",
       "  (210, 56.90966628308401),\n",
       "  (536, 57.00806916426513),\n",
       "  (650, 57.1068129330254),\n",
       "  (286, 57.2058993637941),\n",
       "  (510, 57.338550724637685),\n",
       "  (549, 57.740221833041446),\n",
       "  (553, 57.87536571094207),\n",
       "  (687, 57.87536571094207),\n",
       "  (20, 57.977139507620166),\n",
       "  (307, 58.18176470588235),\n",
       "  (384, 58.3189858490566),\n",
       "  (449, 58.387839433293976),\n",
       "  (47, 58.52603550295858),\n",
       "  (38, 58.664887307236064),\n",
       "  (279, 59.79987908101572),\n",
       "  (41, 60.05403764420158),\n",
       "  (619, 60.53182374541004),\n",
       "  (33, 61.243962848297215),\n",
       "  (374, 61.31990080595164),\n",
       "  (763, 62.36380832282472),\n",
       "  (454, 82.35553705245628),\n",
       "  (63, 83.9634974533107),\n",
       "  (455, 98.31908548707753),\n",
       "  (542, 160.82764227642275),\n",
       "  (499, 179.18297101449275),\n",
       "  (586, 296.13473053892216),\n",
       "  (674, 317.01602564102564)],\n",
       " [(356, 1.0),\n",
       "  (90, 1.0811829887651978),\n",
       "  (361, 1.3324587469178732),\n",
       "  (546, 1.3641100323624595),\n",
       "  (17, 1.6086325993206885),\n",
       "  (516, 1.7757509373551839),\n",
       "  (92, 1.8061875990915712),\n",
       "  (343, 2.413455482393358),\n",
       "  (371, 3.136235119047619),\n",
       "  (1, 3.661165638843047),\n",
       "  (309, 3.897096893491124),\n",
       "  (89, 4.023577701412753),\n",
       "  (150, 4.603647881170817),\n",
       "  (628, 4.769291694953609),\n",
       "  (350, 5.1191401505950935),\n",
       "  (483, 5.132229392426641),\n",
       "  (53, 5.277450857643671),\n",
       "  (581, 5.397055057618438),\n",
       "  (459, 5.651025606649685),\n",
       "  (471, 5.966171266808209),\n",
       "  (552, 6.321385722855429),\n",
       "  (405, 6.5259328069360585),\n",
       "  (183, 7.330608695652174),\n",
       "  (580, 7.418338613164379),\n",
       "  (629, 7.432727913948157),\n",
       "  (653, 7.655466763530693),\n",
       "  (51, 7.957523126297905),\n",
       "  (56, 8.165633475397133),\n",
       "  (34, 8.3401266323704),\n",
       "  (766, 8.555104526080779),\n",
       "  (753, 8.64281320483904),\n",
       "  (378, 8.881373788453434),\n",
       "  (622, 9.05499462943072),\n",
       "  (457, 9.757175925925926),\n",
       "  (590, 9.93424463822767),\n",
       "  (647, 10.019253624910863),\n",
       "  (773, 10.364150479468895),\n",
       "  (776, 10.506231306081755),\n",
       "  (513, 11.054550222921584),\n",
       "  (706, 11.174708377518558),\n",
       "  (76, 11.404491341991342),\n",
       "  (161, 11.718376424798443),\n",
       "  (566, 11.810311011487812),\n",
       "  (729, 12.053474406634258),\n",
       "  (13, 12.15777329102971),\n",
       "  (394, 12.306861313868612),\n",
       "  (204, 12.642771445710858),\n",
       "  (419, 13.005553841406973),\n",
       "  (272, 13.135244624493613),\n",
       "  (11, 13.296845425867508),\n",
       "  (448, 13.601484349790255),\n",
       "  (311, 13.6765087605451),\n",
       "  (218, 14.519807096107476),\n",
       "  (227, 14.640847516498784),\n",
       "  (548, 14.805409202669477),\n",
       "  (32, 14.968394886363637),\n",
       "  (408, 15.027094474153298),\n",
       "  (558, 15.129576453697057),\n",
       "  (721, 15.383576642335766),\n",
       "  (658, 15.502390584773813),\n",
       "  (494, 15.536675267231846),\n",
       "  (93, 15.96628787878788),\n",
       "  (575, 16.79992028696692),\n",
       "  (40, 16.928112449799197),\n",
       "  (402, 16.928112449799197),\n",
       "  (322, 17.051375404530745),\n",
       "  (16, 17.155474155474156),\n",
       "  (611, 17.253786328284896),\n",
       "  (550, 17.289171452009843),\n",
       "  (502, 17.651172529313232),\n",
       "  (42, 17.770236087689714),\n",
       "  (772, 17.95952279505752),\n",
       "  (603, 18.051820128479658),\n",
       "  (312, 18.05955441302485),\n",
       "  (528, 18.05955441302485),\n",
       "  (280, 18.318557149065622),\n",
       "  (600, 18.334493257938234),\n",
       "  (237, 18.503511852502196),\n",
       "  (719, 18.52791208791209),\n",
       "  (306, 18.585097001763668),\n",
       "  (260, 19.124773139745916),\n",
       "  (264, 19.203189066059224),\n",
       "  (152, 19.29107551487414),\n",
       "  (429, 19.361966008268258),\n",
       "  (191, 19.379770114942527),\n",
       "  (547, 19.68752919196637),\n",
       "  (514, 19.696728971962617),\n",
       "  (171, 19.82643461900282),\n",
       "  (731, 20.081467365412102),\n",
       "  (110, 20.158297465327596),\n",
       "  (234, 20.284408084696825),\n",
       "  (582, 20.313734939759037),\n",
       "  (765, 20.392356071601355),\n",
       "  (426, 20.491492464754497),\n",
       "  (478, 20.652131308182263),\n",
       "  (0, 20.83588729609491),\n",
       "  (693, 21.02294264339152),\n",
       "  (669, 21.117735470941884),\n",
       "  (666, 21.149523331660813),\n",
       "  (44, 21.309908998988877),\n",
       "  (768, 21.385591070522576),\n",
       "  (485, 21.48369011213048),\n",
       "  (303, 21.716125708397733),\n",
       "  (639, 21.749742002063982),\n",
       "  (256, 21.919396775871036),\n",
       "  (689, 22.04550209205021),\n",
       "  (116, 22.068586387434554),\n",
       "  (761, 22.080146673651125),\n",
       "  (717, 22.149763531266423),\n",
       "  (168, 22.196419167983148),\n",
       "  (67, 22.208113804004213),\n",
       "  (744, 22.32574152542373),\n",
       "  (174, 22.420744680851065),\n",
       "  (418, 22.55270197966827),\n",
       "  (708, 22.649650725416443),\n",
       "  (267, 23.300718629076837),\n",
       "  (142, 23.32650802434975),\n",
       "  (617, 23.378258458125348),\n",
       "  (685, 23.58757694459989),\n",
       "  (242, 23.787246049661398),\n",
       "  (170, 23.895124716553287),\n",
       "  (357, 24.045065601825442),\n",
       "  (739, 24.14146620847652),\n",
       "  (30, 24.183017785427424),\n",
       "  (620, 24.44953596287703),\n",
       "  (701, 24.577842565597667),\n",
       "  (431, 24.63530099357101),\n",
       "  (62, 24.693028705330992),\n",
       "  (635, 24.823910482921082),\n",
       "  (376, 24.92667060910704),\n",
       "  (694, 24.956187092954412),\n",
       "  (64, 24.970971563981042),\n",
       "  (415, 25.331129807692307),\n",
       "  (486, 25.36161251504212),\n",
       "  (713, 25.468882175226586),\n",
       "  (300, 25.48428053204353),\n",
       "  (529, 25.499697519661222),\n",
       "  (625, 25.499697519661222),\n",
       "  (228, 25.546060606060607),\n",
       "  (425, 25.62370820668693),\n",
       "  (212, 25.639294403892944),\n",
       "  (631, 25.71751067724222),\n",
       "  (163, 25.748930971288942),\n",
       "  (229, 25.82781862745098),\n",
       "  (525, 26.035206917850527),\n",
       "  (391, 26.083539603960396),\n",
       "  (226, 26.16449410304159),\n",
       "  (58, 26.36085053158224),\n",
       "  (178, 26.393863494051345),\n",
       "  (349, 26.576923076923077),\n",
       "  (54, 26.779542566709022),\n",
       "  (328, 26.813613231552164),\n",
       "  (493, 26.968010236724247),\n",
       "  (289, 27.037203335471457),\n",
       "  (115, 27.585732984293195),\n",
       "  (320, 27.658136482939632),\n",
       "  (284, 27.67629678266579),\n",
       "  (623, 27.749177090190916),\n",
       "  (77, 27.804089709762533),\n",
       "  (560, 27.84081902245707),\n",
       "  (181, 27.896095301125083),\n",
       "  (667, 27.95159151193634),\n",
       "  (710, 28.02593085106383),\n",
       "  (338, 28.13818424566088),\n",
       "  (506, 28.175802139037433),\n",
       "  (692, 28.384511784511783),\n",
       "  (153, 28.461174881836598),\n",
       "  (274, 28.538253215978333),\n",
       "  (433, 28.654656696125084),\n",
       "  (73, 28.713215258855588),\n",
       "  (277, 28.713215258855588),\n",
       "  (608, 28.929993136582016),\n",
       "  (380, 28.94986263736264),\n",
       "  (244, 29.00963523743978),\n",
       "  (413, 29.089717046238786),\n",
       "  (656, 29.109806629834253),\n",
       "  (752, 29.150069156293224),\n",
       "  (323, 29.170242214532873),\n",
       "  (728, 29.170242214532873),\n",
       "  (482, 29.190443213296398),\n",
       "  (604, 29.25121443442054),\n",
       "  (96, 29.312239221140473),\n",
       "  (370, 29.35306406685237),\n",
       "  (231, 29.496850944716584),\n",
       "  (681, 29.55890603085554),\n",
       "  (366, 29.8731396172927),\n",
       "  (335, 29.91554293825408),\n",
       "  (358, 30.107857142857142),\n",
       "  (613, 30.324460431654675),\n",
       "  (159, 30.41197691197691),\n",
       "  (527, 30.722303206997086),\n",
       "  (26, 30.78962746530314),\n",
       "  (121, 30.857247437774525),\n",
       "  (599, 30.857247437774525),\n",
       "  (36, 31.01618837380427),\n",
       "  (382, 31.061901252763448),\n",
       "  (7, 31.08480825958702),\n",
       "  (409, 31.199851961509992),\n",
       "  (59, 31.292501855976244),\n",
       "  (125, 31.47946228528753),\n",
       "  (252, 31.57378277153558),\n",
       "  (407, 31.57378277153558),\n",
       "  (444, 31.597451274362818),\n",
       "  (23, 31.621155288822205),\n",
       "  (261, 31.621155288822205),\n",
       "  (134, 31.668670172802404),\n",
       "  (605, 31.716328066215198),\n",
       "  (87, 31.860166288737716),\n",
       "  (705, 31.908402725208177),\n",
       "  (117, 32.12728658536585),\n",
       "  (217, 32.200916730328494),\n",
       "  (245, 32.29961685823755),\n",
       "  (213, 32.32438650306749),\n",
       "  (468, 32.374039938556066),\n",
       "  (15, 32.498843484965306),\n",
       "  (607, 32.498843484965306),\n",
       "  (466, 32.52391975308642),\n",
       "  (559, 32.549034749034746),\n",
       "  (61, 32.67519379844961),\n",
       "  (314, 32.75135975135975),\n",
       "  (652, 32.77682737169518),\n",
       "  (458, 32.82788161993769),\n",
       "  (85, 32.879095163806554),\n",
       "  (737, 32.879095163806554),\n",
       "  (375, 32.98200312989045),\n",
       "  (177, 33.007830853563036),\n",
       "  (714, 33.007830853563036),\n",
       "  (233, 33.11154752553024),\n",
       "  (446, 33.18976377952756),\n",
       "  (216, 33.21591804570528),\n",
       "  (305, 33.21591804570528),\n",
       "  (86, 33.34731012658228),\n",
       "  (427, 33.53301511535402),\n",
       "  (723, 33.53301511535402),\n",
       "  (725, 33.53301511535402),\n",
       "  (341, 33.58645418326693),\n",
       "  (520, 33.66693290734824),\n",
       "  (511, 33.774839743589745),\n",
       "  (396, 33.883440514469456),\n",
       "  (290, 33.99274193548387),\n",
       "  (565, 34.158022690437605),\n",
       "  (273, 34.18572587185726),\n",
       "  (295, 34.21347402597402),\n",
       "  (732, 34.29698942229455),\n",
       "  (602, 34.324918566775246),\n",
       "  (447, 34.38091353996737),\n",
       "  (189, 34.720757825370676),\n",
       "  (71, 34.95107794361526),\n",
       "  (220, 34.95107794361526),\n",
       "  (724, 35.09658617818484),\n",
       "  (601, 35.12583333333333),\n",
       "  (141, 35.15512927439533),\n",
       "  (103, 35.27280334728034),\n",
       "  (428, 35.27280334728034),\n",
       "  (155, 35.33193629505448),\n",
       "  (203, 35.5704641350211),\n",
       "  (503, 35.5704641350211),\n",
       "  (538, 35.5704641350211),\n",
       "  (111, 35.60050675675676),\n",
       "  (377, 35.690939881456394),\n",
       "  (741, 35.72118644067797),\n",
       "  (643, 35.75148430873622),\n",
       "  (327, 35.78183361629881),\n",
       "  (730, 35.8731914893617),\n",
       "  (467, 35.965017064846414),\n",
       "  (318, 36.08818493150685),\n",
       "  (554, 36.08818493150685),\n",
       "  (670, 36.08818493150685),\n",
       "  (301, 36.24333619948409),\n",
       "  (740, 36.24333619948409),\n",
       "  (158, 36.274526678141136),\n",
       "  (504, 36.274526678141136),\n",
       "  (480, 36.30577088716623),\n",
       "  (128, 36.431287813310284),\n",
       "  (385, 36.58940972222222),\n",
       "  (172, 36.68494342906875),\n",
       "  (91, 36.748910200523106),\n",
       "  (420, 36.877515310586176),\n",
       "  (369, 37.03954305799648),\n",
       "  (474, 37.03954305799648),\n",
       "  (496, 37.03954305799648),\n",
       "  (702, 37.10475352112676),\n",
       "  (80, 37.137444933920705),\n",
       "  (432, 37.137444933920705),\n",
       "  (438, 37.137444933920705),\n",
       "  (651, 37.203000882612535),\n",
       "  (637, 37.30176991150442),\n",
       "  (14, 37.401064773735584),\n",
       "  (135, 37.43428063943162),\n",
       "  (734, 37.43428063943162),\n",
       "  (771, 37.56773618538325),\n",
       "  (129, 37.70214669051878),\n",
       "  (442, 37.70214669051878),\n",
       "  (583, 37.70214669051878),\n",
       "  (138, 37.73589973142346),\n",
       "  (46, 37.803587443946185),\n",
       "  (727, 37.803587443946185),\n",
       "  (176, 37.83752244165171),\n",
       "  (186, 37.905575539568346),\n",
       "  (194, 37.905575539568346),\n",
       "  (770, 37.905575539568346),\n",
       "  (157, 37.93969396939694),\n",
       "  (522, 38.042418772563174),\n",
       "  (487, 38.07678410117435),\n",
       "  (530, 38.180253623188406),\n",
       "  (641, 38.21486854034451),\n",
       "  (524, 38.28428701180745),\n",
       "  (564, 38.31909090909091),\n",
       "  (195, 38.42388331814038),\n",
       "  (406, 38.42388331814038),\n",
       "  (441, 38.49406392694064),\n",
       "  (239, 38.529250457038394),\n",
       "  (330, 38.529250457038394),\n",
       "  (336, 38.529250457038394),\n",
       "  (767, 38.529250457038394),\n",
       "  (347, 38.67064220183486),\n",
       "  (778, 38.70615243342516),\n",
       "  (660, 38.74172794117647),\n",
       "  (743, 38.81307550644567),\n",
       "  (101, 38.92059095106187),\n",
       "  (648, 39.028703703703705),\n",
       "  (757, 39.028703703703705),\n",
       "  (556, 39.06487488415199),\n",
       "  (19, 39.21023255813954),\n",
       "  (365, 39.28331780055918),\n",
       "  (644, 39.28331780055918),\n",
       "  (675, 39.356676003734826),\n",
       "  (755, 39.356676003734826),\n",
       "  (392, 39.393457943925235),\n",
       "  (126, 39.69020715630885),\n",
       "  (540, 39.72761545711593),\n",
       "  (481, 39.76509433962264),\n",
       "  (9, 39.80264400377715),\n",
       "  (324, 39.80264400377715),\n",
       "  (733, 39.87795648060549),\n",
       "  (555, 39.915719696969695),\n",
       "  (66, 39.99146110056926),\n",
       "  (292, 39.99146110056926),\n",
       "  (464, 40.029439696106365),\n",
       "  (769, 40.029439696106365),\n",
       "  (401, 40.06749049429658),\n",
       "  (359, 40.14380952380952),\n",
       "  (508, 40.14380952380952),\n",
       "  (703, 40.14380952380952),\n",
       "  (114, 40.297323135755256),\n",
       "  (278, 40.297323135755256),\n",
       "  (606, 40.297323135755256),\n",
       "  (88, 40.33588516746411),\n",
       "  (241, 40.41323106423778),\n",
       "  (28, 40.452015355086374),\n",
       "  (422, 40.452015355086374),\n",
       "  (410, 40.52980769230769),\n",
       "  (246, 40.56881616939365),\n",
       "  (247, 40.56881616939365),\n",
       "  (630, 40.60789980732177),\n",
       "  (4, 40.64705882352941),\n",
       "  (122, 40.64705882352941),\n",
       "  (348, 40.64705882352941),\n",
       "  (664, 40.68629343629344),\n",
       "  (748, 40.76499032882012),\n",
       "  (319, 40.804453049370764),\n",
       "  (206, 40.88360814742968),\n",
       "  (50, 41.1630859375),\n",
       "  (175, 41.20332355816227),\n",
       "  (345, 41.243639921722114),\n",
       "  (398, 41.28403525954946),\n",
       "  (230, 41.36506378802748),\n",
       "  (291, 41.36506378802748),\n",
       "  (223, 41.40569744597249),\n",
       "  (354, 41.40569744597249),\n",
       "  (626, 41.40569744597249),\n",
       "  (332, 41.48720472440945),\n",
       "  (130, 41.56903353057199),\n",
       "  (249, 41.610069101678185),\n",
       "  (722, 41.69238377843719),\n",
       "  (97, 41.77502477700694),\n",
       "  (340, 41.85799404170805),\n",
       "  (780, 41.85799404170805),\n",
       "  (533, 41.94129353233831),\n",
       "  (445, 41.983067729083665),\n",
       "  (3, 42.02492522432702),\n",
       "  (106, 42.02492522432702),\n",
       "  (672, 42.02492522432702),\n",
       "  (567, 42.06686626746507),\n",
       "  (726, 42.06686626746507),\n",
       "  (532, 42.10889110889111),\n",
       "  (329, 42.193193193193196),\n",
       "  (461, 42.193193193193196),\n",
       "  (156, 42.23547094188377),\n",
       "  (682, 42.23547094188377),\n",
       "  (39, 42.27783350050151),\n",
       "  (395, 42.27783350050151),\n",
       "  (615, 42.36281407035176),\n",
       "  (43, 42.405432595573444),\n",
       "  (352, 42.53380423814329),\n",
       "  (238, 42.662955465587046),\n",
       "  (436, 42.79289340101523),\n",
       "  (749, 42.79289340101523),\n",
       "  (5, 42.83638211382114),\n",
       "  (263, 42.83638211382114),\n",
       "  (389, 42.83638211382114),\n",
       "  (754, 42.83638211382114),\n",
       "  (162, 42.92362525458248),\n",
       "  (476, 42.92362525458248),\n",
       "  (587, 42.92362525458248),\n",
       "  (597, 43.01122448979592),\n",
       "  (383, 43.05515832482125),\n",
       "  (414, 43.09918200408998),\n",
       "  (25, 43.143295803480044),\n",
       "  (74, 43.1875),\n",
       "  (545, 43.1875),\n",
       "  (762, 43.1875),\n",
       "  (668, 43.231794871794875),\n",
       "  (143, 43.276180698151954),\n",
       "  (299, 43.276180698151954),\n",
       "  (594, 43.32065775950668),\n",
       "  (37, 43.36522633744856),\n",
       "  (596, 43.36522633744856),\n",
       "  (113, 43.40988671472709),\n",
       "  (140, 43.40988671472709),\n",
       "  (243, 43.40988671472709),\n",
       "  (208, 43.45463917525773),\n",
       "  (646, 43.54442148760331),\n",
       "  (784, 43.54442148760331),\n",
       "  (313, 43.63457556935818),\n",
       "  (452, 43.63457556935818),\n",
       "  (49, 43.67979274611399),\n",
       "  (337, 43.725103734439834),\n",
       "  (759, 43.725103734439834),\n",
       "  (416, 43.770508826583594),\n",
       "  (285, 43.816008316008315),\n",
       "  (197, 43.95307612095933),\n",
       "  (618, 43.95307612095933),\n",
       "  (235, 43.99895615866388),\n",
       "  (251, 43.99895615866388),\n",
       "  (489, 44.09100418410042),\n",
       "  (334, 44.13717277486911),\n",
       "  (707, 44.13717277486911),\n",
       "  (68, 44.27626050420168),\n",
       "  (108, 44.27626050420168),\n",
       "  (133, 44.27626050420168),\n",
       "  (451, 44.27626050420168),\n",
       "  (253, 44.369473684210526),\n",
       "  (294, 44.46308016877637),\n",
       "  (211, 44.55708245243129),\n",
       "  (321, 44.55708245243129),\n",
       "  (423, 44.6042328042328),\n",
       "  (609, 44.6042328042328),\n",
       "  (758, 44.6042328042328),\n",
       "  (123, 44.69883351007423),\n",
       "  (393, 44.69883351007423),\n",
       "  (521, 44.746284501061574),\n",
       "  (585, 44.746284501061574),\n",
       "  (578, 44.793836344314556),\n",
       "  (99, 44.84148936170213),\n",
       "  (255, 44.88924387646432),\n",
       "  (362, 44.88924387646432),\n",
       "  (355, 44.937100213219615),\n",
       "  (421, 45.08128342245989),\n",
       "  (52, 45.12955032119914),\n",
       "  (688, 45.12955032119914),\n",
       "  (690, 45.12955032119914),\n",
       "  (576, 45.32365591397849),\n",
       "  (144, 45.3724434876211),\n",
       "  (469, 45.3724434876211),\n",
       "  (561, 45.3724434876211),\n",
       "  (225, 45.421336206896555),\n",
       "  (490, 45.47033441208198),\n",
       "  (515, 45.47033441208198),\n",
       "  (437, 45.56864864864865),\n",
       "  (700, 45.56864864864865),\n",
       "  (287, 45.61796536796537),\n",
       "  (450, 45.66738894907909),\n",
       "  (296, 45.716919739696316),\n",
       "  (360, 45.76655808903366),\n",
       "  (404, 45.81630434782609),\n",
       "  (70, 45.86615886833515),\n",
       "  (271, 45.86615886833515),\n",
       "  (293, 45.86615886833515),\n",
       "  (691, 45.86615886833515),\n",
       "  (709, 45.86615886833515),\n",
       "  (258, 45.9161220043573),\n",
       "  (591, 45.9161220043573),\n",
       "  (346, 45.96619411123228),\n",
       "  (523, 45.96619411123228),\n",
       "  (781, 46.01637554585153),\n",
       "  (270, 46.11706783369803),\n",
       "  (304, 46.11706783369803),\n",
       "  (673, 46.11706783369803),\n",
       "  (750, 46.11706783369803),\n",
       "  (718, 46.26893523600439),\n",
       "  (276, 46.37073707370737),\n",
       "  (317, 46.42180616740088),\n",
       "  (411, 46.42180616740088),\n",
       "  (570, 46.42180616740088),\n",
       "  (308, 46.5242825607064),\n",
       "  (573, 46.575690607734806),\n",
       "  (463, 46.62721238938053),\n",
       "  (711, 46.62721238938053),\n",
       "  (477, 46.67884828349945),\n",
       "  (94, 46.73059866962306),\n",
       "  (102, 46.73059866962306),\n",
       "  (443, 46.73059866962306),\n",
       "  (24, 46.782463928967815),\n",
       "  (145, 46.782463928967815),\n",
       "  (756, 46.782463928967815),\n",
       "  (79, 46.83444444444444),\n",
       "  (342, 46.83444444444444),\n",
       "  (472, 46.83444444444444),\n",
       "  (593, 46.88654060066741),\n",
       "  (621, 46.93875278396437),\n",
       "  (363, 46.99108138238573),\n",
       "  (577, 46.99108138238573),\n",
       "  (139, 47.043526785714285),\n",
       "  (219, 47.043526785714285),\n",
       "  (665, 47.043526785714285),\n",
       "  (782, 47.043526785714285),\n",
       "  (372, 47.09608938547486),\n",
       "  (417, 47.09608938547486),\n",
       "  (579, 47.09608938547486),\n",
       "  (302, 47.14876957494407),\n",
       "  (509, 47.14876957494407),\n",
       "  (526, 47.14876957494407),\n",
       "  (616, 47.14876957494407),\n",
       "  (109, 47.201567749160134),\n",
       "  (344, 47.201567749160134),\n",
       "  (539, 47.201567749160134),\n",
       "  (149, 47.25448430493274),\n",
       "  (65, 47.30751964085297),\n",
       "  (179, 47.30751964085297),\n",
       "  (250, 47.36067415730337),\n",
       "  (173, 47.41394825646794),\n",
       "  (386, 47.41394825646794),\n",
       "  (124, 47.46734234234234),\n",
       "  (190, 47.52085682074408),\n",
       "  (236, 47.52085682074408),\n",
       "  (645, 47.574492099322796),\n",
       "  (745, 47.628248587570624),\n",
       "  (589, 47.7361268403171),\n",
       "  (747, 47.7361268403171),\n",
       "  (107, 47.844494892167994),\n",
       "  (659, 48.06271379703535),\n",
       "  (118, 48.17257142857143),\n",
       "  (146, 48.17257142857143),\n",
       "  (500, 48.28293241695304),\n",
       "  (683, 48.28293241695304),\n",
       "  (120, 48.39380022962113),\n",
       "  (214, 48.39380022962113),\n",
       "  (262, 48.39380022962113),\n",
       "  (592, 48.39380022962113),\n",
       "  (698, 48.39380022962113),\n",
       "  (488, 48.44942528735632),\n",
       "  (588, 48.44942528735632),\n",
       "  (166, 48.5610599078341),\n",
       "  (193, 48.61707035755479),\n",
       "  (495, 48.61707035755479),\n",
       "  (699, 48.61707035755479),\n",
       "  (661, 48.673210161662816),\n",
       "  (720, 48.673210161662816),\n",
       "  (430, 48.72947976878613),\n",
       "  (55, 48.785879629629626),\n",
       "  (82, 48.785879629629626),\n",
       "  (151, 48.785879629629626),\n",
       "  (735, 48.785879629629626),\n",
       "  (6, 48.84241019698725),\n",
       "  (222, 48.89907192575406),\n",
       "  (507, 48.89907192575406),\n",
       "  (475, 48.955865272938446),\n",
       "  (484, 48.955865272938446),\n",
       "  (517, 48.955865272938446),\n",
       "  (48, 49.01279069767442),\n",
       "  (333, 49.01279069767442),\n",
       "  (519, 49.01279069767442),\n",
       "  (75, 49.06984866123399),\n",
       "  (387, 49.06984866123399),\n",
       "  (655, 49.06984866123399),\n",
       "  (783, 49.06984866123399),\n",
       "  (100, 49.12703962703963),\n",
       "  (738, 49.12703962703963),\n",
       "  (200, 49.18436406067678),\n",
       "  (456, 49.18436406067678),\n",
       "  (492, 49.18436406067678),\n",
       "  (684, 49.18436406067678),\n",
       "  (240, 49.24182242990654),\n",
       "  (69, 49.29941520467836),\n",
       "  (254, 49.29941520467836),\n",
       "  (569, 49.29941520467836),\n",
       "  (136, 49.357142857142854),\n",
       "  (326, 49.357142857142854),\n",
       "  (640, 49.357142857142854),\n",
       "  (775, 49.41500586166471),\n",
       "  (281, 49.47300469483568),\n",
       "  (584, 49.531139835487664),\n",
       "  (269, 49.7063679245283),\n",
       "  (367, 49.7063679245283),\n",
       "  (537, 49.7063679245283),\n",
       "  (642, 49.765053128689495),\n",
       "  (78, 49.82387706855792),\n",
       "  (368, 49.82387706855792),\n",
       "  (614, 49.82387706855792),\n",
       "  (680, 49.82387706855792),\n",
       "  (704, 49.82387706855792),\n",
       "  (112, 49.88284023668639),\n",
       "  (353, 49.88284023668639),\n",
       "  (501, 49.88284023668639),\n",
       "  (574, 49.88284023668639),\n",
       "  (598, 49.88284023668639),\n",
       "  (209, 49.941943127962084),\n",
       "  (435, 49.941943127962084),\n",
       "  (695, 49.941943127962084),\n",
       "  (84, 50.0011862396204),\n",
       "  (105, 50.0011862396204),\n",
       "  (180, 50.0011862396204),\n",
       "  (182, 50.0011862396204),\n",
       "  (104, 50.06057007125891),\n",
       "  (192, 50.06057007125891),\n",
       "  (185, 50.12009512485137),\n",
       "  (479, 50.179761904761904),\n",
       "  (268, 50.23957091775924),\n",
       "  (636, 50.23957091775924),\n",
       "  (634, 50.29952267303103),\n",
       "  (677, 50.29952267303103),\n",
       "  (298, 50.359617682198326),\n",
       "  (331, 50.48023952095809),\n",
       "  (364, 50.48023952095809),\n",
       "  (777, 50.540767386091126),\n",
       "  (440, 50.601440576230495),\n",
       "  (551, 50.78433734939759),\n",
       "  (679, 50.78433734939759),\n",
       "  (137, 50.84559710494572),\n",
       "  (148, 50.84559710494572),\n",
       "  (315, 50.84559710494572),\n",
       "  (563, 50.84559710494572),\n",
       "  (127, 50.90700483091788),\n",
       "  (265, 50.90700483091788),\n",
       "  (388, 50.90700483091788),\n",
       "  (390, 50.90700483091788),\n",
       "  (633, 50.90700483091788),\n",
       "  (662, 50.90700483091788),\n",
       "  (282, 50.96856106408706),\n",
       "  (473, 50.96856106408706),\n",
       "  (2, 51.030266343825666),\n",
       "  (184, 51.092121212121214),\n",
       "  (403, 51.154126213592235),\n",
       "  (562, 51.154126213592235),\n",
       "  (595, 51.154126213592235),\n",
       "  (221, 51.21628189550425),\n",
       "  (686, 51.21628189550425),\n",
       "  (60, 51.27858880778589),\n",
       "  (196, 51.27858880778589),\n",
       "  (434, 51.27858880778589),\n",
       "  (95, 51.52933985330073),\n",
       "  (98, 51.52933985330073),\n",
       "  (531, 51.52933985330073),\n",
       "  (715, 51.52933985330073),\n",
       "  (132, 51.71901840490798),\n",
       "  (424, 51.71901840490798),\n",
       "  (232, 51.846248462484624),\n",
       "  (779, 51.846248462484624),\n",
       "  (199, 51.91009852216749),\n",
       "  (465, 51.91009852216749),\n",
       "  (572, 51.97410604192355),\n",
       "  (742, 51.97410604192355),\n",
       "  (751, 51.97410604192355),\n",
       "  (412, 52.038271604938274),\n",
       "  (27, 52.10259579728059),\n",
       "  (266, 52.16707920792079),\n",
       "  (498, 52.16707920792079),\n",
       "  (568, 52.16707920792079),\n",
       "  (610, 52.16707920792079),\n",
       "  (10, 52.23172242874845),\n",
       "  (512, 52.23172242874845),\n",
       "  (671, 52.23172242874845),\n",
       "  (439, 52.29652605459057),\n",
       "  (632, 52.36149068322981),\n",
       "  (259, 52.42661691542288),\n",
       "  (57, 52.491905354919055),\n",
       "  (164, 52.557356608478806),\n",
       "  (224, 52.557356608478806),\n",
       "  (257, 52.68875),\n",
       "  (491, 52.75469336670839),\n",
       "  (627, 52.75469336670839),\n",
       "  (696, 52.75469336670839),\n",
       "  (119, 52.82080200501253),\n",
       "  (297, 52.887076537013805),\n",
       "  (29, 52.9535175879397),\n",
       "  (373, 52.9535175879397),\n",
       "  (198, 53.08690176322418),\n",
       "  (215, 53.08690176322418),\n",
       "  (310, 53.08690176322418),\n",
       "  (381, 53.15384615384615),\n",
       "  (31, 53.288242730720604),\n",
       "  (207, 53.288242730720604),\n",
       "  (534, 53.288242730720604),\n",
       "  (764, 53.288242730720604),\n",
       "  (774, 53.288242730720604),\n",
       "  (22, 53.35569620253165),\n",
       "  (746, 53.35569620253165),\n",
       "  (676, 53.42332065906211),\n",
       "  (131, 53.49111675126903),\n",
       "  (188, 53.49111675126903),\n",
       "  (470, 53.559085133418044),\n",
       "  (657, 53.559085133418044),\n",
       "  (316, 53.62722646310433),\n",
       "  (248, 53.695541401273886),\n",
       "  (325, 53.764030612244895),\n",
       "  (571, 53.764030612244895),\n",
       "  (283, 53.90153452685422),\n",
       "  (81, 53.97055057618438),\n",
       "  (202, 53.97055057618438),\n",
       "  (286, 54.109114249037226),\n",
       "  (760, 54.109114249037226),\n",
       "  (678, 54.17866323907455),\n",
       "  (785, 54.17866323907455),\n",
       "  (165, 54.24839124839125),\n",
       "  (35, 54.31829896907217),\n",
       "  (339, 54.388387096774196),\n",
       "  (275, 54.458656330749356),\n",
       "  (687, 54.458656330749356),\n",
       "  (154, 54.529107373868044),\n",
       "  (167, 54.529107373868044),\n",
       "  (638, 54.529107373868044),\n",
       "  (505, 54.59974093264249),\n",
       "  (288, 54.74155844155844),\n",
       "  (663, 54.74155844155844),\n",
       "  (712, 54.74155844155844),\n",
       "  (518, 54.812743823146945),\n",
       "  (160, 54.884114583333336),\n",
       "  (400, 54.884114583333336),\n",
       "  (716, 54.95567144719687),\n",
       "  (169, 55.027415143603136),\n",
       "  (624, 55.46184210526316),\n",
       "  (649, 55.608179419525065),\n",
       "  (8, 55.68163804491414),\n",
       "  (21, 55.97742363877822),\n",
       "  (697, 55.97742363877822),\n",
       "  (12, 56.05186170212766),\n",
       "  (462, 56.20133333333333),\n",
       "  (612, 56.27636849132176),\n",
       "  (205, 56.351604278074866),\n",
       "  (379, 56.351604278074866),\n",
       "  (397, 56.351604278074866),\n",
       "  (453, 56.351604278074866),\n",
       "  (510, 56.351604278074866),\n",
       "  (557, 56.351604278074866),\n",
       "  (83, 56.578523489932884),\n",
       "  (147, 56.73082099596232),\n",
       "  (307, 56.73082099596232),\n",
       "  (45, 56.807277628032345),\n",
       "  (544, 56.807277628032345),\n",
       "  (543, 56.883940620782724),\n",
       "  (549, 57.037889039242216),\n",
       "  (72, 57.11517615176152),\n",
       "  (187, 57.11517615176152),\n",
       "  (654, 57.11517615176152),\n",
       "  (460, 57.426430517711175),\n",
       "  (619, 57.66210670314638),\n",
       "  (33, 57.820301783264746),\n",
       "  (497, 57.89972527472528),\n",
       "  (399, 57.97936726272352),\n",
       "  (38, 58.05922865013774),\n",
       "  (47, 58.05922865013774),\n",
       "  (201, 58.05922865013774),\n",
       "  (535, 58.05922865013774),\n",
       "  (536, 58.139310344827585),\n",
       "  (553, 58.30013831258645),\n",
       "  (20, 58.46185852981969),\n",
       "  (541, 58.543055555555554),\n",
       "  (650, 58.543055555555554),\n",
       "  (18, 58.624478442280946),\n",
       "  (210, 58.78800557880056),\n",
       "  (384, 58.78800557880056),\n",
       "  (763, 59.200842696629216),\n",
       "  (279, 59.95874822190612),\n",
       "  (449, 60.04415954415954),\n",
       "  (41, 60.5617816091954),\n",
       "  (351, 60.736311239193085),\n",
       "  (736, 60.736311239193085),\n",
       "  (374, 61.98676470588235),\n",
       "  (63, 79.6805293005671),\n",
       "  (454, 84.81086519114689),\n",
       "  (455, 105.64160401002506),\n",
       "  (542, 147.8982456140351),\n",
       "  (499, 178.60593220338984),\n",
       "  (586, 268.4777070063694),\n",
       "  (674, 282.89261744966444)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Performance Log for the Model\n",
    "\n",
    "We will log the original performance as a reference point as well as the performance of the latest model to the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    simple_log\n",
    "except NameError:\n",
    "    simple_log = []\n",
    "\n",
    "with open('data/simple_log.jsonl') as f:\n",
    "    for line in f:\n",
    "        simple_log.append(json.loads(line))\n",
    "\n",
    "SEQUENCE = simple_log[-1]['sequence'] if len(simple_log) > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a Simple CNN Model to Classify Questions to their Corresponding Tags\n",
    "\n",
    "Now weâ€™re ready to train a model to classify/label questions with tag categories. We start with a simple model with one `Conv1D`/`GlobalMaxPool1D`. We use the functional API and weâ€™ve heavily parametrized the code so as to facilitate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/rjurney/weakly-supervised-learning/runs/9vpdmwcr?apiKey=78682079907f9bebefc601c5d5599a3fb363a1b9\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0920 11:17:28.471515 140318888896320 jupyter.py:104] Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'simple_cnn', 'embedding': 'own', 'architecture': 'Simple Conv1D', 'epochs': 8, 'batch_size': 64, 'filter_count': 128, 'filter_size': 3, 'activation': 'selu', 'conv_padding': 'same', 'sequence': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "W0920 11:17:36.214836 140318888896320 deprecation.py:323] From /home/rjurney/anaconda3/envs/deep/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:4266: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 200, 128)          19328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 786)               101394    \n",
      "=================================================================\n",
      "Total params: 620,722\n",
      "Trainable params: 620,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 758912 samples, validate on 325248 samples\n",
      "Epoch 1/8\n",
      "758912/758912 [==============================] - 150s 197us/sample - loss: 0.3767 - categorical_accuracy: 0.1979 - precision: 0.6360 - recall: 0.1599 - auc: 0.8302 - true_positives: 341662.0000 - false_positives: 195503.0000 - true_negatives: 594182336.0000 - false_negatives: 1794745.0000 - val_loss: 0.2682 - val_categorical_accuracy: 0.3062 - val_precision: 0.5910 - val_recall: 0.4475 - val_auc: 0.9595 - val_true_positives: 409846.0000 - val_false_positives: 283622.0000 - val_true_negatives: 254445760.0000 - val_false_negatives: 505921.0000\n",
      "Epoch 2/8\n",
      "758912/758912 [==============================] - 145s 191us/sample - loss: 0.2068 - categorical_accuracy: 0.3095 - precision: 0.7510 - recall: 0.3527 - auc: 0.9466 - true_positives: 753613.0000 - false_positives: 249843.0000 - true_negatives: 594137216.0000 - false_negatives: 1382794.0000 - val_loss: 0.2650 - val_categorical_accuracy: 0.3039 - val_precision: 0.5336 - val_recall: 0.5612 - val_auc: 0.9725 - val_true_positives: 513962.0000 - val_false_positives: 449175.0000 - val_true_negatives: 254279616.0000 - val_false_negatives: 401805.0000\n",
      "Epoch 3/8\n",
      "758912/758912 [==============================] - 146s 192us/sample - loss: 0.1847 - categorical_accuracy: 0.3149 - precision: 0.7629 - recall: 0.3920 - auc: 0.9565 - true_positives: 837368.0000 - false_positives: 260184.0000 - true_negatives: 594129600.0000 - false_negatives: 1299039.0000 - val_loss: 0.2656 - val_categorical_accuracy: 0.3089 - val_precision: 0.5249 - val_recall: 0.5887 - val_auc: 0.9748 - val_true_positives: 539092.0000 - val_false_positives: 488043.0000 - val_true_negatives: 254241120.0000 - val_false_negatives: 376675.0000\n",
      "Epoch 4/8\n",
      "758912/758912 [==============================] - 148s 195us/sample - loss: 0.1818 - categorical_accuracy: 0.3161 - precision: 0.7623 - recall: 0.3990 - auc: 0.9575 - true_positives: 852494.0000 - false_positives: 265854.0000 - true_negatives: 594123072.0000 - false_negatives: 1283913.0000 - val_loss: 0.2646 - val_categorical_accuracy: 0.3105 - val_precision: 0.5224 - val_recall: 0.5947 - val_auc: 0.9753 - val_true_positives: 544628.0000 - val_false_positives: 497900.0000 - val_true_negatives: 254231712.0000 - val_false_negatives: 371139.0000\n",
      "Epoch 5/8\n",
      "758912/758912 [==============================] - 146s 193us/sample - loss: 0.1800 - categorical_accuracy: 0.3159 - precision: 0.7622 - recall: 0.4031 - auc: 0.9581 - true_positives: 861159.0000 - false_positives: 268611.0000 - true_negatives: 594118720.0000 - false_negatives: 1275248.0000 - val_loss: 0.2634 - val_categorical_accuracy: 0.3095 - val_precision: 0.5211 - val_recall: 0.5990 - val_auc: 0.9755 - val_true_positives: 548520.0000 - val_false_positives: 504167.0000 - val_true_negatives: 254224944.0000 - val_false_negatives: 367247.0000\n",
      "Epoch 6/8\n",
      "758912/758912 [==============================] - 146s 193us/sample - loss: 0.1780 - categorical_accuracy: 0.3163 - precision: 0.7640 - recall: 0.4057 - auc: 0.9589 - true_positives: 866808.0000 - false_positives: 267693.0000 - true_negatives: 594122496.0000 - false_negatives: 1269599.0000 - val_loss: 0.2625 - val_categorical_accuracy: 0.3104 - val_precision: 0.5218 - val_recall: 0.6003 - val_auc: 0.9758 - val_true_positives: 549758.0000 - val_false_positives: 503738.0000 - val_true_negatives: 254225856.0000 - val_false_negatives: 366009.0000\n",
      "Epoch 7/8\n",
      "758912/758912 [==============================] - 147s 193us/sample - loss: 0.1777 - categorical_accuracy: 0.3174 - precision: 0.7637 - recall: 0.4068 - auc: 0.9590 - true_positives: 869070.0000 - false_positives: 268861.0000 - true_negatives: 594119296.0000 - false_negatives: 1267337.0000 - val_loss: 0.2619 - val_categorical_accuracy: 0.3103 - val_precision: 0.5228 - val_recall: 0.5995 - val_auc: 0.9757 - val_true_positives: 549019.0000 - val_false_positives: 501064.0000 - val_true_negatives: 254228400.0000 - val_false_negatives: 366748.0000\n",
      "Epoch 8/8\n",
      "758912/758912 [==============================] - 146s 193us/sample - loss: 0.1777 - categorical_accuracy: 0.3167 - precision: 0.7639 - recall: 0.4066 - auc: 0.9590 - true_positives: 868769.0000 - false_positives: 268483.0000 - true_negatives: 594121280.0000 - false_negatives: 1267638.0000 - val_loss: 0.2618 - val_categorical_accuracy: 0.3103 - val_precision: 0.5231 - val_recall: 0.5993 - val_auc: 0.9757 - val_true_positives: 548830.0000 - val_false_positives: 500415.0000 - val_true_negatives: 254228976.0000 - val_false_negatives: 366937.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = 3\n",
    "EPOCHS              = 8\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "STRIDES             = 1\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "EXPERIMENT_NAME = 'simple_cnn'\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config_dict = {\n",
    "    'name': EXPERIMENT_NAME,\n",
    "    'embedding': 'own',\n",
    "    'architecture': 'Simple Conv1D',\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'filter_count': FILTER_COUNT,\n",
    "    'filter_size': FILTER_SIZE,\n",
    "    'activation': ACTIVATION,\n",
    "    'conv_padding': CONV_PADDING,\n",
    "    'sequence': SEQUENCE\n",
    "}\n",
    "print(config_dict)\n",
    "config.update(\n",
    "    config_dict\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        TOKEN_COUNT,\n",
    "        EMBED_SIZE, \n",
    "        input_length=X_train.shape[1],\n",
    "        embeddings_initializer=RandomUniform(),\n",
    "    )\n",
    ")\n",
    "# model.add(\n",
    "#     Embedding(\n",
    "#         TOKEN_COUNT,\n",
    "#         EMBED_SIZE,\n",
    "#         weights=[embedding_matrix],\n",
    "#         input_length=MAX_LEN,\n",
    "#         trainable=True\n",
    "#     )\n",
    "# )\n",
    "model.add(Dropout(0.1))\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        FILTER_COUNT, \n",
    "        FILTER_SIZE, \n",
    "        padding=CONV_PADDING, \n",
    "        activation=ACTIVATION, \n",
    "        strides=1\n",
    "    )\n",
    ")\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(\n",
    "    Dense(\n",
    "        y_train.shape[1],\n",
    "        activation='sigmoid',\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'categorical_accuracy',\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "        verbose=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        patience=2,\n",
    "        verbos=1,\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=TEST_SPLIT,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Log of the Model's Performance\n",
    "\n",
    "Metrics include names like *precision_66* which aren't consistent between runs. We fix these to cleanup our report on training the model. We also add an f1 score, then make a DataFrame to display the log. This could be extended in repeat experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_metric_name(name):\n",
    "    \"\"\"Remove the trailing _NN, ex. precision_86\"\"\"\n",
    "    if name[-1].isdigit():\n",
    "        repeat_name = '_'.join(name.split('_')[:-1])\n",
    "    else:\n",
    "        repeat_name = name\n",
    "    return repeat_name\n",
    "\n",
    "def fix_value(val):\n",
    "    \"\"\"Convert from numpy to float\"\"\"\n",
    "    return val.item() if isinstance(val, np.float32) else val\n",
    "\n",
    "def fix_metric(name, val):\n",
    "    repeat_name = fix_metric_name(name)\n",
    "    py_val = fix_value(val)\n",
    "    return repeat_name, py_val\n",
    "\n",
    "\n",
    "log = {}\n",
    "for name, val in zip(model.metrics_names, metrics):\n",
    "    \n",
    "    repeat_name, py_val = fix_metric(name, val)\n",
    "    log[repeat_name] = py_val\n",
    "\n",
    "# Add a name and sequence number and an F1 score\n",
    "log.update({'name': EXPERIMENT_NAME})\n",
    "log.update({'sequence': SEQUENCE})\n",
    "log.update({'f1': (log['precision'] * log['recall']) / (log['precision'] + log['recall'])})\n",
    "\n",
    "simple_log.append(log)\n",
    "\n",
    "# Overwrite the old log\n",
    "with open('data/simple_log.jsonl', 'w') as f:\n",
    "    [f.write(json.dumps(l) + '\\n') for l in simple_log]\n",
    "\n",
    "pd.DataFrame([log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Epoch Accuracy\n",
    "\n",
    "We want to know the performance at each epoch so that we don't train needlessly large numbers of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "new_history = {}\n",
    "for key, metrics in history.history.items():\n",
    "    new_history[fix_metric_name(key)] = metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz_keys = ['val_categorical_accuracy', 'val_precision', 'val_recall']\n",
    "# summarize history for accuracy\n",
    "for key in viz_keys:\n",
    "    plt.plot(new_history[key])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(viz_keys, loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Kim-CNN Model to Label Stack Overflow Questions\n",
    "\n",
    "Once again weâ€™re ready to train a model to classify/label questions with tag categories. The model is based on [Kim-CNN](https://arxiv.org/abs/1408.5882), a commonly used convolutional neural network for sentence and document classification. We use the functional API and weâ€™ve heavily parametrized the code so as to facilitate experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Embedding, Flatten, MaxPool1D, GlobalMaxPool1D, Dropout, Conv1D, Input, concatenate\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "EXPERIMENT_NAME = 'kim_cnn_2000_3_4_5_7'\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = [3, 4, 5, 7]\n",
    "EPOCHS              = 5\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'embedding': 'own',\n",
    "        'architecture': 'Kim CNN',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'filter_count': FILTER_COUNT,\n",
    "        'filter_size': FILTER_SIZE,\n",
    "        'activation': ACTIVATION,\n",
    "        'conv_padding': CONV_PADDING,\n",
    "        'sequence': SEQUENCE\n",
    "    }\n",
    ")\n",
    "\n",
    "padded_input = Input(\n",
    "    shape=(X_train.shape[1],),\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "emb = Embedding(\n",
    "    TOKEN_COUNT, \n",
    "    EMBED_SIZE,\n",
    "    embeddings_initializer=RandomUniform(),\n",
    "    input_length=X_train.shape[1]\n",
    ")(padded_input)\n",
    "# emb = Embedding(\n",
    "#     TOKEN_COUNT,\n",
    "#     EMBED_SIZE,\n",
    "#     weights=[embedding_matrix],\n",
    "#     input_length=MAX_LEN,\n",
    "#     trainable=True,\n",
    "# )(padded_input)\n",
    "drp = Dropout(0.1)(emb)\n",
    "\n",
    "# Create convlutions of different sizes\n",
    "convs = []\n",
    "for filter_size in FILTER_SIZE:\n",
    "    f_conv = Conv1D(\n",
    "        filters=FILTER_COUNT,\n",
    "        kernel_size=filter_size,\n",
    "        padding=CONV_PADDING,\n",
    "        activation=ACTIVATION\n",
    "    )(drp)\n",
    "    f_pool = MaxPool1D(filter_size)(f_conv)\n",
    "    convs.append(f_pool)\n",
    "\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "l_conv = Conv1D(\n",
    "    128,\n",
    "    5,\n",
    "    activation=ACTIVATION\n",
    ")(l_merge)\n",
    "l_pool = GlobalMaxPool1D()(l_conv)\n",
    "l_flat = Flatten()(l_pool)\n",
    "l_drp  = Dropout(CONV_DROPOUT_RATIO)(l_flat)\n",
    "l_dense = Dense(\n",
    "    128,\n",
    "    activation=ACTIVATION\n",
    ")(l_drp)\n",
    "out_dense = Dense(\n",
    "    y_train.shape[1],\n",
    "    activation='sigmoid'\n",
    ")(l_dense)\n",
    "\n",
    "model = Model(inputs=padded_input, outputs=out_dense)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "        verbose=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/cnn_tagger.weights.hdf5')\n",
    "metrics = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {}\n",
    "for name, val in zip(model.metrics_names, metrics):\n",
    "    \n",
    "    repeat_name, py_val = fix_metric(name, val)\n",
    "    log[repeat_name] = py_val\n",
    "\n",
    "# Add a name and sequence number and an F1 score\n",
    "log.update({'name': EXPERIMENT_NAME})\n",
    "log.update({'sequence': SEQUENCE})\n",
    "log.update({'f1': (log['precision'] * log['recall']) / (log['precision'] + log['recall'])})\n",
    "\n",
    "simple_log.append(log)\n",
    "\n",
    "# Overwrite the old log\n",
    "with open('data/simple_log.jsonl', 'w') as f:\n",
    "    [f.write(json.dumps(l) + '\\n') for l in simple_log]\n",
    "\n",
    "pd.DataFrame([log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "new_history = {}\n",
    "for key, metrics in history.history.items():\n",
    "    new_history[fix_metric_name(key)] = metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz_keys = ['val_categorical_accuracy', 'val_precision', 'val_recall']\n",
    "# summarize history for accuracy\n",
    "for key in viz_keys:\n",
    "    plt.plot(new_history[key])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(viz_keys, loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare this Run to the 1st and Previous Run\n",
    "\n",
    "To get an idea of performance we need to see where we started and where we just came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to original\n",
    "if len(simple_log) > 1:\n",
    "    d2 = simple_log[-1]\n",
    "    d1 = simple_log[0]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "log_diff_1 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_1['current'] = d2['name']\n",
    "log_diff_1['previous'] = d1['name']\n",
    "\n",
    "# Compare to last run\n",
    "if len(simple_log) > 1:\n",
    "    d1 = simple_log[-2]\n",
    "    d2 = simple_log[-1]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "    \n",
    "log_diff_2 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_2['current'] = d2['name']\n",
    "log_diff_2['previous'] = d1['name']\n",
    "\n",
    "df = pd.DataFrame.from_dict([log_diff_1, log_diff_2])\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('previous')\n",
    "cols.remove('current')\n",
    "show_cols = ['previous', 'current'] + cols\n",
    "df[show_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Last 10 Experiments\n",
    "\n",
    "It can be helpful to see trends of performance among experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(simple_log)\n",
    "log_df['f1'] = (log_df['precision'] * log_df['recall']) / (log_df['precision'] + log_df['recall'])\n",
    "\n",
    "log_df[[\n",
    "    'sequence',\n",
    "    'name',\n",
    "    'loss',\n",
    "    'categorical_accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'f1',\n",
    "    'auc',\n",
    "    'true_positives',\n",
    "    'false_positives',\n",
    "    'true_negatives',\n",
    "    'false_negatives',\n",
    "    'hinge',\n",
    "    'mean_absolute_error',\n",
    "]][0:10 if len(log_df) > 9 else len(log_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Actual Prediction Outputs\n",
    "\n",
    "It is not enough to know theoretical performance. We need to see the actual output of the tagger at different confidence thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COUNT = 1000\n",
    "\n",
    "X_test_text = tokenizer.sequences_to_texts(X_test[:TEST_COUNT])\n",
    "\n",
    "y_test_tags = []\n",
    "for row in y_test[:TEST_COUNT].tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col == 1]\n",
    "    y_test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the threshold for classification\n",
    "\n",
    "This lets us see how well the model generalizes to labeling more classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_THRESHOLD = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > CLASSIFY_THRESHOLD) * 1\n",
    "\n",
    "y_pred_tags = []\n",
    "for row in y_pred.tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col > CLASSIFY_THRESHOLD]\n",
    "    y_pred_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See How Far off we are per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(y_pred, 0).sum(axis=0) - y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Prediction Results\n",
    "\n",
    "It is better to view the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tests = []\n",
    "for x, y, z in zip(X_test_text, y_pred_tags, y_test_tags):\n",
    "    prediction_tests.append({\n",
    "        'Question': x,\n",
    "        'Actual': ' '.join(sorted(z)),\n",
    "        'Predictions': ' '.join(sorted(y)),\n",
    "    })\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(prediction_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Finish\n",
    "\n",
    "That is the big finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
