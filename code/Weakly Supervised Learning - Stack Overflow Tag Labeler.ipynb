{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakly Supervised Learning: Stack Overflow Tag Labeler\n",
    "\n",
    "This is the first project for the book Weakly Supervised Learning, about using natural language processing (NLP) and weakly supervised learning (WSL) to build better models with less data. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier based on 1D Convlutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rjurney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always use a random seed for reproducible results\n",
    "\n",
    "Using random seeds throughout makes our work reproducible - so we get the same tokenization consistently, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPU Support for Tensorflow/Keras\n",
    "\n",
    "The first thing to do is to verify that our JupyterLab Python environment on our Data Science Engine EC2 instance is properly configured to work with its onboard GPU. We use `tensorflow.test.is_gpu_available` and `tensorflow.compat.v2.config.experimental.list_physical_devices` to verify the GPUs are working with Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 or more GPUs is available: True\n",
      "GPUs on tap: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "print(f'1 or more GPUs is available: {gpu_avail}')\n",
    "\n",
    "avail_gpus = tf.compat.v2.config.experimental.list_physical_devices('GPU')\n",
    "print(f'GPUs on tap: {avail_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widen the Max Width of Pandas Columns\n",
    "\n",
    "We want to be able to see the text of the questions in our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Application\n",
    "\n",
    "Setup the batch size, document size, token count for the tokenizer/embedding, the embedding dimensions and the test/train split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 64\n",
    "MAX_LEN     = 200\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE  = 50\n",
    "TEST_SPLIT  = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000, 10,000, 5,000 and 2,000 instances that reduced the maximum imbalance via oversampling using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive. The latter parts of the run could be computed on my own 6 core/64GB machine with NVM drives for data/Spark caching.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_776</th>\n",
       "      <th>label_777</th>\n",
       "      <th>label_778</th>\n",
       "      <th>label_779</th>\n",
       "      <th>label_780</th>\n",
       "      <th>label_781</th>\n",
       "      <th>label_782</th>\n",
       "      <th>label_783</th>\n",
       "      <th>label_784</th>\n",
       "      <th>label_785</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                             _Body  \\\n",
       "0  [How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...   \n",
       "1  [Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...   \n",
       "\n",
       "   label_0  label_1  label_2  label_3  label_4  label_5  label_6  label_7  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   label_8  ...  label_776  label_777  label_778  label_779  label_780  \\\n",
       "0        0  ...          0          0          0          0          0   \n",
       "1        0  ...          0          0          0          0          0   \n",
       "\n",
       "   label_781  label_782  label_783  label_784  label_785  \n",
       "0          0          0          0          0          0  \n",
       "1          0          0          0          0          0  \n",
       "\n",
       "[2 rows x 787 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag limit defines which dataset to load - those with tags having at least 50K, 20K, 10K, 5K or 2K instances\n",
    "TAG_LIMIT = 2000\n",
    "\n",
    "# Pre-computed sorted list of tag/index pairs\n",
    "sorted_all_tags = json.load(open(f'data/stackoverflow/08-05-2019/sorted_all_tags.{TAG_LIMIT}.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1\n",
    "\n",
    "# Load the parquet file using pyarrow for this tag limit, using the sorted tag index to specify the columns\n",
    "posts_df = pd.read_parquet(\n",
    "    f'data/stackoverflow/08-05-2019/Questions.Stratified.Final.{TAG_LIMIT}.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,554,788 Stackoverflow questions with a tag having at least 2,000 occurrences\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,} Stackoverflow questions with a tag having at least 2,000 occurrences'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check the Labels\n",
    "\n",
    "There shouldn't be any rows that are all zeros - it throws off the metrics computed during training. There shouldn't be more than six tags for one question, there is a hard limit in the Stack Overflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero rows: 1,554,788, Total rows: 1,554,788, Non-zero ratio: 1.0, Least tags: 1, Most tags: 6\n"
     ]
    }
   ],
   "source": [
    "test_matrix = posts_df[[f'label_{i}' for i in range(0, max_index)]].as_matrix()\n",
    "\n",
    "tests = np.count_nonzero(test_matrix.sum(axis=1)), \\\n",
    "        test_matrix.sum(axis=1).shape[0], \\\n",
    "        test_matrix.sum(axis=1).min(), \\\n",
    "        test_matrix.sum(axis=1).max()\n",
    "\n",
    "print(f'Non-zero rows: {tests[0]:,}, Total rows: {tests[1]:,}, Non-zero ratio: {tests[0]/tests[1]:,}, Least tags: {tests[2]:,}, Most tags: {tests[3]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Precomputed Indexes for Converting Between Tag Indexes and Tags\n",
    "\n",
    "At the end, when we visualize the predictions of our model versus the test data, we will use these indexes to convert from numeric predictions corresponding to tag indexes in a matrix to actual text tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open(f'data/stackoverflow/08-05-2019/tag_index.{TAG_LIMIT}.json'))\n",
    "index_tag = json.load(open(f'data/stackoverflow/08-05-2019/index_tag.{TAG_LIMIT}.json'))\n",
    "\n",
    "# Sanity check the different files\n",
    "assert( len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "We need to join the previously tokenized text back into a string for use in a Tokenizer, which provides useful properties. In addition, making the number of documents a multiple of batch size is a requirement for Tensorflow/Keras to split work among multiple GPUs and to use certain models such as Elmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 121 Training Count: 1,548,800\n"
     ]
    }
   ],
   "source": [
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# Training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Conserve RAM\n",
    "del posts_df\n",
    "gc.collect()\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the Sequences\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a numpy matrix below.\n",
    "\n",
    "Note that the string `__PAD__` has been used previously to pad the documents, so we use it here for words missing in the top `TOKEN_COUNT` words, in our `Tokenizer`.\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a *numpy* matrix below. \n",
    "\n",
    "In addition to converting the text to numeric sequences with a key, Kerasâ€™ *Tokenizer* class is handy for producing the final results of the model via the [*keras.preprocessing.text.Tokenizer.sequences_to_texts*](https://keras.io/preprocessing/text/#tokenizer) method. Then we use Kerasâ€™ [*keras.preprocessing.sequence.pad_sequences*](https://keras.io/preprocessing/sequence/#pad_sequences) method and check the output to ensure the sequences are all 200 items long or they wonâ€™t convert properly into a matrix. The string `__PAD__` has been used previously to pad the documents, so we reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1548800, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=TOKEN_COUNT,\n",
    "    oov_token='__PAD__'\n",
    ")\n",
    "tokenizer.fit_on_texts(documents)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del documents\n",
    "del sequences\n",
    "gc.collect()\n",
    "\n",
    "# Verify that all padded documents are now the same length\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]) )\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embeddings\n",
    "\n",
    "Stanford defines [GloVe Embeddings](https://nlp.stanford.edu/projects/glove/) as:\n",
    "\n",
    "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "We'll try them out to see if they can beat our own embedding, specific to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 50))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Train Datasets\n",
    "\n",
    "We need one dataset to train with and one separate dataset to test and validate our model with.  The oft used [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) makes it so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del padded_sequences\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Class Weights to Balance Uneven Label Counts\n",
    "\n",
    "Although there has already been filtering and up-sampling of the data to restrict it to a sample of questions with at least one tag that occurs more than 2,000 times, there are still as much as a 8000:1 ratio between common and uncommon labels. Without class weights, the most common label will be much more likely to be predicted than the least common. Class weights will make the loss function consider uncommon classes more than frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(356, 1.0),\n",
       "  (90, 1.0873881204231082),\n",
       "  (361, 1.3316481188181122),\n",
       "  (546, 1.3770594934649385),\n",
       "  (17, 1.6345822067722415),\n",
       "  (516, 1.7743211325870334),\n",
       "  (92, 1.811382670604517),\n",
       "  (343, 2.4092987949520825),\n",
       "  (371, 3.157138238877975),\n",
       "  (1, 3.686995411511878),\n",
       "  (309, 3.9223306969846763),\n",
       "  (89, 4.019693515704154),\n",
       "  (150, 4.610451392671678),\n",
       "  (628, 4.77231891140097),\n",
       "  (483, 5.107946087306377),\n",
       "  (350, 5.164673338214954),\n",
       "  (53, 5.270591165725673),\n",
       "  (581, 5.448139724498992),\n",
       "  (459, 5.677906976744186),\n",
       "  (471, 6.0246761258482415),\n",
       "  (552, 6.332069825436409),\n",
       "  (405, 6.601050278167732),\n",
       "  (629, 7.383425414364641),\n",
       "  (580, 7.415771028037383),\n",
       "  (183, 7.422708138447147),\n",
       "  (653, 7.643467790487658),\n",
       "  (51, 7.91558077186857),\n",
       "  (34, 8.287075718015666),\n",
       "  (753, 8.489334670678703),\n",
       "  (56, 8.502980376398098),\n",
       "  (766, 8.639537257570602),\n",
       "  (378, 8.76781767955801),\n",
       "  (622, 9.085301273794189),\n",
       "  (457, 9.644333029474323),\n",
       "  (590, 9.853162592161429),\n",
       "  (647, 10.145277289435832),\n",
       "  (773, 10.342810590631364),\n",
       "  (776, 10.435475916488574),\n",
       "  (513, 11.123981424691141),\n",
       "  (76, 11.387389003498072),\n",
       "  (706, 11.391475998205474),\n",
       "  (161, 11.826548672566371),\n",
       "  (566, 12.015710770395609),\n",
       "  (729, 12.054500569692365),\n",
       "  (13, 12.116625310173697),\n",
       "  (204, 12.526689689195855),\n",
       "  (394, 12.566366425814115),\n",
       "  (419, 12.932464092900071),\n",
       "  (11, 13.137210264900663),\n",
       "  (272, 13.195925579461594),\n",
       "  (448, 13.823824041811847),\n",
       "  (311, 14.039367466548711),\n",
       "  (227, 14.645057100011535),\n",
       "  (408, 14.812507292031269),\n",
       "  (548, 14.850625804187624),\n",
       "  (218, 15.097871328338686),\n",
       "  (558, 15.13386577661223),\n",
       "  (32, 15.270387298532595),\n",
       "  (494, 15.362778315585674),\n",
       "  (721, 15.450651089205307),\n",
       "  (93, 15.716513988611043),\n",
       "  (658, 15.828200972447325),\n",
       "  (575, 16.67866526537047),\n",
       "  (40, 16.820084790673025),\n",
       "  (322, 17.013937282229964),\n",
       "  (16, 17.193661971830984),\n",
       "  (312, 17.75884739124353),\n",
       "  (611, 17.776253150378047),\n",
       "  (42, 17.80617110799439),\n",
       "  (502, 17.886446886446887),\n",
       "  (550, 17.904103793541108),\n",
       "  (402, 18.11873840445269),\n",
       "  (603, 18.248957884145465),\n",
       "  (528, 18.317414514500072),\n",
       "  (719, 18.4424753050552),\n",
       "  (600, 18.51779463243874),\n",
       "  (772, 18.601904761904763),\n",
       "  (306, 18.686782455107448),\n",
       "  (280, 18.900997469108233),\n",
       "  (429, 18.95744363147678),\n",
       "  (237, 19.05417979888939),\n",
       "  (547, 19.356304314682117),\n",
       "  (264, 19.36516168395363),\n",
       "  (171, 19.385860436707894),\n",
       "  (260, 19.46014714898835),\n",
       "  (514, 19.5862388151805),\n",
       "  (152, 19.76),\n",
       "  (191, 20.22590409431257),\n",
       "  (234, 20.25494575622208),\n",
       "  (731, 20.310030395136778),\n",
       "  (478, 20.47049338922928),\n",
       "  (426, 20.59669046073978),\n",
       "  (666, 20.63351210791484),\n",
       "  (110, 20.734607218683653),\n",
       "  (582, 20.915650741350905),\n",
       "  (485, 21.08236466290269),\n",
       "  (765, 21.117431803060544),\n",
       "  (44, 21.142048293089093),\n",
       "  (693, 21.330309139784948),\n",
       "  (669, 21.5438656032581),\n",
       "  (0, 21.565822999830136),\n",
       "  (256, 21.57681849082257),\n",
       "  (768, 21.646717817561807),\n",
       "  (689, 21.814089347079037),\n",
       "  (116, 21.825339522090424),\n",
       "  (639, 21.900638261169572),\n",
       "  (761, 21.911977908180877),\n",
       "  (303, 22.246013667425967),\n",
       "  (168, 22.36751233262861),\n",
       "  (744, 22.474420251371924),\n",
       "  (708, 22.58637253157801),\n",
       "  (717, 22.602456827487984),\n",
       "  (67, 22.793177737881507),\n",
       "  (174, 23.020489573889392),\n",
       "  (685, 23.08327272727273),\n",
       "  (267, 23.282229965156795),\n",
       "  (418, 23.56310319227914),\n",
       "  (170, 23.58937198067633),\n",
       "  (617, 23.58937198067633),\n",
       "  (142, 24.0678672985782),\n",
       "  (62, 24.187083253953134),\n",
       "  (739, 24.233250620347395),\n",
       "  (620, 24.261035734760174),\n",
       "  (415, 24.363461907503357),\n",
       "  (701, 24.429093707908407),\n",
       "  (242, 24.585205267234702),\n",
       "  (357, 24.585205267234702),\n",
       "  (300, 24.680793157076206),\n",
       "  (529, 24.690392843251654),\n",
       "  (376, 24.75780031201248),\n",
       "  (694, 24.830432231566594),\n",
       "  (625, 24.908377476947223),\n",
       "  (713, 24.96715830875123),\n",
       "  (30, 25.21008737092931),\n",
       "  (58, 25.22010329757648),\n",
       "  (525, 25.255221802267755),\n",
       "  (431, 25.361166600079905),\n",
       "  (631, 25.64808080808081),\n",
       "  (635, 25.809717422240293),\n",
       "  (486, 25.846498371335503),\n",
       "  (178, 25.862293746180484),\n",
       "  (229, 25.86756316218419),\n",
       "  (212, 26.085473597698787),\n",
       "  (228, 26.117671261057396),\n",
       "  (64, 26.15533580552122),\n",
       "  (425, 26.176907216494847),\n",
       "  (328, 26.32345013477089),\n",
       "  (54, 26.482686691697957),\n",
       "  (493, 26.488211975798038),\n",
       "  (349, 26.576931128323217),\n",
       "  (226, 26.582495812395308),\n",
       "  (163, 26.621513944223107),\n",
       "  (752, 26.62709731543624),\n",
       "  (289, 26.694280908326323),\n",
       "  (391, 26.841014799154333),\n",
       "  (623, 27.041107561235357),\n",
       "  (181, 27.515821413090595),\n",
       "  (284, 27.605566427484234),\n",
       "  (320, 27.61757668044377),\n",
       "  (506, 27.665722379603398),\n",
       "  (681, 27.847773634568984),\n",
       "  (115, 28.00750055151114),\n",
       "  (667, 28.144092219020173),\n",
       "  (77, 28.244271412680757),\n",
       "  (96, 28.288324420677363),\n",
       "  (338, 28.288324420677363),\n",
       "  (433, 28.536300292200494),\n",
       "  (153, 28.594144144144146),\n",
       "  (274, 28.730029418420457),\n",
       "  (370, 28.93959425575564),\n",
       "  (73, 28.96600501939311),\n",
       "  (277, 28.96600501939311),\n",
       "  (728, 28.96600501939311),\n",
       "  (692, 29.04552733928163),\n",
       "  (608, 29.1455463728191),\n",
       "  (323, 29.165632896852745),\n",
       "  (244, 29.239520958083833),\n",
       "  (710, 29.259737266651303),\n",
       "  (560, 29.26648224988474),\n",
       "  (656, 29.442949907235622),\n",
       "  (231, 29.47713025307639),\n",
       "  (482, 29.538855281526292),\n",
       "  (380, 29.711677978001404),\n",
       "  (366, 29.760431317393344),\n",
       "  (413, 29.795353203473365),\n",
       "  (159, 29.90061234102685),\n",
       "  (335, 30.01371158392435),\n",
       "  (358, 30.056344696969695),\n",
       "  (604, 30.170627376425855),\n",
       "  (559, 30.875),\n",
       "  (7, 30.920116902094495),\n",
       "  (261, 30.920116902094495),\n",
       "  (217, 31.162984781541482),\n",
       "  (409, 31.30128205128205),\n",
       "  (87, 31.339916070106145),\n",
       "  (599, 31.363142292490117),\n",
       "  (134, 31.487599206349206),\n",
       "  (613, 31.503225806451614),\n",
       "  (705, 31.5188679245283),\n",
       "  (252, 31.542360248447206),\n",
       "  (466, 31.61304780876494),\n",
       "  (407, 31.676147704590818),\n",
       "  (26, 31.787180771156734),\n",
       "  (59, 31.835005015045134),\n",
       "  (382, 31.85897114178168),\n",
       "  (121, 31.97934508816121),\n",
       "  (444, 32.04391721352852),\n",
       "  (605, 32.06819904016166),\n",
       "  (23, 32.125),\n",
       "  (125, 32.403777437468094),\n",
       "  (527, 32.586755646817245),\n",
       "  (723, 32.74645344338406),\n",
       "  (245, 32.77181208053691),\n",
       "  (117, 32.78027368964627),\n",
       "  (213, 32.78873966942149),\n",
       "  (36, 32.82264736297828),\n",
       "  (652, 32.82264736297828),\n",
       "  (468, 32.916256157635466),\n",
       "  (375, 32.94187856772185),\n",
       "  (15, 33.06197916666667),\n",
       "  (714, 33.37486855941115),\n",
       "  (458, 33.4275934702475),\n",
       "  (273, 33.445205479452056),\n",
       "  (520, 33.5246897280169),\n",
       "  (601, 33.53354463814052),\n",
       "  (85, 33.54240422721268),\n",
       "  (737, 33.55126849894292),\n",
       "  (305, 33.569011105235326),\n",
       "  (396, 33.658006362672324),\n",
       "  (511, 33.675862068965515),\n",
       "  (446, 33.78339542309739),\n",
       "  (725, 33.78339542309739),\n",
       "  (71, 33.8193926478423),\n",
       "  (427, 33.91878172588832),\n",
       "  (732, 34.10099382218641),\n",
       "  (428, 34.21126381029372),\n",
       "  (607, 34.248179120582684),\n",
       "  (61, 34.34081687855018),\n",
       "  (314, 34.36870600974553),\n",
       "  (155, 34.38732394366197),\n",
       "  (141, 34.42462039045553),\n",
       "  (216, 34.42462039045553),\n",
       "  (177, 34.43395714673176),\n",
       "  (643, 34.584037047126124),\n",
       "  (496, 34.6879781420765),\n",
       "  (86, 34.86899203515518),\n",
       "  (233, 34.92654745529573),\n",
       "  (565, 34.9457748417286),\n",
       "  (341, 35.02289655172414),\n",
       "  (290, 35.090657822001106),\n",
       "  (327, 35.13921948519236),\n",
       "  (447, 35.13921948519236),\n",
       "  (724, 35.15868180559402),\n",
       "  (377, 35.30533926585095),\n",
       "  (602, 35.5923745444351),\n",
       "  (295, 35.61234221598878),\n",
       "  (103, 35.71251758087201),\n",
       "  (651, 35.74268018018018),\n",
       "  (158, 35.8638418079096),\n",
       "  (111, 35.894260672886624),\n",
       "  (741, 35.904411764705884),\n",
       "  (503, 35.924731182795696),\n",
       "  (438, 35.96543909348442),\n",
       "  (318, 35.97563049022386),\n",
       "  (467, 36.06761363636364),\n",
       "  (189, 36.09837930054023),\n",
       "  (135, 36.180678255913364),\n",
       "  (730, 36.25299828669332),\n",
       "  (504, 36.36723002005156),\n",
       "  (220, 36.38807681284035),\n",
       "  (670, 36.440298507462686),\n",
       "  (203, 36.608419838523645),\n",
       "  (385, 36.67186597342577),\n",
       "  (740, 36.72490598785074),\n",
       "  (474, 36.735532407407405),\n",
       "  (480, 36.76744859542427),\n",
       "  (301, 36.77809965237543),\n",
       "  (644, 36.8956698634118),\n",
       "  (176, 37.07885514018692),\n",
       "  (91, 37.11137094416837),\n",
       "  (186, 37.1439438267993),\n",
       "  (420, 37.16569086651054),\n",
       "  (641, 37.18746338605741),\n",
       "  (778, 37.22017003811199),\n",
       "  (347, 37.25293427230047),\n",
       "  (555, 37.263868506017026),\n",
       "  (487, 37.28575624082232),\n",
       "  (583, 37.318636096413876),\n",
       "  (554, 37.40659988214496),\n",
       "  (172, 37.50605612998523),\n",
       "  (432, 37.51713947990544),\n",
       "  (88, 37.52822938220514),\n",
       "  (129, 37.55042886719905),\n",
       "  (195, 37.57265463154779),\n",
       "  (771, 37.59490672194255),\n",
       "  (538, 37.69536817102138),\n",
       "  (194, 37.706563706563706),\n",
       "  (46, 37.76264128494943),\n",
       "  (530, 37.77387682237429),\n",
       "  (441, 37.78511904761905),\n",
       "  (660, 37.78511904761905),\n",
       "  (743, 37.81888591003872),\n",
       "  (157, 37.84143070044709),\n",
       "  (442, 37.92054958183991),\n",
       "  (336, 37.93187929489095),\n",
       "  (648, 37.97726592880646),\n",
       "  (675, 37.97726592880646),\n",
       "  (522, 37.988629563135845),\n",
       "  (702, 38.034152186938286),\n",
       "  (369, 38.05695443645084),\n",
       "  (14, 38.068365817091454),\n",
       "  (734, 38.11407985589913),\n",
       "  (770, 38.1484375),\n",
       "  (128, 38.182857142857145),\n",
       "  (239, 38.182857142857145),\n",
       "  (767, 38.483782964534704),\n",
       "  (80, 38.53050075872534),\n",
       "  (138, 38.6242774566474),\n",
       "  (637, 38.6242774566474),\n",
       "  (247, 38.86072849709213),\n",
       "  (324, 38.98004298434142),\n",
       "  (330, 38.98004298434142),\n",
       "  (727, 38.98004298434142),\n",
       "  (606, 39.00399385560676),\n",
       "  (524, 39.19666563754245),\n",
       "  (392, 39.28155940594059),\n",
       "  (398, 39.35461872287663),\n",
       "  (406, 39.489269051321926),\n",
       "  (733, 39.58777673838478),\n",
       "  (66, 39.637215110833594),\n",
       "  (365, 39.68677711784933),\n",
       "  (223, 39.71160462934001),\n",
       "  (345, 39.71160462934001),\n",
       "  (564, 39.73646322378717),\n",
       "  (9, 39.748904195366315),\n",
       "  (292, 39.773809523809526),\n",
       "  (556, 39.786273895330616),\n",
       "  (19, 39.7987460815047),\n",
       "  (230, 39.861224489795916),\n",
       "  (703, 39.93645800566216),\n",
       "  (114, 39.9490245437382),\n",
       "  (241, 39.97418136020151),\n",
       "  (238, 40.03721223588773),\n",
       "  (508, 40.13847613025609),\n",
       "  (278, 40.25301204819277),\n",
       "  (626, 40.3425484588497),\n",
       "  (291, 40.35537190082645),\n",
       "  (464, 40.35537190082645),\n",
       "  (122, 40.4969696969697),\n",
       "  (395, 40.613563659628916),\n",
       "  (28, 40.66559897501602),\n",
       "  (359, 40.704713049054185),\n",
       "  (101, 40.730830927173564),\n",
       "  (348, 40.756982343499196),\n",
       "  (594, 40.756982343499196),\n",
       "  (401, 40.783167362672664),\n",
       "  (757, 40.83563846896108),\n",
       "  (445, 40.84877734877735),\n",
       "  (755, 40.9014175257732),\n",
       "  (246, 40.99386503067485),\n",
       "  (540, 40.99386503067485),\n",
       "  (748, 41.08673139158576),\n",
       "  (533, 41.10003237293623),\n",
       "  (126, 41.1399870382372),\n",
       "  (422, 41.19338092147956),\n",
       "  (410, 41.24691358024691),\n",
       "  (749, 41.28715447154472),\n",
       "  (726, 41.36787227109808),\n",
       "  (762, 41.51667756703728),\n",
       "  (664, 41.61193051458538),\n",
       "  (630, 41.666557269445356),\n",
       "  (25, 41.74876685300888),\n",
       "  (769, 41.7625),\n",
       "  (587, 41.83130148270181),\n",
       "  (319, 41.886506103596176),\n",
       "  (3, 41.92800528401585),\n",
       "  (754, 41.94185662371986),\n",
       "  (352, 42.05299768135144),\n",
       "  (436, 42.0948275862069),\n",
       "  (4, 42.122760451227606),\n",
       "  (596, 42.17873754152824),\n",
       "  (481, 42.347565043362245),\n",
       "  (545, 42.37583444592791),\n",
       "  (106, 42.38998330550918),\n",
       "  (383, 42.38998330550918),\n",
       "  (668, 42.40414161656646),\n",
       "  (249, 42.47507527601204),\n",
       "  (567, 42.51774949765573),\n",
       "  (130, 42.54624664879356),\n",
       "  (340, 42.57478202548625),\n",
       "  (263, 42.646288209606986),\n",
       "  (354, 42.67495798319328),\n",
       "  (476, 42.68930733019502),\n",
       "  (43, 42.87673083417764),\n",
       "  (646, 42.891216216216215),\n",
       "  (175, 42.92021636240703),\n",
       "  (143, 42.94925575101489),\n",
       "  (162, 42.97833446174678),\n",
       "  (423, 42.992888587876735),\n",
       "  (523, 42.992888587876735),\n",
       "  (74, 43.036610169491524),\n",
       "  (5, 43.05120379789759),\n",
       "  (332, 43.124320652173914),\n",
       "  (206, 43.19768628785301),\n",
       "  (615, 43.22710248552945),\n",
       "  (208, 43.27130197682345),\n",
       "  (389, 43.28605523354927),\n",
       "  (672, 43.28605523354927),\n",
       "  (140, 43.30081855388813),\n",
       "  (251, 43.30081855388813),\n",
       "  (39, 43.315591948140565),\n",
       "  (722, 43.33037542662116),\n",
       "  (682, 43.35997267759563),\n",
       "  (597, 43.404444444444444),\n",
       "  (50, 43.46388223211229),\n",
       "  (113, 43.583247511156884),\n",
       "  (97, 43.64317634926091),\n",
       "  (293, 43.71831955922865),\n",
       "  (780, 43.76352981730438),\n",
       "  (461, 43.79372197309417),\n",
       "  (299, 43.854231433506044),\n",
       "  (156, 43.869384934346925),\n",
       "  (414, 43.869384934346925),\n",
       "  (784, 43.96052631578947),\n",
       "  (321, 44.006239168110916),\n",
       "  (578, 44.021497919556175),\n",
       "  (618, 44.08263888888889),\n",
       "  (532, 44.097950677318515),\n",
       "  (337, 44.12860618700035),\n",
       "  (416, 44.17466945024356),\n",
       "  (711, 44.17466945024356),\n",
       "  (489, 44.19004524886878),\n",
       "  (166, 44.20543175487465),\n",
       "  (452, 44.20543175487465),\n",
       "  (329, 44.22082897944967),\n",
       "  (108, 44.29797627355199),\n",
       "  (421, 44.31343804537522),\n",
       "  (197, 44.359888190076866),\n",
       "  (515, 44.4219734079776),\n",
       "  (579, 44.577949438202246),\n",
       "  (593, 44.62495606326889),\n",
       "  (235, 44.65634892718959),\n",
       "  (450, 44.79816513761468),\n",
       "  (99, 44.86148409893993),\n",
       "  (317, 44.86148409893993),\n",
       "  (700, 44.86148409893993),\n",
       "  (68, 44.9727240524265),\n",
       "  (144, 44.9727240524265),\n",
       "  (271, 44.988660524450744),\n",
       "  (585, 45.05251951738822),\n",
       "  (70, 45.11656005685857),\n",
       "  (707, 45.148648648648646),\n",
       "  (49, 45.21296296296296),\n",
       "  (688, 45.229070181688634),\n",
       "  (346, 45.24518888096935),\n",
       "  (253, 45.261319073083776),\n",
       "  (691, 45.261319073083776),\n",
       "  (561, 45.277460770328105),\n",
       "  (490, 45.34214285714286),\n",
       "  (360, 45.40701001430615),\n",
       "  (521, 45.42325581395349),\n",
       "  (393, 45.43951324266285),\n",
       "  (750, 45.48835542816195),\n",
       "  (133, 45.684778697373154),\n",
       "  (437, 45.70122390208783),\n",
       "  (756, 45.70122390208783),\n",
       "  (304, 45.800144300144304),\n",
       "  (123, 45.83321299638989),\n",
       "  (718, 45.84976525821596),\n",
       "  (362, 45.86632947976879),\n",
       "  (107, 45.88290567401518),\n",
       "  (355, 45.88290567401518),\n",
       "  (296, 45.96596669080377),\n",
       "  (313, 45.96596669080377),\n",
       "  (69, 46.06603773584906),\n",
       "  (609, 46.08275862068965),\n",
       "  (451, 46.13299418604651),\n",
       "  (139, 46.149763722282806),\n",
       "  (211, 46.16654545454546),\n",
       "  (287, 46.16654545454546),\n",
       "  (262, 46.20014556040757),\n",
       "  (37, 46.21696396068438),\n",
       "  (276, 46.318132068588106),\n",
       "  (463, 46.368882395909424),\n",
       "  (588, 46.368882395909424),\n",
       "  (577, 46.3858238947753),\n",
       "  (149, 46.40277777777778),\n",
       "  (136, 46.43672275054865),\n",
       "  (683, 46.45371386754482),\n",
       "  (404, 46.48773343097766),\n",
       "  (472, 46.53885630498534),\n",
       "  (258, 46.55592225889256),\n",
       "  (285, 46.59009174311927),\n",
       "  (294, 46.59009174311927),\n",
       "  (759, 46.62431142122659),\n",
       "  (255, 46.67573529411764),\n",
       "  (539, 46.72727272727273),\n",
       "  (758, 46.778924097273396),\n",
       "  (222, 46.81342182890855),\n",
       "  (665, 46.86526393503138),\n",
       "  (79, 46.93456561922366),\n",
       "  (334, 47.0040725657164),\n",
       "  (698, 47.0040725657164),\n",
       "  (65, 47.0389032975176),\n",
       "  (243, 47.07378568780126),\n",
       "  (673, 47.09124629080119),\n",
       "  (52, 47.12620638455828),\n",
       "  (94, 47.19628252788104),\n",
       "  (219, 47.19628252788104),\n",
       "  (344, 47.23139880952381),\n",
       "  (704, 47.301788375558864),\n",
       "  (616, 47.37238805970149),\n",
       "  (214, 47.39007092198582),\n",
       "  (469, 47.42547627941726),\n",
       "  (709, 47.42547627941726),\n",
       "  (745, 47.42547627941726),\n",
       "  (659, 47.44319880418535),\n",
       "  (417, 47.49644594089038),\n",
       "  (589, 47.49644594089038),\n",
       "  (591, 47.49644594089038),\n",
       "  (118, 47.532010482965184),\n",
       "  (6, 47.549812734082394),\n",
       "  (570, 47.549812734082394),\n",
       "  (363, 47.58545727136432),\n",
       "  (526, 47.62115528882221),\n",
       "  (55, 47.67480285392415),\n",
       "  (75, 47.692712246431256),\n",
       "  (270, 47.782461422657136),\n",
       "  (315, 47.818455743879476),\n",
       "  (124, 47.854504334715415),\n",
       "  (302, 47.89060731799321),\n",
       "  (576, 47.962976955043445),\n",
       "  (342, 48.07194244604317),\n",
       "  (411, 48.07194244604317),\n",
       "  (24, 48.09015151515152),\n",
       "  (372, 48.12661106899166),\n",
       "  (424, 48.12661106899166),\n",
       "  (475, 48.12661106899166),\n",
       "  (655, 48.16312594840668),\n",
       "  (573, 48.18140417457305),\n",
       "  (145, 48.23632218844985),\n",
       "  (308, 48.23632218844985),\n",
       "  (621, 48.29136553822746),\n",
       "  (281, 48.30974124809741),\n",
       "  (225, 48.36495238095238),\n",
       "  (236, 48.401829965688144),\n",
       "  (507, 48.42028985507246),\n",
       "  (200, 48.45725190839695),\n",
       "  (781, 48.45725190839695),\n",
       "  (109, 48.549904397705546),\n",
       "  (679, 48.549904397705546),\n",
       "  (173, 48.56847742922724),\n",
       "  (179, 48.56847742922724),\n",
       "  (193, 48.58706467661692),\n",
       "  (509, 48.58706467661692),\n",
       "  (368, 48.64291187739464),\n",
       "  (386, 48.69888761028002),\n",
       "  (164, 48.754992319508446),\n",
       "  (102, 48.77372262773723),\n",
       "  (495, 48.792467332820905),\n",
       "  (500, 48.83),\n",
       "  (82, 48.84878799538284),\n",
       "  (592, 48.84878799538284),\n",
       "  (517, 48.92408477842004),\n",
       "  (387, 48.94294525828836),\n",
       "  (645, 48.94294525828836),\n",
       "  (569, 48.98070987654321),\n",
       "  (738, 48.99961404862987),\n",
       "  (120, 49.01853281853282),\n",
       "  (250, 49.03746620316725),\n",
       "  (537, 49.03746620316725),\n",
       "  (699, 49.189461449050754),\n",
       "  (477, 49.26581296080714),\n",
       "  (333, 49.284937888198755),\n",
       "  (112, 49.323232323232325),\n",
       "  (783, 49.38078568650331),\n",
       "  (684, 49.4),\n",
       "  (185, 49.419229272090305),\n",
       "  (182, 49.43847352024922),\n",
       "  (2, 49.47700701480904),\n",
       "  (326, 49.53492001560671),\n",
       "  (48, 49.5736040609137),\n",
       "  (747, 49.59296875),\n",
       "  (190, 49.65115369573719),\n",
       "  (488, 49.67057902973396),\n",
       "  (562, 49.67057902973396),\n",
       "  (690, 49.69001956947162),\n",
       "  (720, 49.69001956947162),\n",
       "  (779, 49.69001956947162),\n",
       "  (199, 49.74843260188088),\n",
       "  (209, 49.74843260188088),\n",
       "  (254, 49.74843260188088),\n",
       "  (492, 49.76793414347315),\n",
       "  (353, 49.78745098039216),\n",
       "  (735, 49.78745098039216),\n",
       "  (501, 49.826530612244895),\n",
       "  (661, 49.90487421383648),\n",
       "  (443, 49.9637937819756),\n",
       "  (364, 50.00315084679008),\n",
       "  (775, 50.00315084679008),\n",
       "  (269, 50.02285263987392),\n",
       "  (367, 50.04256996452503),\n",
       "  (146, 50.12159494670351),\n",
       "  (78, 50.141390205371245),\n",
       "  (782, 50.141390205371245),\n",
       "  (95, 50.200869909054965),\n",
       "  (435, 50.22072784810127),\n",
       "  (221, 50.28039603960396),\n",
       "  (127, 50.34020618556701),\n",
       "  (551, 50.34020618556701),\n",
       "  (636, 50.34020618556701),\n",
       "  (137, 50.42017474185862),\n",
       "  (232, 50.42017474185862),\n",
       "  (265, 50.46025437201908),\n",
       "  (777, 50.46025437201908),\n",
       "  (430, 50.56073277578654),\n",
       "  (676, 50.580876494023904),\n",
       "  (192, 50.601036269430054),\n",
       "  (148, 50.621212121212125),\n",
       "  (240, 50.621212121212125),\n",
       "  (595, 50.621212121212125),\n",
       "  (633, 50.621212121212125),\n",
       "  (598, 50.6414040686079),\n",
       "  (677, 50.70207667731629),\n",
       "  (640, 50.72233320015981),\n",
       "  (456, 50.82385908726982),\n",
       "  (491, 50.905372894947874),\n",
       "  (100, 50.98714859437751),\n",
       "  (439, 50.98714859437751),\n",
       "  (696, 50.98714859437751),\n",
       "  (268, 51.00763358778626),\n",
       "  (84, 51.06918744971842),\n",
       "  (104, 51.0897384305835),\n",
       "  (465, 51.13089005235602),\n",
       "  (563, 51.25474364150182),\n",
       "  (634, 51.25474364150182),\n",
       "  (105, 51.27544426494346),\n",
       "  (642, 51.27544426494346),\n",
       "  (610, 51.42081814499797),\n",
       "  (298, 51.50425963488844),\n",
       "  (715, 51.56701868399675),\n",
       "  (198, 51.65093572009764),\n",
       "  (584, 51.65093572009764),\n",
       "  (695, 51.65093572009764),\n",
       "  (498, 51.67195767195767),\n",
       "  (751, 51.67195767195767),\n",
       "  (484, 51.692996742671006),\n",
       "  (512, 51.692996742671006),\n",
       "  (649, 51.71405295315682),\n",
       "  (310, 51.75621687729311),\n",
       "  (473, 51.798449612403104),\n",
       "  (331, 51.88312219043727),\n",
       "  (316, 51.90433360588716),\n",
       "  (390, 51.9468085106383),\n",
       "  (680, 51.9468085106383),\n",
       "  (572, 51.96807204257061),\n",
       "  (98, 52.01065137238837),\n",
       "  (132, 52.01065137238837),\n",
       "  (180, 52.1816687217427),\n",
       "  (60, 52.224598930481285),\n",
       "  (671, 52.28912685337726),\n",
       "  (518, 52.332234130255564),\n",
       "  (440, 52.39702847709451),\n",
       "  (479, 52.39702847709451),\n",
       "  (519, 52.39702847709451),\n",
       "  (259, 52.418662262592896),\n",
       "  (638, 52.418662262592896),\n",
       "  (282, 52.440313919867826),\n",
       "  (81, 52.46198347107438),\n",
       "  (614, 52.46198347107438),\n",
       "  (151, 52.505376344086024),\n",
       "  (686, 52.570600414078676),\n",
       "  (434, 52.61417322834646),\n",
       "  (568, 52.7234219269103),\n",
       "  (379, 52.74532613211466),\n",
       "  (388, 52.76724854530341),\n",
       "  (742, 52.76724854530341),\n",
       "  (188, 52.85512073272273),\n",
       "  (785, 52.8771345272803),\n",
       "  (10, 52.899166666666666),\n",
       "  (400, 52.92121717382243),\n",
       "  (470, 52.965373383395914),\n",
       "  (184, 52.98747913188648),\n",
       "  (35, 53.03174603174603),\n",
       "  (297, 53.03174603174603),\n",
       "  (154, 53.05390722941914),\n",
       "  (224, 53.14273754709083),\n",
       "  (215, 53.164991624790616),\n",
       "  (266, 53.164991624790616),\n",
       "  (202, 53.18726434855467),\n",
       "  (248, 53.20955574182732),\n",
       "  (165, 53.29890848026868),\n",
       "  (574, 53.29890848026868),\n",
       "  (678, 53.32129357412852),\n",
       "  (657, 53.43350168350168),\n",
       "  (31, 53.456),\n",
       "  (119, 53.456),\n",
       "  (147, 53.5913887716336),\n",
       "  (663, 53.5913887716336),\n",
       "  (632, 53.65934065934066),\n",
       "  (131, 53.72746508675412),\n",
       "  (746, 53.7502116850127),\n",
       "  (169, 53.77297755188479),\n",
       "  (403, 53.77297755188479),\n",
       "  (27, 53.795762711864406),\n",
       "  (760, 53.795762711864406),\n",
       "  (712, 53.81856718948707),\n",
       "  (29, 53.88709677419355),\n",
       "  (207, 53.88709677419355),\n",
       "  (57, 53.93288020390824),\n",
       "  (257, 54.070698466780236),\n",
       "  (462, 54.11679454390452),\n",
       "  (662, 54.11679454390452),\n",
       "  (627, 54.186086214255226),\n",
       "  (196, 54.20922288642186),\n",
       "  (534, 54.23237932507475),\n",
       "  (697, 54.30196749358426),\n",
       "  (275, 54.34845890410959),\n",
       "  (571, 54.39502999143102),\n",
       "  (531, 54.511807642765135),\n",
       "  (736, 54.5352233676976),\n",
       "  (83, 54.55865921787709),\n",
       "  (774, 54.55865921787709),\n",
       "  (187, 54.65260439087387),\n",
       "  (72, 54.77049180327869),\n",
       "  (339, 54.794130340958134),\n",
       "  (381, 54.794130340958134),\n",
       "  (497, 54.794130340958134),\n",
       "  (624, 54.794130340958134),\n",
       "  (160, 54.888888888888886),\n",
       "  (716, 54.936391172652534),\n",
       "  (325, 54.983975747076656),\n",
       "  (373, 55.00779896013865),\n",
       "  (535, 55.03164282618119),\n",
       "  (505, 55.103298611111114),\n",
       "  (283, 55.1272253582284),\n",
       "  (612, 55.2712233347845),\n",
       "  (412, 55.295296167247386),\n",
       "  (544, 55.39179755671902),\n",
       "  (8, 55.440174672489086),\n",
       "  (654, 55.440174672489086),\n",
       "  (12, 55.65892152564665),\n",
       "  (22, 55.73222124670764),\n",
       "  (201, 55.78119507908612),\n",
       "  (167, 55.83025505716799),\n",
       "  (543, 55.83025505716799),\n",
       "  (205, 55.879401408450704),\n",
       "  (288, 55.95328338475099),\n",
       "  (453, 55.95328338475099),\n",
       "  (764, 56.00264666960741),\n",
       "  (541, 56.027360988526034),\n",
       "  (45, 56.05209713024283),\n",
       "  (286, 56.32564330079858),\n",
       "  (21, 56.501112594570536),\n",
       "  (397, 56.576648841354725),\n",
       "  (557, 56.576648841354725),\n",
       "  (460, 56.67767857142857),\n",
       "  (18, 56.82990152193375),\n",
       "  (351, 57.111111111111114),\n",
       "  (399, 57.18828828828829),\n",
       "  (536, 57.18828828828829),\n",
       "  (510, 57.291516245487365),\n",
       "  (687, 57.317381489841985),\n",
       "  (210, 57.62959600544712),\n",
       "  (47, 57.681962744207176),\n",
       "  (553, 57.998172681589764),\n",
       "  (650, 58.077767612076855),\n",
       "  (549, 58.15758131012368),\n",
       "  (307, 58.2910927456382),\n",
       "  (38, 58.39834406623735),\n",
       "  (20, 58.50599078341014),\n",
       "  (384, 58.58698661744347),\n",
       "  (449, 58.58698661744347),\n",
       "  (279, 59.46510538641686),\n",
       "  (619, 59.57672454246833),\n",
       "  (41, 59.9423984891407),\n",
       "  (33, 60.513822688274544),\n",
       "  (374, 61.332367149758454),\n",
       "  (763, 61.96095656417765),\n",
       "  (63, 82.70879478827362),\n",
       "  (454, 83.30577427821522),\n",
       "  (455, 100.20363062352013),\n",
       "  (542, 157.12623762376236),\n",
       "  (499, 179.8271954674221),\n",
       "  (586, 288.5409090909091),\n",
       "  (674, 313.4765432098765)],\n",
       " [(356, 1.0),\n",
       "  (90, 1.097090399875525),\n",
       "  (361, 1.340366885277065),\n",
       "  (546, 1.36488579171506),\n",
       "  (17, 1.5974173085636612),\n",
       "  (516, 1.7724987430869783),\n",
       "  (92, 1.8276308968377397),\n",
       "  (343, 2.4355785837651123),\n",
       "  (371, 3.1130242825607066),\n",
       "  (1, 3.739591620259878),\n",
       "  (309, 3.9380061435353255),\n",
       "  (89, 3.9690402476780187),\n",
       "  (150, 4.610003269042171),\n",
       "  (628, 4.814612495732332),\n",
       "  (350, 5.213308687615527),\n",
       "  (483, 5.219096965210955),\n",
       "  (53, 5.263904441955954),\n",
       "  (581, 5.523697610654132),\n",
       "  (459, 5.629540918163673),\n",
       "  (471, 5.856312292358804),\n",
       "  (552, 6.357980162308386),\n",
       "  (405, 6.456959706959707),\n",
       "  (183, 7.125821121778676),\n",
       "  (580, 7.541176470588235),\n",
       "  (629, 7.602156334231806),\n",
       "  (653, 7.701802293828509),\n",
       "  (51, 7.926925238898257),\n",
       "  (56, 8.058285714285715),\n",
       "  (34, 8.500301386377336),\n",
       "  (766, 8.557038834951456),\n",
       "  (378, 8.89155107187894),\n",
       "  (622, 9.186970684039087),\n",
       "  (753, 9.216993464052287),\n",
       "  (457, 9.567164179104477),\n",
       "  (590, 9.937984496124031),\n",
       "  (647, 10.001418439716312),\n",
       "  (773, 10.399705014749262),\n",
       "  (776, 10.659108087679517),\n",
       "  (706, 10.8727833461835),\n",
       "  (513, 11.051724137931034),\n",
       "  (76, 11.437145174371452),\n",
       "  (729, 11.559016393442622),\n",
       "  (566, 11.72236076475478),\n",
       "  (161, 12.337707786526684),\n",
       "  (13, 12.54626334519573),\n",
       "  (394, 12.704504504504504),\n",
       "  (204, 12.973321067157313),\n",
       "  (419, 13.167133520074696),\n",
       "  (11, 13.291234684260132),\n",
       "  (311, 13.366824644549762),\n",
       "  (272, 13.59884281581485),\n",
       "  (448, 13.638297872340425),\n",
       "  (227, 14.404494382022472),\n",
       "  (32, 14.751046025104603),\n",
       "  (558, 14.986184909670563),\n",
       "  (218, 15.034115138592751),\n",
       "  (408, 15.114683815648446),\n",
       "  (721, 15.179763186221743),\n",
       "  (548, 15.278439869989166),\n",
       "  (658, 15.378407851690294),\n",
       "  (494, 15.547960308710033),\n",
       "  (93, 16.025),\n",
       "  (402, 16.435897435897434),\n",
       "  (550, 16.888622754491017),\n",
       "  (16, 16.908872901678656),\n",
       "  (611, 17.05199516324063),\n",
       "  (312, 17.176613885505482),\n",
       "  (322, 17.239608801955992),\n",
       "  (40, 17.366995073891626),\n",
       "  (575, 17.518012422360247),\n",
       "  (502, 17.58354114713217),\n",
       "  (528, 17.94147582697201),\n",
       "  (600, 18.033248081841432),\n",
       "  (603, 18.172680412371133),\n",
       "  (264, 18.2905317769131),\n",
       "  (280, 18.2905317769131),\n",
       "  (719, 18.361979166666668),\n",
       "  (42, 18.67814569536424),\n",
       "  (191, 18.727755644090305),\n",
       "  (772, 18.802666666666667),\n",
       "  (260, 18.979811574697173),\n",
       "  (237, 19.29138166894665),\n",
       "  (152, 19.317808219178083),\n",
       "  (514, 19.451034482758622),\n",
       "  (765, 19.668061366806135),\n",
       "  (306, 19.695530726256983),\n",
       "  (547, 19.750700280112046),\n",
       "  (582, 19.834036568213783),\n",
       "  (429, 19.889985895627646),\n",
       "  (731, 20.03125),\n",
       "  (171, 20.29064748201439),\n",
       "  (234, 20.29064748201439),\n",
       "  (110, 20.43768115942029),\n",
       "  (426, 20.64714494875549),\n",
       "  (478, 20.799410029498524),\n",
       "  (485, 21.206015037593986),\n",
       "  (256, 21.399089529590288),\n",
       "  (693, 21.399089529590288),\n",
       "  (0, 21.496951219512194),\n",
       "  (669, 21.529770992366412),\n",
       "  (303, 21.56269113149847),\n",
       "  (639, 21.56269113149847),\n",
       "  (761, 21.695384615384615),\n",
       "  (666, 21.93157076205288),\n",
       "  (44, 22.103448275862068),\n",
       "  (768, 22.384126984126983),\n",
       "  (242, 22.455414012738853),\n",
       "  (717, 22.527156549520768),\n",
       "  (689, 22.672025723472668),\n",
       "  (708, 22.781906300484653),\n",
       "  (67, 22.818770226537218),\n",
       "  (174, 22.818770226537218),\n",
       "  (418, 22.818770226537218),\n",
       "  (267, 23.27062706270627),\n",
       "  (116, 23.425249169435215),\n",
       "  (168, 23.46422628951747),\n",
       "  (685, 23.503333333333334),\n",
       "  (739, 23.6214405360134),\n",
       "  (357, 23.820945945945947),\n",
       "  (620, 23.820945945945947),\n",
       "  (617, 24.18867924528302),\n",
       "  (744, 24.18867924528302),\n",
       "  (142, 24.230240549828178),\n",
       "  (635, 24.230240549828178),\n",
       "  (431, 24.313793103448276),\n",
       "  (62, 24.6108202443281),\n",
       "  (30, 24.653846153846153),\n",
       "  (376, 24.697022767075307),\n",
       "  (486, 24.697022767075307),\n",
       "  (170, 24.74035087719298),\n",
       "  (300, 24.95929203539823),\n",
       "  (212, 25.04795737122558),\n",
       "  (64, 25.137254901960784),\n",
       "  (228, 25.137254901960784),\n",
       "  (701, 25.182142857142857),\n",
       "  (425, 25.31777378815081),\n",
       "  (54, 25.409009009009008),\n",
       "  (415, 25.409009009009008),\n",
       "  (115, 25.454873646209386),\n",
       "  (525, 25.454873646209386),\n",
       "  (625, 25.780621572212066),\n",
       "  (163, 25.82783882783883),\n",
       "  (328, 25.82783882783883),\n",
       "  (694, 25.82783882783883),\n",
       "  (77, 25.875229357798165),\n",
       "  (529, 25.875229357798165),\n",
       "  (349, 26.163265306122447),\n",
       "  (631, 26.507518796992482),\n",
       "  (178, 26.60754716981132),\n",
       "  (229, 26.759013282732447),\n",
       "  (284, 26.86095238095238),\n",
       "  (391, 26.912213740458014),\n",
       "  (226, 27.27659574468085),\n",
       "  (181, 27.38252427184466),\n",
       "  (713, 27.54296875),\n",
       "  (560, 27.705304518664047),\n",
       "  (710, 27.705304518664047),\n",
       "  (58, 27.869565217391305),\n",
       "  (338, 27.869565217391305),\n",
       "  (493, 27.98015873015873),\n",
       "  (623, 28.03578528827038),\n",
       "  (274, 28.09163346613546),\n",
       "  (320, 28.09163346613546),\n",
       "  (73, 28.37424547283702),\n",
       "  (244, 28.37424547283702),\n",
       "  (289, 28.431451612903224),\n",
       "  (482, 28.431451612903224),\n",
       "  (323, 28.60446247464503),\n",
       "  (667, 28.60446247464503),\n",
       "  (433, 28.779591836734692),\n",
       "  (153, 28.897540983606557),\n",
       "  (413, 29.016460905349795),\n",
       "  (358, 29.136363636363637),\n",
       "  (380, 29.136363636363637),\n",
       "  (527, 29.136363636363637),\n",
       "  (604, 29.196687370600415),\n",
       "  (692, 29.196687370600415),\n",
       "  (231, 29.25726141078838),\n",
       "  (96, 29.31808731808732),\n",
       "  (506, 29.31808731808732),\n",
       "  (277, 29.379166666666666),\n",
       "  (366, 29.502092050209207),\n",
       "  (656, 29.563941299790358),\n",
       "  (728, 29.813953488372093),\n",
       "  (335, 29.877118644067796),\n",
       "  (159, 29.940552016985137),\n",
       "  (261, 30.068230277185503),\n",
       "  (370, 30.197002141327623),\n",
       "  (26, 30.261802575107296),\n",
       "  (608, 30.32688172043011),\n",
       "  (59, 30.457883369330453),\n",
       "  (407, 30.457883369330453),\n",
       "  (121, 30.523809523809526),\n",
       "  (714, 30.59002169197397),\n",
       "  (252, 30.656521739130437),\n",
       "  (599, 30.656521739130437),\n",
       "  (125, 30.92543859649123),\n",
       "  (752, 30.92543859649123),\n",
       "  (382, 30.993406593406593),\n",
       "  (605, 30.993406593406593),\n",
       "  (61, 31.061674008810574),\n",
       "  (36, 31.130242825607063),\n",
       "  (213, 31.199115044247787),\n",
       "  (444, 31.407572383073497),\n",
       "  (375, 31.689887640449438),\n",
       "  (613, 31.689887640449438),\n",
       "  (7, 31.76126126126126),\n",
       "  (134, 31.83295711060948),\n",
       "  (409, 31.83295711060948),\n",
       "  (607, 31.83295711060948),\n",
       "  (446, 31.90497737556561),\n",
       "  (216, 31.97732426303855),\n",
       "  (245, 32.05),\n",
       "  (652, 32.123006833712985),\n",
       "  (314, 32.19634703196347),\n",
       "  (466, 32.2700228832952),\n",
       "  (117, 32.34403669724771),\n",
       "  (681, 32.34403669724771),\n",
       "  (458, 32.56812933025404),\n",
       "  (723, 32.56812933025404),\n",
       "  (15, 32.64351851851852),\n",
       "  (23, 32.64351851851852),\n",
       "  (85, 32.71925754060325),\n",
       "  (177, 32.7953488372093),\n",
       "  (732, 32.87179487179487),\n",
       "  (87, 33.02576112412178),\n",
       "  (305, 33.10328638497653),\n",
       "  (565, 33.33806146572104),\n",
       "  (217, 33.41706161137441),\n",
       "  (447, 33.41706161137441),\n",
       "  (86, 33.496437054631826),\n",
       "  (233, 33.576190476190476),\n",
       "  (290, 33.656324582338904),\n",
       "  (468, 33.81774580335731),\n",
       "  (559, 33.89903846153846),\n",
       "  (737, 33.89903846153846),\n",
       "  (520, 33.980722891566266),\n",
       "  (705, 33.980722891566266),\n",
       "  (189, 34.14527845036319),\n",
       "  (427, 34.311435523114355),\n",
       "  (725, 34.311435523114355),\n",
       "  (511, 34.479217603911984),\n",
       "  (19, 34.56372549019608),\n",
       "  (602, 34.648648648648646),\n",
       "  (155, 34.73399014778325),\n",
       "  (295, 34.73399014778325),\n",
       "  (637, 34.73399014778325),\n",
       "  (643, 34.73399014778325),\n",
       "  (71, 34.81975308641975),\n",
       "  (396, 34.90594059405941),\n",
       "  (442, 35.07960199004975),\n",
       "  (341, 35.255),\n",
       "  (141, 35.4321608040201),\n",
       "  (220, 35.4321608040201),\n",
       "  (601, 35.4321608040201),\n",
       "  (554, 35.611111111111114),\n",
       "  (420, 35.70126582278481),\n",
       "  (428, 35.70126582278481),\n",
       "  (273, 35.88295165394402),\n",
       "  (730, 35.88295165394402),\n",
       "  (524, 35.974489795918366),\n",
       "  (111, 36.066496163682864),\n",
       "  (330, 36.066496163682864),\n",
       "  (14, 36.15897435897436),\n",
       "  (467, 36.15897435897436),\n",
       "  (530, 36.15897435897436),\n",
       "  (564, 36.15897435897436),\n",
       "  (741, 36.15897435897436),\n",
       "  (377, 36.25192802056555),\n",
       "  (734, 36.25192802056555),\n",
       "  (740, 36.25192802056555),\n",
       "  (128, 36.345360824742265),\n",
       "  (522, 36.345360824742265),\n",
       "  (538, 36.345360824742265),\n",
       "  (91, 36.43927648578811),\n",
       "  (724, 36.43927648578811),\n",
       "  (186, 36.53367875647668),\n",
       "  (195, 36.819843342036556),\n",
       "  (301, 36.916230366492144),\n",
       "  (664, 37.01312335958005),\n",
       "  (172, 37.11052631578947),\n",
       "  (327, 37.11052631578947),\n",
       "  (503, 37.11052631578947),\n",
       "  (555, 37.20844327176781),\n",
       "  (496, 37.3068783068783),\n",
       "  (103, 37.40583554376658),\n",
       "  (480, 37.40583554376658),\n",
       "  (670, 37.40583554376658),\n",
       "  (727, 37.40583554376658),\n",
       "  (158, 37.505319148936174),\n",
       "  (660, 37.505319148936174),\n",
       "  (432, 37.605333333333334),\n",
       "  (481, 37.605333333333334),\n",
       "  (406, 37.705882352941174),\n",
       "  (487, 37.705882352941174),\n",
       "  (203, 37.806970509383376),\n",
       "  (318, 37.806970509383376),\n",
       "  (702, 37.806970509383376),\n",
       "  (129, 37.90860215053763),\n",
       "  (508, 37.90860215053763),\n",
       "  (385, 38.01078167115903),\n",
       "  (46, 38.32065217391305),\n",
       "  (755, 38.32065217391305),\n",
       "  (504, 38.42506811989101),\n",
       "  (138, 38.53005464480874),\n",
       "  (369, 38.53005464480874),\n",
       "  (672, 38.53005464480874),\n",
       "  (9, 38.74175824175824),\n",
       "  (438, 38.74175824175824),\n",
       "  (757, 38.74175824175824),\n",
       "  (206, 38.84848484848485),\n",
       "  (66, 38.95580110497237),\n",
       "  (126, 38.95580110497237),\n",
       "  (651, 38.95580110497237),\n",
       "  (770, 38.95580110497237),\n",
       "  (194, 39.06371191135734),\n",
       "  (743, 39.06371191135734),\n",
       "  (365, 39.172222222222224),\n",
       "  (771, 39.172222222222224),\n",
       "  (135, 39.28133704735376),\n",
       "  (401, 39.28133704735376),\n",
       "  (726, 39.28133704735376),\n",
       "  (39, 39.391061452513966),\n",
       "  (157, 39.391061452513966),\n",
       "  (336, 39.391061452513966),\n",
       "  (641, 39.391061452513966),\n",
       "  (767, 39.391061452513966),\n",
       "  (114, 39.50140056022409),\n",
       "  (176, 39.50140056022409),\n",
       "  (556, 39.50140056022409),\n",
       "  (615, 39.50140056022409),\n",
       "  (630, 39.50140056022409),\n",
       "  (398, 39.6123595505618),\n",
       "  (567, 39.6123595505618),\n",
       "  (648, 39.6123595505618),\n",
       "  (324, 39.72394366197183),\n",
       "  (759, 39.72394366197183),\n",
       "  (101, 39.836158192090394),\n",
       "  (241, 39.836158192090394),\n",
       "  (156, 39.94900849858357),\n",
       "  (319, 39.94900849858357),\n",
       "  (461, 39.94900849858357),\n",
       "  (130, 40.0625),\n",
       "  (4, 40.17663817663818),\n",
       "  (239, 40.17663817663818),\n",
       "  (682, 40.17663817663818),\n",
       "  (733, 40.17663817663818),\n",
       "  (348, 40.29142857142857),\n",
       "  (703, 40.29142857142857),\n",
       "  (778, 40.29142857142857),\n",
       "  (25, 40.40687679083094),\n",
       "  (247, 40.40687679083094),\n",
       "  (540, 40.40687679083094),\n",
       "  (80, 40.52298850574713),\n",
       "  (243, 40.52298850574713),\n",
       "  (359, 40.52298850574713),\n",
       "  (392, 40.52298850574713),\n",
       "  (278, 40.75722543352601),\n",
       "  (410, 40.75722543352601),\n",
       "  (583, 40.75722543352601),\n",
       "  (292, 40.87536231884058),\n",
       "  (294, 40.87536231884058),\n",
       "  (533, 40.99418604651163),\n",
       "  (340, 41.113702623906704),\n",
       "  (441, 41.113702623906704),\n",
       "  (532, 41.113702623906704),\n",
       "  (675, 41.113702623906704),\n",
       "  (140, 41.23391812865497),\n",
       "  (436, 41.23391812865497),\n",
       "  (175, 41.354838709677416),\n",
       "  (769, 41.354838709677416),\n",
       "  (355, 41.476470588235294),\n",
       "  (452, 41.476470588235294),\n",
       "  (748, 41.476470588235294),\n",
       "  (113, 41.59882005899705),\n",
       "  (422, 41.59882005899705),\n",
       "  (474, 41.59882005899705),\n",
       "  (347, 41.72189349112426),\n",
       "  (50, 41.845697329376854),\n",
       "  (143, 41.845697329376854),\n",
       "  (354, 41.845697329376854),\n",
       "  (28, 41.970238095238095),\n",
       "  (43, 41.970238095238095),\n",
       "  (106, 41.970238095238095),\n",
       "  (249, 41.970238095238095),\n",
       "  (597, 41.970238095238095),\n",
       "  (211, 42.0955223880597),\n",
       "  (230, 42.0955223880597),\n",
       "  (291, 42.0955223880597),\n",
       "  (445, 42.0955223880597),\n",
       "  (606, 42.0955223880597),\n",
       "  (332, 42.221556886227546),\n",
       "  (451, 42.221556886227546),\n",
       "  (68, 42.348348348348345),\n",
       "  (235, 42.348348348348345),\n",
       "  (285, 42.348348348348345),\n",
       "  (337, 42.348348348348345),\n",
       "  (389, 42.348348348348345),\n",
       "  (246, 42.475903614457835),\n",
       "  (345, 42.475903614457835),\n",
       "  (476, 42.475903614457835),\n",
       "  (99, 42.60422960725076),\n",
       "  (208, 42.733333333333334),\n",
       "  (780, 42.733333333333334),\n",
       "  (784, 42.733333333333334),\n",
       "  (197, 42.86322188449848),\n",
       "  (450, 42.86322188449848),\n",
       "  (668, 42.86322188449848),\n",
       "  (88, 42.99390243902439),\n",
       "  (626, 42.99390243902439),\n",
       "  (707, 42.99390243902439),\n",
       "  (587, 43.12538226299694),\n",
       "  (352, 43.39076923076923),\n",
       "  (722, 43.39076923076923),\n",
       "  (3, 43.52469135802469),\n",
       "  (5, 43.52469135802469),\n",
       "  (37, 43.52469135802469),\n",
       "  (97, 43.52469135802469),\n",
       "  (133, 43.52469135802469),\n",
       "  (263, 43.52469135802469),\n",
       "  (395, 43.52469135802469),\n",
       "  (102, 43.6594427244582),\n",
       "  (238, 43.6594427244582),\n",
       "  (304, 43.6594427244582),\n",
       "  (576, 43.79503105590062),\n",
       "  (313, 43.93146417445483),\n",
       "  (596, 43.93146417445483),\n",
       "  (749, 43.93146417445483),\n",
       "  (299, 44.06875),\n",
       "  (414, 44.06875),\n",
       "  (464, 44.06875),\n",
       "  (519, 44.06875),\n",
       "  (539, 44.06875),\n",
       "  (644, 44.06875),\n",
       "  (49, 44.34591194968554),\n",
       "  (469, 44.34591194968554),\n",
       "  (223, 44.48580441640379),\n",
       "  (151, 44.62658227848101),\n",
       "  (754, 44.62658227848101),\n",
       "  (65, 44.768253968253966),\n",
       "  (79, 44.768253968253966),\n",
       "  (360, 44.768253968253966),\n",
       "  (521, 44.768253968253966),\n",
       "  (688, 44.768253968253966),\n",
       "  (750, 44.768253968253966),\n",
       "  (758, 44.768253968253966),\n",
       "  (383, 44.910828025477706),\n",
       "  (517, 44.910828025477706),\n",
       "  (646, 44.910828025477706),\n",
       "  (344, 45.054313099041536),\n",
       "  (673, 45.054313099041536),\n",
       "  (74, 45.19871794871795),\n",
       "  (162, 45.19871794871795),\n",
       "  (214, 45.19871794871795),\n",
       "  (270, 45.19871794871795),\n",
       "  (609, 45.19871794871795),\n",
       "  (621, 45.19871794871795),\n",
       "  (700, 45.19871794871795),\n",
       "  (180, 45.344051446945336),\n",
       "  (404, 45.344051446945336),\n",
       "  (585, 45.344051446945336),\n",
       "  (640, 45.344051446945336),\n",
       "  (782, 45.344051446945336),\n",
       "  (122, 45.49032258064516),\n",
       "  (329, 45.49032258064516),\n",
       "  (443, 45.49032258064516),\n",
       "  (475, 45.49032258064516),\n",
       "  (108, 45.637540453074436),\n",
       "  (139, 45.785714285714285),\n",
       "  (411, 45.785714285714285),\n",
       "  (745, 45.785714285714285),\n",
       "  (146, 45.93485342019544),\n",
       "  (190, 45.93485342019544),\n",
       "  (321, 45.93485342019544),\n",
       "  (477, 45.93485342019544),\n",
       "  (636, 45.93485342019544),\n",
       "  (690, 45.93485342019544),\n",
       "  (173, 46.08496732026144),\n",
       "  (293, 46.08496732026144),\n",
       "  (756, 46.08496732026144),\n",
       "  (123, 46.23606557377049),\n",
       "  (185, 46.23606557377049),\n",
       "  (490, 46.23606557377049),\n",
       "  (655, 46.23606557377049),\n",
       "  (251, 46.38815789473684),\n",
       "  (372, 46.38815789473684),\n",
       "  (509, 46.38815789473684),\n",
       "  (537, 46.38815789473684),\n",
       "  (545, 46.38815789473684),\n",
       "  (709, 46.38815789473684),\n",
       "  (735, 46.38815789473684),\n",
       "  (762, 46.38815789473684),\n",
       "  (196, 46.54125412541254),\n",
       "  (317, 46.54125412541254),\n",
       "  (334, 46.54125412541254),\n",
       "  (594, 46.54125412541254),\n",
       "  (24, 46.6953642384106),\n",
       "  (118, 46.6953642384106),\n",
       "  (124, 46.6953642384106),\n",
       "  (137, 46.6953642384106),\n",
       "  (258, 46.6953642384106),\n",
       "  (302, 46.6953642384106),\n",
       "  (507, 46.6953642384106),\n",
       "  (584, 46.6953642384106),\n",
       "  (616, 46.6953642384106),\n",
       "  (691, 46.6953642384106),\n",
       "  (393, 46.85049833887043),\n",
       "  (570, 46.85049833887043),\n",
       "  (255, 47.00666666666667),\n",
       "  (342, 47.00666666666667),\n",
       "  (472, 47.00666666666667),\n",
       "  (747, 47.00666666666667),\n",
       "  (250, 47.163879598662206),\n",
       "  (253, 47.163879598662206),\n",
       "  (276, 47.163879598662206),\n",
       "  (287, 47.163879598662206),\n",
       "  (346, 47.163879598662206),\n",
       "  (367, 47.163879598662206),\n",
       "  (473, 47.163879598662206),\n",
       "  (526, 47.163879598662206),\n",
       "  (642, 47.163879598662206),\n",
       "  (718, 47.163879598662206),\n",
       "  (70, 47.32214765100671),\n",
       "  (144, 47.32214765100671),\n",
       "  (271, 47.32214765100671),\n",
       "  (388, 47.32214765100671),\n",
       "  (495, 47.32214765100671),\n",
       "  (698, 47.32214765100671),\n",
       "  (363, 47.48148148148148),\n",
       "  (479, 47.48148148148148),\n",
       "  (488, 47.48148148148148),\n",
       "  (593, 47.48148148148148),\n",
       "  (296, 47.641891891891895),\n",
       "  (618, 47.641891891891895),\n",
       "  (100, 47.80338983050847),\n",
       "  (308, 47.80338983050847),\n",
       "  (416, 47.80338983050847),\n",
       "  (561, 47.80338983050847),\n",
       "  (592, 47.80338983050847),\n",
       "  (665, 47.80338983050847),\n",
       "  (333, 47.965986394557824),\n",
       "  (423, 47.965986394557824),\n",
       "  (683, 47.965986394557824),\n",
       "  (127, 48.129692832764505),\n",
       "  (269, 48.129692832764505),\n",
       "  (562, 48.129692832764505),\n",
       "  (715, 48.129692832764505),\n",
       "  (240, 48.294520547945204),\n",
       "  (412, 48.294520547945204),\n",
       "  (48, 48.460481099656356),\n",
       "  (145, 48.460481099656356),\n",
       "  (236, 48.460481099656356),\n",
       "  (551, 48.460481099656356),\n",
       "  (588, 48.460481099656356),\n",
       "  (221, 48.62758620689655),\n",
       "  (283, 48.62758620689655),\n",
       "  (390, 48.62758620689655),\n",
       "  (591, 48.62758620689655),\n",
       "  (659, 48.62758620689655),\n",
       "  (57, 48.79584775086505),\n",
       "  (120, 48.79584775086505),\n",
       "  (182, 48.79584775086505),\n",
       "  (188, 48.79584775086505),\n",
       "  (192, 48.79584775086505),\n",
       "  (362, 48.79584775086505),\n",
       "  (437, 48.79584775086505),\n",
       "  (614, 48.79584775086505),\n",
       "  (109, 48.96527777777778),\n",
       "  (386, 48.96527777777778),\n",
       "  (387, 48.96527777777778),\n",
       "  (563, 48.96527777777778),\n",
       "  (781, 48.96527777777778),\n",
       "  (69, 49.13588850174216),\n",
       "  (200, 49.13588850174216),\n",
       "  (573, 49.13588850174216),\n",
       "  (577, 49.13588850174216),\n",
       "  (579, 49.13588850174216),\n",
       "  (193, 49.30769230769231),\n",
       "  (353, 49.30769230769231),\n",
       "  (52, 49.48070175438596),\n",
       "  (331, 49.48070175438596),\n",
       "  (484, 49.48070175438596),\n",
       "  (574, 49.48070175438596),\n",
       "  (578, 49.48070175438596),\n",
       "  (598, 49.48070175438596),\n",
       "  (78, 49.65492957746479),\n",
       "  (148, 49.65492957746479),\n",
       "  (149, 49.65492957746479),\n",
       "  (282, 49.65492957746479),\n",
       "  (421, 49.65492957746479),\n",
       "  (463, 49.65492957746479),\n",
       "  (512, 49.65492957746479),\n",
       "  (209, 49.830388692579504),\n",
       "  (219, 49.830388692579504),\n",
       "  (430, 49.830388692579504),\n",
       "  (489, 49.830388692579504),\n",
       "  (633, 49.830388692579504),\n",
       "  (679, 49.830388692579504),\n",
       "  (742, 49.830388692579504),\n",
       "  (75, 50.00709219858156),\n",
       "  (184, 50.00709219858156),\n",
       "  (589, 50.00709219858156),\n",
       "  (179, 50.18505338078292),\n",
       "  (222, 50.18505338078292),\n",
       "  (225, 50.18505338078292),\n",
       "  (326, 50.18505338078292),\n",
       "  (492, 50.18505338078292),\n",
       "  (501, 50.18505338078292),\n",
       "  (764, 50.18505338078292),\n",
       "  (60, 50.364285714285714),\n",
       "  (95, 50.364285714285714),\n",
       "  (569, 50.364285714285714),\n",
       "  (738, 50.364285714285714),\n",
       "  (751, 50.364285714285714),\n",
       "  (27, 50.54480286738351),\n",
       "  (254, 50.54480286738351),\n",
       "  (259, 50.54480286738351),\n",
       "  (265, 50.54480286738351),\n",
       "  (29, 50.726618705035975),\n",
       "  (297, 50.726618705035975),\n",
       "  (373, 50.726618705035975),\n",
       "  (440, 50.726618705035975),\n",
       "  (677, 50.726618705035975),\n",
       "  (82, 50.90974729241877),\n",
       "  (500, 50.90974729241877),\n",
       "  (678, 50.90974729241877),\n",
       "  (777, 50.90974729241877),\n",
       "  (84, 51.094202898550726),\n",
       "  (94, 51.094202898550726),\n",
       "  (105, 51.094202898550726),\n",
       "  (470, 51.094202898550726),\n",
       "  (515, 51.094202898550726),\n",
       "  (523, 51.094202898550726),\n",
       "  (704, 51.094202898550726),\n",
       "  (711, 51.094202898550726),\n",
       "  (6, 51.28),\n",
       "  (232, 51.28),\n",
       "  (298, 51.28),\n",
       "  (364, 51.28),\n",
       "  (281, 51.46715328467153),\n",
       "  (315, 51.46715328467153),\n",
       "  (634, 51.46715328467153),\n",
       "  (785, 51.46715328467153),\n",
       "  (107, 51.65567765567766),\n",
       "  (595, 51.65567765567766),\n",
       "  (645, 51.65567765567766),\n",
       "  (661, 51.65567765567766),\n",
       "  (424, 51.845588235294116),\n",
       "  (632, 51.845588235294116),\n",
       "  (775, 51.845588235294116),\n",
       "  (104, 52.03690036900369),\n",
       "  (262, 52.03690036900369),\n",
       "  (268, 52.03690036900369),\n",
       "  (417, 52.22962962962963),\n",
       "  (783, 52.22962962962963),\n",
       "  (55, 52.42379182156134),\n",
       "  (132, 52.42379182156134),\n",
       "  (248, 52.42379182156134),\n",
       "  (505, 52.42379182156134),\n",
       "  (531, 52.42379182156134),\n",
       "  (549, 52.42379182156134),\n",
       "  (686, 52.42379182156134),\n",
       "  (699, 52.42379182156134),\n",
       "  (136, 52.61940298507463),\n",
       "  (687, 52.61940298507463),\n",
       "  (716, 52.61940298507463),\n",
       "  (760, 52.61940298507463),\n",
       "  (167, 53.015037593984964),\n",
       "  (169, 53.015037593984964),\n",
       "  (439, 53.015037593984964),\n",
       "  (638, 53.015037593984964),\n",
       "  (650, 53.015037593984964),\n",
       "  (31, 53.21509433962264),\n",
       "  (160, 53.21509433962264),\n",
       "  (307, 53.21509433962264),\n",
       "  (498, 53.21509433962264),\n",
       "  (568, 53.21509433962264),\n",
       "  (662, 53.21509433962264),\n",
       "  (712, 53.21509433962264),\n",
       "  (21, 53.416666666666664),\n",
       "  (98, 53.416666666666664),\n",
       "  (368, 53.416666666666664),\n",
       "  (434, 53.416666666666664),\n",
       "  (465, 53.416666666666664),\n",
       "  (779, 53.416666666666664),\n",
       "  (131, 53.61977186311787),\n",
       "  (680, 53.61977186311787),\n",
       "  (339, 53.82442748091603),\n",
       "  (571, 53.82442748091603),\n",
       "  (207, 54.030651340996165),\n",
       "  (266, 54.030651340996165),\n",
       "  (534, 54.030651340996165),\n",
       "  (624, 54.030651340996165),\n",
       "  (684, 54.030651340996165),\n",
       "  (695, 54.030651340996165),\n",
       "  (697, 54.030651340996165),\n",
       "  (654, 54.238461538461536),\n",
       "  (746, 54.238461538461536),\n",
       "  (22, 54.447876447876446),\n",
       "  (202, 54.447876447876446),\n",
       "  (456, 54.447876447876446),\n",
       "  (627, 54.447876447876446),\n",
       "  (649, 54.447876447876446),\n",
       "  (671, 54.447876447876446),\n",
       "  (198, 54.65891472868217),\n",
       "  (20, 54.8715953307393),\n",
       "  (166, 54.8715953307393),\n",
       "  (288, 54.8715953307393),\n",
       "  (510, 54.8715953307393),\n",
       "  (10, 55.0859375),\n",
       "  (12, 55.0859375),\n",
       "  (379, 55.0859375),\n",
       "  (610, 55.0859375),\n",
       "  (657, 55.0859375),\n",
       "  (164, 55.30196078431373),\n",
       "  (663, 55.30196078431373),\n",
       "  (199, 55.51968503937008),\n",
       "  (205, 55.51968503937008),\n",
       "  (257, 55.51968503937008),\n",
       "  (286, 55.51968503937008),\n",
       "  (316, 55.51968503937008),\n",
       "  (557, 55.51968503937008),\n",
       "  (696, 55.51968503937008),\n",
       "  (310, 55.73913043478261),\n",
       "  (147, 55.96031746031746),\n",
       "  (210, 55.96031746031746),\n",
       "  (2, 56.18326693227092),\n",
       "  (45, 56.18326693227092),\n",
       "  (83, 56.18326693227092),\n",
       "  (154, 56.18326693227092),\n",
       "  (275, 56.18326693227092),\n",
       "  (491, 56.18326693227092),\n",
       "  (774, 56.18326693227092),\n",
       "  (35, 56.408),\n",
       "  (112, 56.408),\n",
       "  (224, 56.408),\n",
       "  (720, 56.408),\n",
       "  (81, 56.63453815261044),\n",
       "  (397, 56.63453815261044),\n",
       "  (399, 56.63453815261044),\n",
       "  (543, 56.63453815261044),\n",
       "  (763, 56.63453815261044),\n",
       "  (119, 56.86290322580645),\n",
       "  (544, 56.86290322580645),\n",
       "  (72, 57.093117408906885),\n",
       "  (165, 57.093117408906885),\n",
       "  (403, 57.093117408906885),\n",
       "  (33, 57.32520325203252),\n",
       "  (384, 57.32520325203252),\n",
       "  (676, 57.32520325203252),\n",
       "  (572, 57.559183673469384),\n",
       "  (215, 58.03292181069959),\n",
       "  (553, 58.03292181069959),\n",
       "  (201, 58.27272727272727),\n",
       "  (381, 58.51452282157676),\n",
       "  (460, 58.51452282157676),\n",
       "  (536, 58.75833333333333),\n",
       "  (400, 59.00418410041841),\n",
       "  (497, 59.00418410041841),\n",
       "  (38, 59.252100840336134),\n",
       "  (435, 59.252100840336134),\n",
       "  (8, 59.50210970464135),\n",
       "  (535, 59.50210970464135),\n",
       "  (541, 59.50210970464135),\n",
       "  (518, 59.75423728813559),\n",
       "  (325, 60.00851063829787),\n",
       "  (612, 60.00851063829787),\n",
       "  (187, 60.26495726495727),\n",
       "  (619, 60.26495726495727),\n",
       "  (736, 60.78448275862069),\n",
       "  (449, 61.580786026200876),\n",
       "  (462, 61.580786026200876),\n",
       "  (453, 62.12334801762115),\n",
       "  (18, 62.39823008849557),\n",
       "  (41, 62.675555555555555),\n",
       "  (374, 63.237668161434975),\n",
       "  (279, 63.52252252252252),\n",
       "  (351, 63.52252252252252),\n",
       "  (47, 65.5906976744186),\n",
       "  (454, 81.04597701149426),\n",
       "  (63, 81.98837209302326),\n",
       "  (455, 102.18840579710145),\n",
       "  (542, 153.2826086956522),\n",
       "  (499, 171.97560975609755),\n",
       "  (674, 251.82142857142858),\n",
       "  (586, 276.5098039215686)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Performance Log for the Model\n",
    "\n",
    "We will log the original performance as a reference point as well as the performance of the latest model to the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    simple_log\n",
    "except NameError:\n",
    "    simple_log = []\n",
    "\n",
    "with open('data/simple_log.jsonl') as f:\n",
    "    for line in f:\n",
    "        simple_log.append(json.loads(line))\n",
    "\n",
    "SEQUENCE = simple_log[-1]['sequence'] if len(simple_log) > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a Simple CNN Model to Classify Questions to their Corresponding Tags\n",
    "\n",
    "Now weâ€™re ready to train a model to classify/label questions with tag categories. We start with a simple model with one `Conv1D`/`GlobalMaxPool1D`. We use the functional API and weâ€™ve heavily parametrized the code so as to facilitate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.028156400370478098,\n",
       "  'categorical_accuracy': 0.4214495122432709,\n",
       "  'precision': 0.6774758100509644,\n",
       "  'recall': 0.6599706411361694,\n",
       "  'binary_accuracy': 0.9903233051300049,\n",
       "  'hinge': 1.0019246339797974,\n",
       "  'auc': 0.9820438027381897,\n",
       "  'accuracy': 0.0005144624155946076,\n",
       "  'mean_absolute_error': 0.01671472191810608,\n",
       "  'mean_absolute_percentage_error': 11342052.0,\n",
       "  'true_positives': 209903.0,\n",
       "  'false_positives': 99928.0,\n",
       "  'true_negatives': 21086010.0,\n",
       "  'false_negatives': 108146.0,\n",
       "  'name': 'basic_cnn',\n",
       "  'sequence': 36,\n",
       "  'f1': 0.33430433369255025},\n",
       " {'loss': 0.01694748630270624,\n",
       "  'categorical_accuracy': 0.3807617127895355,\n",
       "  'precision': 0.6619125604629517,\n",
       "  'recall': 0.5946587920188904,\n",
       "  'binary_accuracy': 0.9945917129516602,\n",
       "  'hinge': 1.002063512802124,\n",
       "  'auc': 0.9780333042144775,\n",
       "  'accuracy': 0.0007571718888357282,\n",
       "  'mean_absolute_error': 0.00968870334327221,\n",
       "  'mean_absolute_percentage_error': 6508402.5,\n",
       "  'true_positives': 242908.0,\n",
       "  'false_positives': 124071.0,\n",
       "  'true_negatives': 53043088.0,\n",
       "  'false_negatives': 165575.0,\n",
       "  'name': 'basic_cnn_5000_limit',\n",
       "  'sequence': 37,\n",
       "  'f1': 0.3132429550057862},\n",
       " {'loss': 0.017176187729819502,\n",
       "  'categorical_accuracy': 0.381195068359375,\n",
       "  'precision': 0.6433826088905334,\n",
       "  'recall': 0.611038863658905,\n",
       "  'binary_accuracy': 0.9944475293159485,\n",
       "  'hinge': 1.0023584365844727,\n",
       "  'auc': 0.9792392253875732,\n",
       "  'accuracy': 0.0009749013115651906,\n",
       "  'mean_absolute_error': 0.009982646442949772,\n",
       "  'mean_absolute_percentage_error': 6896639.0,\n",
       "  'true_positives': 249599.0,\n",
       "  'false_positives': 138349.0,\n",
       "  'true_negatives': 53028884.0,\n",
       "  'false_negatives': 158884.0,\n",
       "  'name': 'basic_cnn_5000_limit_10_epochs',\n",
       "  'sequence': 38,\n",
       "  'f1': 0.3133968820187582},\n",
       " {'loss': 0.013641377153544455,\n",
       "  'categorical_accuracy': 0.3978332579135895,\n",
       "  'precision': 0.7862475514411926,\n",
       "  'recall': 0.548923671245575,\n",
       "  'binary_accuracy': 0.9954243898391724,\n",
       "  'hinge': 0.9992377161979675,\n",
       "  'auc': 0.9775896072387695,\n",
       "  'accuracy': 0.09095804393291473,\n",
       "  'mean_absolute_error': 0.006862356327474117,\n",
       "  'mean_absolute_percentage_error': 3360255.5,\n",
       "  'true_positives': 224226.0,\n",
       "  'false_positives': 60959.0,\n",
       "  'true_negatives': 53106280.0,\n",
       "  'false_negatives': 184257.0,\n",
       "  'name': 'kim_cnn_5000_limit_10_epochs',\n",
       "  'sequence': 40,\n",
       "  'f1': 0.323246850375081},\n",
       " {'loss': 0.014030436042412475,\n",
       "  'categorical_accuracy': 0.3990112245082855,\n",
       "  'precision': 0.7948366403579712,\n",
       "  'recall': 0.5220902562141418,\n",
       "  'binary_accuracy': 0.9953285455703735,\n",
       "  'hinge': 0.9993232488632202,\n",
       "  'auc': 0.9759425520896912,\n",
       "  'accuracy': 0.07194202393293381,\n",
       "  'mean_absolute_error': 0.006946335546672344,\n",
       "  'mean_absolute_percentage_error': 3280998.75,\n",
       "  'true_positives': 213265.0,\n",
       "  'false_positives': 55048.0,\n",
       "  'true_negatives': 53112236.0,\n",
       "  'false_negatives': 195218.0,\n",
       "  'name': 'kim_cnn_5000_limit_10_epochs_save_best',\n",
       "  'sequence': 41,\n",
       "  'f1': 0.31510971967619567},\n",
       " {'loss': 0.024671485709404806,\n",
       "  'categorical_accuracy': 0.3803955018520355,\n",
       "  'precision': 0.4904581904411316,\n",
       "  'recall': 0.7498427033424377,\n",
       "  'binary_accuracy': 0.9921571016311646,\n",
       "  'hinge': 1.0084298849105835,\n",
       "  'auc': 0.9857017993927002,\n",
       "  'accuracy': 0.0006525535718537867,\n",
       "  'mean_absolute_error': 0.01605524867773056,\n",
       "  'mean_absolute_percentage_error': 13890733.0,\n",
       "  'true_positives': 306298.0,\n",
       "  'false_positives': 318216.0,\n",
       "  'true_negatives': 52849048.0,\n",
       "  'false_negatives': 102185.0,\n",
       "  'name': 'simple_5000_limit_10_epochs_save_best',\n",
       "  'sequence': 42,\n",
       "  'f1': 0.29651393241759044},\n",
       " {'loss': 0.02332179286331666,\n",
       "  'categorical_accuracy': 0.3853210508823395,\n",
       "  'precision': 0.5106842517852783,\n",
       "  'recall': 0.7252468466758728,\n",
       "  'binary_accuracy': 0.9926097989082336,\n",
       "  'hinge': 1.0073416233062744,\n",
       "  'auc': 0.9842875003814697,\n",
       "  'accuracy': 0.0008280996116809547,\n",
       "  'mean_absolute_error': 0.014967193827033043,\n",
       "  'mean_absolute_percentage_error': 12642280.0,\n",
       "  'true_positives': 296251.0,\n",
       "  'false_positives': 283855.0,\n",
       "  'true_negatives': 52883420.0,\n",
       "  'false_negatives': 112232.0,\n",
       "  'name': 'kim_cnn_limit_10_epochs_save_best',\n",
       "  'sequence': 44,\n",
       "  'f1': 0.2996705428930854},\n",
       " {'loss': 0.02494977526657749,\n",
       "  'categorical_accuracy': 0.38348388671875,\n",
       "  'precision': 0.48939934372901917,\n",
       "  'recall': 0.7291515469551086,\n",
       "  'binary_accuracy': 0.9921374320983887,\n",
       "  'hinge': 1.0085182189941406,\n",
       "  'auc': 0.9841805100440979,\n",
       "  'accuracy': 0.0005458820378407836,\n",
       "  'mean_absolute_error': 0.016143226996064186,\n",
       "  'mean_absolute_percentage_error': 13851891.0,\n",
       "  'true_positives': 297846.0,\n",
       "  'false_positives': 310749.0,\n",
       "  'true_negatives': 52856432.0,\n",
       "  'false_negatives': 110637.0,\n",
       "  'name': 'kim_cnn_limit_10_epochs_save_best_2',\n",
       "  'sequence': 45,\n",
       "  'f1': 0.2928447972808801},\n",
       " {'loss': 0.017647538313734508,\n",
       "  'categorical_accuracy': 0.31393980979919434,\n",
       "  'precision': 0.3907146155834198,\n",
       "  'recall': 0.718410074710846,\n",
       "  'binary_accuracy': 0.9949800968170166,\n",
       "  'hinge': 1.0085320472717285,\n",
       "  'auc': 0.9840664267539978,\n",
       "  'accuracy': 0.0018648107070475817,\n",
       "  'mean_absolute_error': 0.012112497352063656,\n",
       "  'mean_absolute_percentage_error': 11001994.0,\n",
       "  'true_positives': 313114.0,\n",
       "  'false_positives': 488274.0,\n",
       "  'true_negatives': 120811464.0,\n",
       "  'false_negatives': 122729.0,\n",
       "  'name': 'kim_cnn_2000',\n",
       "  'sequence': 46,\n",
       "  'f1': 0.25307642921323154}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = 3\n",
    "EPOCHS              = 5\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "STRIDES             = 1\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "EXPERIMENT_NAME = 'simple_cnn'\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'embedding': 'own',\n",
    "        'architecture': 'Kim CNN',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'filter_count': FILTER_COUNT,\n",
    "        'filter_size': FILTER_SIZE,\n",
    "        'activation': ACTIVATION,\n",
    "        'conv_padding': CONV_PADDING,\n",
    "        'sequence': SEQUENCE\n",
    "    }\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        TOKEN_COUNT,\n",
    "        EMBED_SIZE, \n",
    "        input_length=X_train.shape[1],\n",
    "        embeddings_initializer=RandomUniform(),\n",
    "    )\n",
    ")\n",
    "# model.add(\n",
    "#     Embedding(\n",
    "#         TOKEN_COUNT,\n",
    "#         EMBED_SIZE,\n",
    "#         weights=[embedding_matrix],\n",
    "#         input_length=MAX_LEN,\n",
    "#         trainable=True\n",
    "#     )\n",
    "# )\n",
    "model.add(Dropout(0.1))\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        FILTER_COUNT, \n",
    "        FILTER_SIZE, \n",
    "        padding=CONV_PADDING, \n",
    "        activation=ACTIVATION, \n",
    "        strides=1\n",
    "    )\n",
    ")\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(\n",
    "    Dense(\n",
    "        y_train.shape[1],\n",
    "        activation='sigmoid',\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'categorical_accuracy',\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        patience=2\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=TEST_SPLIT,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Kim-CNN Model to Label Stack Overflow Questions\n",
    "\n",
    "Once again weâ€™re ready to train a model to classify/label questions with tag categories. The model is based on [Kim-CNN](https://arxiv.org/abs/1408.5882), a commonly used convolutional neural network for sentence and document classification. We use the functional API and weâ€™ve heavily parametrized the code so as to facilitate experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simple_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb30cadc9bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mCONV_DROPOUT_RATIO\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mEXPERIMENT_NAME\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msimple_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RENAME YOUR EXPERIMENT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RENAME YOUR EXPERIMENT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_log' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Embedding, Flatten, MaxPool1D, GlobalMaxPool1D, Dropout, Conv1D, Input, concatenate\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "EXPERIMENT_NAME = 'kim_cnn_2000_embed_50_drop'\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = [3, 4, 5]\n",
    "EPOCHS              = 5\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'embedding': 'own',\n",
    "        'architecture': 'Kim CNN',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'filter_count': FILTER_COUNT,\n",
    "        'filter_size': FILTER_SIZE,\n",
    "        'activation': ACTIVATION,\n",
    "        'conv_padding': CONV_PADDING,\n",
    "        'sequence': SEQUENCE\n",
    "    }\n",
    ")\n",
    "\n",
    "padded_input = Input(\n",
    "    shape=(X_train.shape[1],),\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "emb = Embedding(\n",
    "    TOKEN_COUNT, \n",
    "    EMBED_SIZE,\n",
    "    embeddings_initializer=RandomUniform(),\n",
    "    input_length=X_train.shape[1]\n",
    ")(padded_input)\n",
    "# emb = Embedding(\n",
    "#     TOKEN_COUNT,\n",
    "#     EMBED_SIZE,\n",
    "#     weights=[embedding_matrix],\n",
    "#     input_length=MAX_LEN,\n",
    "#     trainable=True,\n",
    "# )(padded_input)\n",
    "drp = Dropout(0.1)(emb)\n",
    "\n",
    "# Create convlutions of different sizes\n",
    "convs = []\n",
    "for filter_size in FILTER_SIZE:\n",
    "    f_conv = Conv1D(\n",
    "        filters=FILTER_COUNT,\n",
    "        kernel_size=filter_size,\n",
    "        padding=CONV_PADDING,\n",
    "        activation=ACTIVATION\n",
    "    )(drp)\n",
    "    f_pool = MaxPool1D(filter_size)(f_conv)\n",
    "    convs.append(f_pool)\n",
    "\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "l_conv = Conv1D(\n",
    "    128,\n",
    "    5,\n",
    "    activation=ACTIVATION\n",
    ")(l_merge)\n",
    "l_pool = GlobalMaxPool1D()(l_conv)\n",
    "l_flat = Flatten()(l_pool)\n",
    "l_drp  = Dropout(CONV_DROPOUT_RATIO)(l_flat)\n",
    "l_dense = Dense(\n",
    "    128,\n",
    "    activation=ACTIVATION\n",
    ")(l_drp)\n",
    "out_dense = Dense(\n",
    "    y_train.shape[1],\n",
    "    activation='sigmoid'\n",
    ")(l_dense)\n",
    "\n",
    "model = Model(inputs=padded_input, outputs=out_dense)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        patience=2\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/cnn_tagger.weights.hdf5')\n",
    "metrics = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.018\n",
      "categorical_accuracy: 0.314\n",
      "precision: 0.391\n",
      "recall: 0.718\n",
      "binary_accuracy: 0.995\n",
      "hinge: 1.009\n",
      "auc: 0.984\n",
      "accuracy: 0.002\n",
      "mean_absolute_error: 0.012\n",
      "mean_absolute_percentage_error: 11001994.000\n",
      "true_positives: 313114.000\n",
      "false_positives: 488274.000\n",
      "true_negatives: 120811464.000\n",
      "false_negatives: 122729.000\n",
      "f1: 0.253\n"
     ]
    }
   ],
   "source": [
    "def fix_metric_name(name):\n",
    "    \"\"\"Remove the trailing _NN, ex. precision_86\"\"\"\n",
    "    if name[-1].isdigit():\n",
    "        repeat_name = '_'.join(name.split('_')[:-1])\n",
    "    else:\n",
    "        repeat_name = name\n",
    "    return repeat_name\n",
    "\n",
    "def fix_value(val):\n",
    "    \"\"\"Convert from numpy to float\"\"\"\n",
    "    return val.item() if isinstance(val, np.float32) else val\n",
    "\n",
    "def fix_metric(name, val):\n",
    "    repeat_name = fix_metric_name(name)\n",
    "    py_val = fix_value(val)\n",
    "    return repeat_name, py_val\n",
    "\n",
    "\n",
    "log = {}\n",
    "for name, val in zip(model.metrics_names, metrics):\n",
    "    \n",
    "    repeat_name, py_val = fix_metric(name, val)\n",
    "    log[repeat_name] = py_val\n",
    "\n",
    "# Add a name and sequence number and an F1 score\n",
    "log.update({'name': EXPERIMENT_NAME})\n",
    "log.update({'sequence': SEQUENCE})\n",
    "log.update({'f1': (log['precision'] * log['recall']) / (log['precision'] + log['recall'])})\n",
    "\n",
    "simple_log.append(log)\n",
    "\n",
    "# Overwrite the old log\n",
    "with open('data/simple_log.jsonl', 'w') as f:\n",
    "    [f.write(json.dumps(l) + '\\n') for l in simple_log]\n",
    "\n",
    "pd.DataFrame([log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "new_history = {}\n",
    "for key, metrics in history.history.items():\n",
    "    new_history[fix_metric_name(key)] = metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz_keys = ['val_categorical_accuracy', 'val_precision', 'val_recall']\n",
    "# summarize history for accuracy\n",
    "for key in viz_keys:\n",
    "    plt.plot(new_history[key])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(viz_keys, loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare this Run to the 1st and Previous Run\n",
    "\n",
    "To get an idea of performance we need to see where we started and where we just came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous</th>\n",
       "      <th>current</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>hinge</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>true_positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>-0.026696</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.106261</td>\n",
       "      <td>-0.074153</td>\n",
       "      <td>-16857.0</td>\n",
       "      <td>439410.0</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>-0.007781</td>\n",
       "      <td>-0.001203</td>\n",
       "      <td>4262551.0</td>\n",
       "      <td>-0.394335</td>\n",
       "      <td>0.157292</td>\n",
       "      <td>99674350.0</td>\n",
       "      <td>134651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best_2</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>-0.069544</td>\n",
       "      <td>-0.039768</td>\n",
       "      <td>12092.0</td>\n",
       "      <td>177525.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007302</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-2849897.0</td>\n",
       "      <td>-0.098685</td>\n",
       "      <td>-0.010741</td>\n",
       "      <td>67955032.0</td>\n",
       "      <td>15268.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              previous       current  accuracy       auc  \\\n",
       "0                      kim_cnn_complex  kim_cnn_2000 -0.026696  0.006748   \n",
       "1  kim_cnn_limit_10_epochs_save_best_2  kim_cnn_2000  0.001319 -0.000114   \n",
       "\n",
       "   binary_accuracy  categorical_accuracy        f1  false_negatives  \\\n",
       "0         0.003744             -0.106261 -0.074153         -16857.0   \n",
       "1         0.002843             -0.069544 -0.039768          12092.0   \n",
       "\n",
       "   false_positives     hinge      loss  mean_absolute_error  \\\n",
       "0         439410.0  0.010007 -0.007781            -0.001203   \n",
       "1         177525.0  0.000014 -0.007302            -0.004031   \n",
       "\n",
       "   mean_absolute_percentage_error  precision    recall  true_negatives  \\\n",
       "0                       4262551.0  -0.394335  0.157292      99674350.0   \n",
       "1                      -2849897.0  -0.098685 -0.010741      67955032.0   \n",
       "\n",
       "   true_positives  \n",
       "0        134651.0  \n",
       "1         15268.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to original\n",
    "if len(simple_log) > 1:\n",
    "    d2 = simple_log[-1]\n",
    "    d1 = simple_log[0]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "log_diff_1 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_1['current'] = d2['name']\n",
    "log_diff_1['previous'] = d1['name']\n",
    "\n",
    "# Compare to last run\n",
    "if len(simple_log) > 1:\n",
    "    d1 = simple_log[-2]\n",
    "    d2 = simple_log[-1]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "    \n",
    "log_diff_2 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_2['current'] = d2['name']\n",
    "log_diff_2['previous'] = d1['name']\n",
    "\n",
    "df = pd.DataFrame.from_dict([log_diff_1, log_diff_2])\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('previous')\n",
    "cols.remove('current')\n",
    "show_cols = ['previous', 'current'] + cols\n",
    "df[show_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Last 10 Experiments\n",
    "\n",
    "It can be helpful to see trends of performance among experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>hinge</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>basic_cnn</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.421450</td>\n",
       "      <td>0.677476</td>\n",
       "      <td>0.659971</td>\n",
       "      <td>0.334304</td>\n",
       "      <td>0.982044</td>\n",
       "      <td>209903.0</td>\n",
       "      <td>99928.0</td>\n",
       "      <td>21086010.0</td>\n",
       "      <td>108146.0</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>0.016715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>basic_cnn_5000_limit</td>\n",
       "      <td>0.016947</td>\n",
       "      <td>0.380762</td>\n",
       "      <td>0.661913</td>\n",
       "      <td>0.594659</td>\n",
       "      <td>0.313243</td>\n",
       "      <td>0.978033</td>\n",
       "      <td>242908.0</td>\n",
       "      <td>124071.0</td>\n",
       "      <td>53043088.0</td>\n",
       "      <td>165575.0</td>\n",
       "      <td>1.002064</td>\n",
       "      <td>0.009689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>basic_cnn_5000_limit_10_epochs</td>\n",
       "      <td>0.017176</td>\n",
       "      <td>0.381195</td>\n",
       "      <td>0.643383</td>\n",
       "      <td>0.611039</td>\n",
       "      <td>0.313397</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>249599.0</td>\n",
       "      <td>138349.0</td>\n",
       "      <td>53028884.0</td>\n",
       "      <td>158884.0</td>\n",
       "      <td>1.002358</td>\n",
       "      <td>0.009983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>kim_cnn_5000_limit_10_epochs</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.397833</td>\n",
       "      <td>0.786248</td>\n",
       "      <td>0.548924</td>\n",
       "      <td>0.323247</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>224226.0</td>\n",
       "      <td>60959.0</td>\n",
       "      <td>53106280.0</td>\n",
       "      <td>184257.0</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>0.006862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>kim_cnn_5000_limit_10_epochs_save_best</td>\n",
       "      <td>0.014030</td>\n",
       "      <td>0.399011</td>\n",
       "      <td>0.794837</td>\n",
       "      <td>0.522090</td>\n",
       "      <td>0.315110</td>\n",
       "      <td>0.975943</td>\n",
       "      <td>213265.0</td>\n",
       "      <td>55048.0</td>\n",
       "      <td>53112236.0</td>\n",
       "      <td>195218.0</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.006946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>simple_5000_limit_10_epochs_save_best</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>0.380396</td>\n",
       "      <td>0.490458</td>\n",
       "      <td>0.749843</td>\n",
       "      <td>0.296514</td>\n",
       "      <td>0.985702</td>\n",
       "      <td>306298.0</td>\n",
       "      <td>318216.0</td>\n",
       "      <td>52849048.0</td>\n",
       "      <td>102185.0</td>\n",
       "      <td>1.008430</td>\n",
       "      <td>0.016055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.510684</td>\n",
       "      <td>0.725247</td>\n",
       "      <td>0.299671</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>296251.0</td>\n",
       "      <td>283855.0</td>\n",
       "      <td>52883420.0</td>\n",
       "      <td>112232.0</td>\n",
       "      <td>1.007342</td>\n",
       "      <td>0.014967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best_2</td>\n",
       "      <td>0.024950</td>\n",
       "      <td>0.383484</td>\n",
       "      <td>0.489399</td>\n",
       "      <td>0.729152</td>\n",
       "      <td>0.292845</td>\n",
       "      <td>0.984181</td>\n",
       "      <td>297846.0</td>\n",
       "      <td>310749.0</td>\n",
       "      <td>52856432.0</td>\n",
       "      <td>110637.0</td>\n",
       "      <td>1.008518</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.313940</td>\n",
       "      <td>0.390715</td>\n",
       "      <td>0.718410</td>\n",
       "      <td>0.253076</td>\n",
       "      <td>0.984066</td>\n",
       "      <td>313114.0</td>\n",
       "      <td>488274.0</td>\n",
       "      <td>120811464.0</td>\n",
       "      <td>122729.0</td>\n",
       "      <td>1.008532</td>\n",
       "      <td>0.012112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence                                    name      loss  \\\n",
       "0         35                         kim_cnn_complex  0.025429   \n",
       "1         35                         kim_cnn_complex  0.025429   \n",
       "2         35                         kim_cnn_complex  0.025429   \n",
       "3         36                               basic_cnn  0.028156   \n",
       "4         37                    basic_cnn_5000_limit  0.016947   \n",
       "5         38          basic_cnn_5000_limit_10_epochs  0.017176   \n",
       "6         40            kim_cnn_5000_limit_10_epochs  0.013641   \n",
       "7         41  kim_cnn_5000_limit_10_epochs_save_best  0.014030   \n",
       "8         42   simple_5000_limit_10_epochs_save_best  0.024671   \n",
       "9         44       kim_cnn_limit_10_epochs_save_best  0.023322   \n",
       "10        45     kim_cnn_limit_10_epochs_save_best_2  0.024950   \n",
       "11        46                            kim_cnn_2000  0.017648   \n",
       "\n",
       "    categorical_accuracy  precision    recall        f1       auc  \\\n",
       "0               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "1               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "2               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "3               0.421450   0.677476  0.659971  0.334304  0.982044   \n",
       "4               0.380762   0.661913  0.594659  0.313243  0.978033   \n",
       "5               0.381195   0.643383  0.611039  0.313397  0.979239   \n",
       "6               0.397833   0.786248  0.548924  0.323247  0.977590   \n",
       "7               0.399011   0.794837  0.522090  0.315110  0.975943   \n",
       "8               0.380396   0.490458  0.749843  0.296514  0.985702   \n",
       "9               0.385321   0.510684  0.725247  0.299671  0.984288   \n",
       "10              0.383484   0.489399  0.729152  0.292845  0.984181   \n",
       "11              0.313940   0.390715  0.718410  0.253076  0.984066   \n",
       "\n",
       "    true_positives  false_positives  true_negatives  false_negatives  \\\n",
       "0         178463.0          48864.0      21137114.0         139586.0   \n",
       "1         178463.0          48864.0      21137114.0         139586.0   \n",
       "2         178463.0          48864.0      21137114.0         139586.0   \n",
       "3         209903.0          99928.0      21086010.0         108146.0   \n",
       "4         242908.0         124071.0      53043088.0         165575.0   \n",
       "5         249599.0         138349.0      53028884.0         158884.0   \n",
       "6         224226.0          60959.0      53106280.0         184257.0   \n",
       "7         213265.0          55048.0      53112236.0         195218.0   \n",
       "8         306298.0         318216.0      52849048.0         102185.0   \n",
       "9         296251.0         283855.0      52883420.0         112232.0   \n",
       "10        297846.0         310749.0      52856432.0         110637.0   \n",
       "11        313114.0         488274.0     120811464.0         122729.0   \n",
       "\n",
       "       hinge  mean_absolute_error  \n",
       "0   0.998525             0.013316  \n",
       "1   0.998525             0.013316  \n",
       "2   0.998525             0.013316  \n",
       "3   1.001925             0.016715  \n",
       "4   1.002064             0.009689  \n",
       "5   1.002358             0.009983  \n",
       "6   0.999238             0.006862  \n",
       "7   0.999323             0.006946  \n",
       "8   1.008430             0.016055  \n",
       "9   1.007342             0.014967  \n",
       "10  1.008518             0.016143  \n",
       "11  1.008532             0.012112  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = pd.DataFrame(simple_log)\n",
    "log_df['f1'] = (log_df['precision'] * log_df['recall']) / (log_df['precision'] + log_df['recall'])\n",
    "\n",
    "log_df[[\n",
    "    'sequence',\n",
    "    'name',\n",
    "    'loss',\n",
    "    'categorical_accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'f1',\n",
    "    'auc',\n",
    "    'true_positives',\n",
    "    'false_positives',\n",
    "    'true_negatives',\n",
    "    'false_negatives',\n",
    "    'hinge',\n",
    "    'mean_absolute_error',\n",
    "]][0:10 if len(log_df) > 9 else len(log_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Actual Prediction Outputs\n",
    "\n",
    "It is not enough to know theoretical performance. We need to see the actual output of the tagger at different confidence thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COUNT = 1000\n",
    "\n",
    "X_test_text = tokenizer.sequences_to_texts(X_test[:TEST_COUNT])\n",
    "\n",
    "y_test_tags = []\n",
    "for row in y_test[:TEST_COUNT].tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col == 1]\n",
    "    y_test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the threshold for classification\n",
    "\n",
    "This lets us see how well the model generalizes to labeling more classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_THRESHOLD = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > CLASSIFY_THRESHOLD) * 1\n",
    "\n",
    "y_pred_tags = []\n",
    "for row in y_pred.tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col > CLASSIFY_THRESHOLD]\n",
    "    y_pred_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See How Far off we are per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  190, -3649,   310,   129,   228,    84,   155,   127,    92,\n",
       "         233,   503,  -351,    81,  -153,    55,    99,   -24, -2473,\n",
       "         793,   827,   173,   422,   738,  1243,   261,   948,  1331,\n",
       "         629,   474,   153,   531,   585,    59,   624,  -124,   146,\n",
       "         218,    62,    36,    80,     8,    20,     2,   100,  -446,\n",
       "         444,  -251,    84,   460,   172,   567,  -286,   178, -1255,\n",
       "         115,   108,  -647,   396,   162,    88,   134,    64,   333,\n",
       "         240,  -166,   117,   162,  -398,    88,   800,  -106,   233,\n",
       "           8,    51,    23,   281,  -178,   154,   296,    46,   -94,\n",
       "         293,   101,    28,   263,   517,     0,  -320,   384,   266,\n",
       "       -6893,  -364, -2516,    78,   252,   198,  -107,    -1,   146,\n",
       "          10,   172,   149,     9,   154,   216,   321,   171,   -12,\n",
       "         198,   187,   -65,     8,    63,    48,   346,   120,   717,\n",
       "         -25,  -149,     3,   -26,   191,   219,    41,    82,   -60,\n",
       "         140,   -85,  -312,  -139,  -276,   135,    58,   317,     0,\n",
       "        -311,   170,    86,   313,   -32,     2,    85,    38,   156,\n",
       "          58,   167,   -29,    45,   584,   157,  -251,   534,   616,\n",
       "          12,    73,    53,    57,    19,  -151,     9,   232, -1078,\n",
       "          22,   268,    39,    37,    13,   119,   198,    68,   107,\n",
       "        -393,   226,   105,    63,   -63,    15,  -189,  -176,    84,\n",
       "         130,     0,    69,  -112,   216,   235,   271,    80,   160,\n",
       "         -49,    60,   -17,    42,   187,   151,   155,   -12,    77,\n",
       "         227,   128,   270,    -4,   173,  -199,   201,   167,    47,\n",
       "         278,    16,   179,     9,   178,    87,   116,    27,     7,\n",
       "           8,    64,   -74,   228,    66,    46,    -4,  -179,   176,\n",
       "         -75,  -268,    -2,   144,  -258,     8,   204,    73,    24,\n",
       "         171,   178,    61,  -591,  -198,   112,   -93,   -30,    -1,\n",
       "          80,   217,   100,    51,    97,   488,   166,   148,    58,\n",
       "          64,    40,   -53,   -27,    55,   100,   -64,    42,  -252,\n",
       "        -135,   456,    38,   156,    90,   237,   127,   109,   136,\n",
       "          61,   137,   -49,    12,   401,    25,    -4,   -12,   128,\n",
       "          33,   159,   132,    -9,    36,   106,   184,   180,    36,\n",
       "          49,   168,    22,   -73,  -205,   173,    19,    39,   183,\n",
       "          39,    13,     5,    50,    39,   175,    -4,   -35,    41,\n",
       "         237,    45,    36, -1039,   181,  -285,  -289,   101,    85,\n",
       "         -22,   253,   -82,    92,    84,  -123,   -27,  -265,   106,\n",
       "           6,    82,     1,    39,     1,    78,   -53,    65,  -110,\n",
       "         944,   177,   -11,    64,    85,    20,    82,  -183,    91,\n",
       "          93,  -665,   399,   292,   635,   195,   206,   -87,   391,\n",
       "          35,    32,   272,   263,   159, -1006,   -22,  -171,   211,\n",
       "         -12, -2257,    90,   188,    55,   -39,    28,    77,    78,\n",
       "         188,   113,  -471,   104,   -91,   378,   215,   118,   228,\n",
       "        -211,   115,   144,    19,   185,   -24,     2,    58,   214,\n",
       "          76,    19,   153,    13,  -133,  -273,   -39,    63,   109,\n",
       "          60,    83,   179,    78,   120,   -20,  -223,   184,   249,\n",
       "        -964,   237,   210,   -69,   169,    -7,   -10,    60,   -94,\n",
       "          76,    -6,    -7,    71,    38,  -248,    37,    19,     7,\n",
       "         -45,    30,    93,    47,    67,   140,    91,   146,  -408,\n",
       "        -101,  -202,    87,    51,    86,    44,  -209,   183,    12,\n",
       "         -56,    96,   142,   218,  -117,  -218,    -7,   -51,   122,\n",
       "          48,   -69,   148,   132,   115,    17,    91,  -647,    10,\n",
       "        -458,   116,    34,    -1,  -116,    48,    39,  -147,   187,\n",
       "          59,     5,     5,  -268,   -52,    26,    29,   126,    -5,\n",
       "          87,    40,    58,   -82,   -93,  -311,   531,    70,  -159,\n",
       "          83,   119,    21,    62,   119,    69,   114,  -371,   -43,\n",
       "          16,  -128,    24,   100,   -73,    90,   -30,    73,   -96,\n",
       "        -129,    75,  -265,   159,   -56,   -33,    42,   124,     3,\n",
       "       -1151,   -29,   -56,   398,    73,   212,    79,    69,   130,\n",
       "          77,   143,  -267,   345,   -80,   -55,  -103,   -27,   156,\n",
       "         114,   104,   -34,   -62,    53,   123,    32,  -183,    30,\n",
       "         -21,   192,   144,     2,   382,    53,  -480,  -674,  -746,\n",
       "         124,   -45,    43,   319,     0,     6,    92,    67,   144,\n",
       "          88,   -39,    80,    18,    13,  -113,  -372,  -137,  -193,\n",
       "          27,    82,  -144,  -141,   -10,    40,  -171,    89,   -81,\n",
       "         104,    76,    13,    88,   -16,   163,  -127,   -94,    22,\n",
       "         589,    46,    40,    20,    53,    52,   120,   -19,    65,\n",
       "         -74,    12,  -176,   149,  -158,  -307,  -520,   179,   -30,\n",
       "          28,    43,  -202,  -237,    52,   -20,  -280,    27,  -114,\n",
       "           5,   -28,     5,    16,    50,    -7,    -5,     4,   -17,\n",
       "           3,   -90,   -60,    18,   101,    23,   495,  -115,  -327,\n",
       "           1,   -31,    23,   -27,    -3,   -51,    26,     0,    29,\n",
       "         152,     3,   109,  -247,   390,   102,   190,  -184,   -50,\n",
       "         137,    57,    23,    65,    54,  -331,   191,    -4,   322,\n",
       "          62,    74,  -130,  -302,    36,   -37,   181,    17,   -90,\n",
       "        -214,    -9,    64,  -521,  -245,    68,     8,   -93,   118,\n",
       "         -67,   -95,   -26,    14,    30,    26,   163,   -39,     3,\n",
       "          78,   344,    66,    15,    26,    98,   145,  -205,  -158,\n",
       "          43,  -178,   134,   158,   383,    54,   348,   194,   176,\n",
       "         624,   425,   139,   -47,    -4,   -15,    33,   -71,  -133,\n",
       "         225,    67,  -339,    48,    26,    10,  -107,  -192,  -227,\n",
       "         121,   222,    85,   -32,    62,    42,  -226,    22,   410,\n",
       "        -424,    15,     4,   -83,   -50,    -8,    42,    85,   -12,\n",
       "          69,    21,  -385,  -367,    42,    11,  -214,   129,   -88,\n",
       "         -37,   -23,   -21,    71,    99,   169,  -405,    42,    11,\n",
       "          88,    94,   151,   -49,   135,    55,    99,    95,    48,\n",
       "          73,   264,   355,    64,   239,   121,    44,    93,  -273,\n",
       "          16,    99,  -423,   -19,   133,    81,    65,     3,    85,\n",
       "         106,    19,    -5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(y_pred, 0).sum(axis=0) - y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Prediction Results\n",
    "\n",
    "It is better to view the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tests = []\n",
    "for x, y, z in zip(X_test_text, y_pred_tags, y_test_tags):\n",
    "    prediction_tests.append({\n",
    "        'Question': x,\n",
    "        'Actual': ' '.join(sorted(z)),\n",
    "        'Predictions': ' '.join(sorted(y)),\n",
    "    })\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(prediction_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Finish\n",
    "\n",
    "That is the big finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
