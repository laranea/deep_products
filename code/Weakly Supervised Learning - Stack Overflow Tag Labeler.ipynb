{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakly Supervised Learning: Stack Overflow Tag Labeler\n",
    "\n",
    "This is the first project for the book Weakly Supervised Learning, about using natural language processing (NLP) and weakly supervised learning (WSL) to build better models with less data. Using the non-code text of Stack Overflow posts (question and answers) to tag them using a multi-class, multi-label classifier based on 1D Convlutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rjurney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always use a random seed for reproducible results\n",
    "\n",
    "Using random seeds throughout makes our work reproducible - so we get the same tokenization consistently, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPU Support for Tensorflow/Keras\n",
    "\n",
    "The first thing to do is to verify that our JupyterLab Python environment on our Data Science Engine EC2 instance is properly configured to work with its onboard GPU. We use `tensorflow.test.is_gpu_available` and `tensorflow.compat.v2.config.experimental.list_physical_devices` to verify the GPUs are working with Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 or more GPUs is available: True\n",
      "GPUs on tap: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "print(f'1 or more GPUs is available: {gpu_avail}')\n",
    "\n",
    "avail_gpus = tf.compat.v2.config.experimental.list_physical_devices('GPU')\n",
    "print(f'GPUs on tap: {avail_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widen the Max Width of Pandas Columns\n",
    "\n",
    "We want to be able to see the text of the questions in our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Application\n",
    "\n",
    "Setup the batch size, document size, token count for the tokenizer/embedding, the embedding dimensions and the test/train split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 64\n",
    "MAX_LEN     = 200\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE  = 50\n",
    "TEST_SPLIT  = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Stratified Sample of Answered Stack Overflow Questions with Tags\n",
    "\n",
    "We load a sample pulled from all answered questions from Stack Overflow. This data was converted from XML to parquet format via [code/stackoverflow/xml_to_parquet.py](stackoverflow/sample_json.spark.py) and then a more balanced stratified sample was computed for tags with over 50,000, 20,000, 10,000, 5,000 and 2,000 instances that reduced the maximum imbalance via oversampling using [code/stackoverflow/get_questions.spark.py](stackoverflow/get_questions.spark.py).\n",
    "\n",
    "These scripts were run using a Spark cluster via Amazon Elastic MapReduce using 13 r5.12xlarge machines for about 24 hours at a cost of about \\\\$300 per full run, and about \\\\$1,500 overall to create and debug. Big data is expensive. The latter parts of the run could be computed on my own 6 core/64GB machine with NVM drives for data/Spark caching.\n",
    "\n",
    "With this dataset the challenge isn't the number of records per say but rather the imbalance of the dataset if we wish to expand the number of tags the model can predict beyond low 3 digits. This leads us to some of the other techniques we'll cover involving weakly supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_776</th>\n",
       "      <th>label_777</th>\n",
       "      <th>label_778</th>\n",
       "      <th>label_779</th>\n",
       "      <th>label_780</th>\n",
       "      <th>label_781</th>\n",
       "      <th>label_782</th>\n",
       "      <th>label_783</th>\n",
       "      <th>label_784</th>\n",
       "      <th>label_785</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                             _Body  \\\n",
       "0  [How, animate, Flutter, layout, keyboard, appearance, I, building, Flutter, app, mainly, iOS, One, views, text, field, iOS, keyboard, appears, tap, The, problem, layout, change, smoothly, like, native, iOS, apps, Instead, instantly, jumps, final, available, screen, height, even, keyboard, opening, animation, finishes, I, tried, wrapping, SafeArea, element, help, My, layout, code, How, I, make,...   \n",
       "1  [Creating, Carousel, using, FutureBuilder, I, storing, images, Firestore, Document, would, like, load, using, FutureBuilder, Here, I, done, far, Using, code, I, display, image, without, errors, However, I, know, loop, snapshot, data, display, list, images, Below, firestore, structure, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __PAD__, __...   \n",
       "\n",
       "   label_0  label_1  label_2  label_3  label_4  label_5  label_6  label_7  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   label_8  ...  label_776  label_777  label_778  label_779  label_780  \\\n",
       "0        0  ...          0          0          0          0          0   \n",
       "1        0  ...          0          0          0          0          0   \n",
       "\n",
       "   label_781  label_782  label_783  label_784  label_785  \n",
       "0          0          0          0          0          0  \n",
       "1          0          0          0          0          0  \n",
       "\n",
       "[2 rows x 787 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag limit defines which dataset to load - those with tags having at least 50K, 20K, 10K, 5K or 2K instances\n",
    "TAG_LIMIT = 2000\n",
    "\n",
    "# Pre-computed sorted list of tag/index pairs\n",
    "sorted_all_tags = json.load(open(f'data/stackoverflow/08-05-2019/sorted_all_tags.{TAG_LIMIT}.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1\n",
    "\n",
    "# Load the parquet file using pyarrow for this tag limit, using the sorted tag index to specify the columns\n",
    "posts_df = pd.read_parquet(\n",
    "    f'data/stackoverflow/08-05-2019/Questions.Stratified.Final.{TAG_LIMIT}.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,554,788 Stackoverflow questions with a tag having at least 2,000 occurrences\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,} Stackoverflow questions with a tag having at least 2,000 occurrences'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check the Labels\n",
    "\n",
    "There shouldn't be any rows that are all zeros - it throws off the metrics computed during training. There shouldn't be more than six tags for one question, there is a hard limit in the Stack Overflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero rows: 1,554,788, Total rows: 1,554,788, Non-zero ratio: 1.0, Least tags: 1, Most tags: 6\n"
     ]
    }
   ],
   "source": [
    "test_matrix = posts_df[[f'label_{i}' for i in range(0, max_index)]].as_matrix()\n",
    "\n",
    "tests = np.count_nonzero(test_matrix.sum(axis=1)), \\\n",
    "        test_matrix.sum(axis=1).shape[0], \\\n",
    "        test_matrix.sum(axis=1).min(), \\\n",
    "        test_matrix.sum(axis=1).max()\n",
    "\n",
    "print(f'Non-zero rows: {tests[0]:,}, Total rows: {tests[1]:,}, Non-zero ratio: {tests[0]/tests[1]:,}, Least tags: {tests[2]:,}, Most tags: {tests[3]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Precomputed Indexes for Converting Between Tag Indexes and Tags\n",
    "\n",
    "At the end, when we visualize the predictions of our model versus the test data, we will use these indexes to convert from numeric predictions corresponding to tag indexes in a matrix to actual text tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open(f'data/stackoverflow/08-05-2019/tag_index.{TAG_LIMIT}.json'))\n",
    "index_tag = json.load(open(f'data/stackoverflow/08-05-2019/index_tag.{TAG_LIMIT}.json'))\n",
    "\n",
    "# Sanity check the different files\n",
    "assert( len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "GloVe embeddings and multi-GPU support require that the number of records be a multiple of the batch size times the number of tokens in the padded posts.\n",
    "\n",
    "We also join the previously tokenized text into a single string to use `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 121 Training Count: 1,548,800\n"
     ]
    }
   ],
   "source": [
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# Training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Conserve RAM\n",
    "del posts_df\n",
    "gc.collect()\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim/Pad the Questions to 200 Words\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a numpy matrix below.\n",
    "\n",
    "Note that the string `__PAD__` has been used previously to pad the documents, so we use it here for words missing in the top `TOKEN_COUNT` words, in our `Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1548800, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=TOKEN_COUNT,\n",
    "    oov_token='__PAD__'\n",
    ")\n",
    "tokenizer.fit_on_texts(documents)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del documents\n",
    "del sequences\n",
    "gc.collect()\n",
    "\n",
    "# Verify that all padded documents are now the same length\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]) )\n",
    "\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embeddings\n",
    "\n",
    "Stanford defines [GloVe Embeddings](https://nlp.stanford.edu/projects/glove/) as:\n",
    "\n",
    "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "We'll try them out to see if they can beat our own embedding, specific to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('data/GloVe/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index', word_index)\n",
    "nb_words = min(TOKEN_COUNT, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 50))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "missing_count = 0\n",
    "too_short_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOKEN_COUNT: \n",
    "        too_short_count += 1\n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(missing_count, too_short_count, embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Train Datasets\n",
    "\n",
    "We must split our data so that we test on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del padded_sequences\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Class Weights to Balance Uneven Label Counts\n",
    "\n",
    "Although there has already been filtering and up-sampling of the data to restrict it to a sample of questions with at least one tag that occurs more than 2,000 times, there are still as much as a 8000:1 ratio between common and uncommon labels. Without class weights, the most common label will be much more likely to be predicted than the least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(356, 1.0),\n",
       "  (90, 1.0873881204231082),\n",
       "  (361, 1.3316481188181122),\n",
       "  (546, 1.3770594934649385),\n",
       "  (17, 1.6345822067722415),\n",
       "  (516, 1.7743211325870334),\n",
       "  (92, 1.811382670604517),\n",
       "  (343, 2.4092987949520825),\n",
       "  (371, 3.157138238877975),\n",
       "  (1, 3.686995411511878),\n",
       "  (309, 3.9223306969846763),\n",
       "  (89, 4.019693515704154),\n",
       "  (150, 4.610451392671678),\n",
       "  (628, 4.77231891140097),\n",
       "  (483, 5.107946087306377),\n",
       "  (350, 5.164673338214954),\n",
       "  (53, 5.270591165725673),\n",
       "  (581, 5.448139724498992),\n",
       "  (459, 5.677906976744186),\n",
       "  (471, 6.0246761258482415),\n",
       "  (552, 6.332069825436409),\n",
       "  (405, 6.601050278167732),\n",
       "  (629, 7.383425414364641),\n",
       "  (580, 7.415771028037383),\n",
       "  (183, 7.422708138447147),\n",
       "  (653, 7.643467790487658),\n",
       "  (51, 7.91558077186857),\n",
       "  (34, 8.287075718015666),\n",
       "  (753, 8.489334670678703),\n",
       "  (56, 8.502980376398098),\n",
       "  (766, 8.639537257570602),\n",
       "  (378, 8.76781767955801),\n",
       "  (622, 9.085301273794189),\n",
       "  (457, 9.644333029474323),\n",
       "  (590, 9.853162592161429),\n",
       "  (647, 10.145277289435832),\n",
       "  (773, 10.342810590631364),\n",
       "  (776, 10.435475916488574),\n",
       "  (513, 11.123981424691141),\n",
       "  (76, 11.387389003498072),\n",
       "  (706, 11.391475998205474),\n",
       "  (161, 11.826548672566371),\n",
       "  (566, 12.015710770395609),\n",
       "  (729, 12.054500569692365),\n",
       "  (13, 12.116625310173697),\n",
       "  (204, 12.526689689195855),\n",
       "  (394, 12.566366425814115),\n",
       "  (419, 12.932464092900071),\n",
       "  (11, 13.137210264900663),\n",
       "  (272, 13.195925579461594),\n",
       "  (448, 13.823824041811847),\n",
       "  (311, 14.039367466548711),\n",
       "  (227, 14.645057100011535),\n",
       "  (408, 14.812507292031269),\n",
       "  (548, 14.850625804187624),\n",
       "  (218, 15.097871328338686),\n",
       "  (558, 15.13386577661223),\n",
       "  (32, 15.270387298532595),\n",
       "  (494, 15.362778315585674),\n",
       "  (721, 15.450651089205307),\n",
       "  (93, 15.716513988611043),\n",
       "  (658, 15.828200972447325),\n",
       "  (575, 16.67866526537047),\n",
       "  (40, 16.820084790673025),\n",
       "  (322, 17.013937282229964),\n",
       "  (16, 17.193661971830984),\n",
       "  (312, 17.75884739124353),\n",
       "  (611, 17.776253150378047),\n",
       "  (42, 17.80617110799439),\n",
       "  (502, 17.886446886446887),\n",
       "  (550, 17.904103793541108),\n",
       "  (402, 18.11873840445269),\n",
       "  (603, 18.248957884145465),\n",
       "  (528, 18.317414514500072),\n",
       "  (719, 18.4424753050552),\n",
       "  (600, 18.51779463243874),\n",
       "  (772, 18.601904761904763),\n",
       "  (306, 18.686782455107448),\n",
       "  (280, 18.900997469108233),\n",
       "  (429, 18.95744363147678),\n",
       "  (237, 19.05417979888939),\n",
       "  (547, 19.356304314682117),\n",
       "  (264, 19.36516168395363),\n",
       "  (171, 19.385860436707894),\n",
       "  (260, 19.46014714898835),\n",
       "  (514, 19.5862388151805),\n",
       "  (152, 19.76),\n",
       "  (191, 20.22590409431257),\n",
       "  (234, 20.25494575622208),\n",
       "  (731, 20.310030395136778),\n",
       "  (478, 20.47049338922928),\n",
       "  (426, 20.59669046073978),\n",
       "  (666, 20.63351210791484),\n",
       "  (110, 20.734607218683653),\n",
       "  (582, 20.915650741350905),\n",
       "  (485, 21.08236466290269),\n",
       "  (765, 21.117431803060544),\n",
       "  (44, 21.142048293089093),\n",
       "  (693, 21.330309139784948),\n",
       "  (669, 21.5438656032581),\n",
       "  (0, 21.565822999830136),\n",
       "  (256, 21.57681849082257),\n",
       "  (768, 21.646717817561807),\n",
       "  (689, 21.814089347079037),\n",
       "  (116, 21.825339522090424),\n",
       "  (639, 21.900638261169572),\n",
       "  (761, 21.911977908180877),\n",
       "  (303, 22.246013667425967),\n",
       "  (168, 22.36751233262861),\n",
       "  (744, 22.474420251371924),\n",
       "  (708, 22.58637253157801),\n",
       "  (717, 22.602456827487984),\n",
       "  (67, 22.793177737881507),\n",
       "  (174, 23.020489573889392),\n",
       "  (685, 23.08327272727273),\n",
       "  (267, 23.282229965156795),\n",
       "  (418, 23.56310319227914),\n",
       "  (170, 23.58937198067633),\n",
       "  (617, 23.58937198067633),\n",
       "  (142, 24.0678672985782),\n",
       "  (62, 24.187083253953134),\n",
       "  (739, 24.233250620347395),\n",
       "  (620, 24.261035734760174),\n",
       "  (415, 24.363461907503357),\n",
       "  (701, 24.429093707908407),\n",
       "  (242, 24.585205267234702),\n",
       "  (357, 24.585205267234702),\n",
       "  (300, 24.680793157076206),\n",
       "  (529, 24.690392843251654),\n",
       "  (376, 24.75780031201248),\n",
       "  (694, 24.830432231566594),\n",
       "  (625, 24.908377476947223),\n",
       "  (713, 24.96715830875123),\n",
       "  (30, 25.21008737092931),\n",
       "  (58, 25.22010329757648),\n",
       "  (525, 25.255221802267755),\n",
       "  (431, 25.361166600079905),\n",
       "  (631, 25.64808080808081),\n",
       "  (635, 25.809717422240293),\n",
       "  (486, 25.846498371335503),\n",
       "  (178, 25.862293746180484),\n",
       "  (229, 25.86756316218419),\n",
       "  (212, 26.085473597698787),\n",
       "  (228, 26.117671261057396),\n",
       "  (64, 26.15533580552122),\n",
       "  (425, 26.176907216494847),\n",
       "  (328, 26.32345013477089),\n",
       "  (54, 26.482686691697957),\n",
       "  (493, 26.488211975798038),\n",
       "  (349, 26.576931128323217),\n",
       "  (226, 26.582495812395308),\n",
       "  (163, 26.621513944223107),\n",
       "  (752, 26.62709731543624),\n",
       "  (289, 26.694280908326323),\n",
       "  (391, 26.841014799154333),\n",
       "  (623, 27.041107561235357),\n",
       "  (181, 27.515821413090595),\n",
       "  (284, 27.605566427484234),\n",
       "  (320, 27.61757668044377),\n",
       "  (506, 27.665722379603398),\n",
       "  (681, 27.847773634568984),\n",
       "  (115, 28.00750055151114),\n",
       "  (667, 28.144092219020173),\n",
       "  (77, 28.244271412680757),\n",
       "  (96, 28.288324420677363),\n",
       "  (338, 28.288324420677363),\n",
       "  (433, 28.536300292200494),\n",
       "  (153, 28.594144144144146),\n",
       "  (274, 28.730029418420457),\n",
       "  (370, 28.93959425575564),\n",
       "  (73, 28.96600501939311),\n",
       "  (277, 28.96600501939311),\n",
       "  (728, 28.96600501939311),\n",
       "  (692, 29.04552733928163),\n",
       "  (608, 29.1455463728191),\n",
       "  (323, 29.165632896852745),\n",
       "  (244, 29.239520958083833),\n",
       "  (710, 29.259737266651303),\n",
       "  (560, 29.26648224988474),\n",
       "  (656, 29.442949907235622),\n",
       "  (231, 29.47713025307639),\n",
       "  (482, 29.538855281526292),\n",
       "  (380, 29.711677978001404),\n",
       "  (366, 29.760431317393344),\n",
       "  (413, 29.795353203473365),\n",
       "  (159, 29.90061234102685),\n",
       "  (335, 30.01371158392435),\n",
       "  (358, 30.056344696969695),\n",
       "  (604, 30.170627376425855),\n",
       "  (559, 30.875),\n",
       "  (7, 30.920116902094495),\n",
       "  (261, 30.920116902094495),\n",
       "  (217, 31.162984781541482),\n",
       "  (409, 31.30128205128205),\n",
       "  (87, 31.339916070106145),\n",
       "  (599, 31.363142292490117),\n",
       "  (134, 31.487599206349206),\n",
       "  (613, 31.503225806451614),\n",
       "  (705, 31.5188679245283),\n",
       "  (252, 31.542360248447206),\n",
       "  (466, 31.61304780876494),\n",
       "  (407, 31.676147704590818),\n",
       "  (26, 31.787180771156734),\n",
       "  (59, 31.835005015045134),\n",
       "  (382, 31.85897114178168),\n",
       "  (121, 31.97934508816121),\n",
       "  (444, 32.04391721352852),\n",
       "  (605, 32.06819904016166),\n",
       "  (23, 32.125),\n",
       "  (125, 32.403777437468094),\n",
       "  (527, 32.586755646817245),\n",
       "  (723, 32.74645344338406),\n",
       "  (245, 32.77181208053691),\n",
       "  (117, 32.78027368964627),\n",
       "  (213, 32.78873966942149),\n",
       "  (36, 32.82264736297828),\n",
       "  (652, 32.82264736297828),\n",
       "  (468, 32.916256157635466),\n",
       "  (375, 32.94187856772185),\n",
       "  (15, 33.06197916666667),\n",
       "  (714, 33.37486855941115),\n",
       "  (458, 33.4275934702475),\n",
       "  (273, 33.445205479452056),\n",
       "  (520, 33.5246897280169),\n",
       "  (601, 33.53354463814052),\n",
       "  (85, 33.54240422721268),\n",
       "  (737, 33.55126849894292),\n",
       "  (305, 33.569011105235326),\n",
       "  (396, 33.658006362672324),\n",
       "  (511, 33.675862068965515),\n",
       "  (446, 33.78339542309739),\n",
       "  (725, 33.78339542309739),\n",
       "  (71, 33.8193926478423),\n",
       "  (427, 33.91878172588832),\n",
       "  (732, 34.10099382218641),\n",
       "  (428, 34.21126381029372),\n",
       "  (607, 34.248179120582684),\n",
       "  (61, 34.34081687855018),\n",
       "  (314, 34.36870600974553),\n",
       "  (155, 34.38732394366197),\n",
       "  (141, 34.42462039045553),\n",
       "  (216, 34.42462039045553),\n",
       "  (177, 34.43395714673176),\n",
       "  (643, 34.584037047126124),\n",
       "  (496, 34.6879781420765),\n",
       "  (86, 34.86899203515518),\n",
       "  (233, 34.92654745529573),\n",
       "  (565, 34.9457748417286),\n",
       "  (341, 35.02289655172414),\n",
       "  (290, 35.090657822001106),\n",
       "  (327, 35.13921948519236),\n",
       "  (447, 35.13921948519236),\n",
       "  (724, 35.15868180559402),\n",
       "  (377, 35.30533926585095),\n",
       "  (602, 35.5923745444351),\n",
       "  (295, 35.61234221598878),\n",
       "  (103, 35.71251758087201),\n",
       "  (651, 35.74268018018018),\n",
       "  (158, 35.8638418079096),\n",
       "  (111, 35.894260672886624),\n",
       "  (741, 35.904411764705884),\n",
       "  (503, 35.924731182795696),\n",
       "  (438, 35.96543909348442),\n",
       "  (318, 35.97563049022386),\n",
       "  (467, 36.06761363636364),\n",
       "  (189, 36.09837930054023),\n",
       "  (135, 36.180678255913364),\n",
       "  (730, 36.25299828669332),\n",
       "  (504, 36.36723002005156),\n",
       "  (220, 36.38807681284035),\n",
       "  (670, 36.440298507462686),\n",
       "  (203, 36.608419838523645),\n",
       "  (385, 36.67186597342577),\n",
       "  (740, 36.72490598785074),\n",
       "  (474, 36.735532407407405),\n",
       "  (480, 36.76744859542427),\n",
       "  (301, 36.77809965237543),\n",
       "  (644, 36.8956698634118),\n",
       "  (176, 37.07885514018692),\n",
       "  (91, 37.11137094416837),\n",
       "  (186, 37.1439438267993),\n",
       "  (420, 37.16569086651054),\n",
       "  (641, 37.18746338605741),\n",
       "  (778, 37.22017003811199),\n",
       "  (347, 37.25293427230047),\n",
       "  (555, 37.263868506017026),\n",
       "  (487, 37.28575624082232),\n",
       "  (583, 37.318636096413876),\n",
       "  (554, 37.40659988214496),\n",
       "  (172, 37.50605612998523),\n",
       "  (432, 37.51713947990544),\n",
       "  (88, 37.52822938220514),\n",
       "  (129, 37.55042886719905),\n",
       "  (195, 37.57265463154779),\n",
       "  (771, 37.59490672194255),\n",
       "  (538, 37.69536817102138),\n",
       "  (194, 37.706563706563706),\n",
       "  (46, 37.76264128494943),\n",
       "  (530, 37.77387682237429),\n",
       "  (441, 37.78511904761905),\n",
       "  (660, 37.78511904761905),\n",
       "  (743, 37.81888591003872),\n",
       "  (157, 37.84143070044709),\n",
       "  (442, 37.92054958183991),\n",
       "  (336, 37.93187929489095),\n",
       "  (648, 37.97726592880646),\n",
       "  (675, 37.97726592880646),\n",
       "  (522, 37.988629563135845),\n",
       "  (702, 38.034152186938286),\n",
       "  (369, 38.05695443645084),\n",
       "  (14, 38.068365817091454),\n",
       "  (734, 38.11407985589913),\n",
       "  (770, 38.1484375),\n",
       "  (128, 38.182857142857145),\n",
       "  (239, 38.182857142857145),\n",
       "  (767, 38.483782964534704),\n",
       "  (80, 38.53050075872534),\n",
       "  (138, 38.6242774566474),\n",
       "  (637, 38.6242774566474),\n",
       "  (247, 38.86072849709213),\n",
       "  (324, 38.98004298434142),\n",
       "  (330, 38.98004298434142),\n",
       "  (727, 38.98004298434142),\n",
       "  (606, 39.00399385560676),\n",
       "  (524, 39.19666563754245),\n",
       "  (392, 39.28155940594059),\n",
       "  (398, 39.35461872287663),\n",
       "  (406, 39.489269051321926),\n",
       "  (733, 39.58777673838478),\n",
       "  (66, 39.637215110833594),\n",
       "  (365, 39.68677711784933),\n",
       "  (223, 39.71160462934001),\n",
       "  (345, 39.71160462934001),\n",
       "  (564, 39.73646322378717),\n",
       "  (9, 39.748904195366315),\n",
       "  (292, 39.773809523809526),\n",
       "  (556, 39.786273895330616),\n",
       "  (19, 39.7987460815047),\n",
       "  (230, 39.861224489795916),\n",
       "  (703, 39.93645800566216),\n",
       "  (114, 39.9490245437382),\n",
       "  (241, 39.97418136020151),\n",
       "  (238, 40.03721223588773),\n",
       "  (508, 40.13847613025609),\n",
       "  (278, 40.25301204819277),\n",
       "  (626, 40.3425484588497),\n",
       "  (291, 40.35537190082645),\n",
       "  (464, 40.35537190082645),\n",
       "  (122, 40.4969696969697),\n",
       "  (395, 40.613563659628916),\n",
       "  (28, 40.66559897501602),\n",
       "  (359, 40.704713049054185),\n",
       "  (101, 40.730830927173564),\n",
       "  (348, 40.756982343499196),\n",
       "  (594, 40.756982343499196),\n",
       "  (401, 40.783167362672664),\n",
       "  (757, 40.83563846896108),\n",
       "  (445, 40.84877734877735),\n",
       "  (755, 40.9014175257732),\n",
       "  (246, 40.99386503067485),\n",
       "  (540, 40.99386503067485),\n",
       "  (748, 41.08673139158576),\n",
       "  (533, 41.10003237293623),\n",
       "  (126, 41.1399870382372),\n",
       "  (422, 41.19338092147956),\n",
       "  (410, 41.24691358024691),\n",
       "  (749, 41.28715447154472),\n",
       "  (726, 41.36787227109808),\n",
       "  (762, 41.51667756703728),\n",
       "  (664, 41.61193051458538),\n",
       "  (630, 41.666557269445356),\n",
       "  (25, 41.74876685300888),\n",
       "  (769, 41.7625),\n",
       "  (587, 41.83130148270181),\n",
       "  (319, 41.886506103596176),\n",
       "  (3, 41.92800528401585),\n",
       "  (754, 41.94185662371986),\n",
       "  (352, 42.05299768135144),\n",
       "  (436, 42.0948275862069),\n",
       "  (4, 42.122760451227606),\n",
       "  (596, 42.17873754152824),\n",
       "  (481, 42.347565043362245),\n",
       "  (545, 42.37583444592791),\n",
       "  (106, 42.38998330550918),\n",
       "  (383, 42.38998330550918),\n",
       "  (668, 42.40414161656646),\n",
       "  (249, 42.47507527601204),\n",
       "  (567, 42.51774949765573),\n",
       "  (130, 42.54624664879356),\n",
       "  (340, 42.57478202548625),\n",
       "  (263, 42.646288209606986),\n",
       "  (354, 42.67495798319328),\n",
       "  (476, 42.68930733019502),\n",
       "  (43, 42.87673083417764),\n",
       "  (646, 42.891216216216215),\n",
       "  (175, 42.92021636240703),\n",
       "  (143, 42.94925575101489),\n",
       "  (162, 42.97833446174678),\n",
       "  (423, 42.992888587876735),\n",
       "  (523, 42.992888587876735),\n",
       "  (74, 43.036610169491524),\n",
       "  (5, 43.05120379789759),\n",
       "  (332, 43.124320652173914),\n",
       "  (206, 43.19768628785301),\n",
       "  (615, 43.22710248552945),\n",
       "  (208, 43.27130197682345),\n",
       "  (389, 43.28605523354927),\n",
       "  (672, 43.28605523354927),\n",
       "  (140, 43.30081855388813),\n",
       "  (251, 43.30081855388813),\n",
       "  (39, 43.315591948140565),\n",
       "  (722, 43.33037542662116),\n",
       "  (682, 43.35997267759563),\n",
       "  (597, 43.404444444444444),\n",
       "  (50, 43.46388223211229),\n",
       "  (113, 43.583247511156884),\n",
       "  (97, 43.64317634926091),\n",
       "  (293, 43.71831955922865),\n",
       "  (780, 43.76352981730438),\n",
       "  (461, 43.79372197309417),\n",
       "  (299, 43.854231433506044),\n",
       "  (156, 43.869384934346925),\n",
       "  (414, 43.869384934346925),\n",
       "  (784, 43.96052631578947),\n",
       "  (321, 44.006239168110916),\n",
       "  (578, 44.021497919556175),\n",
       "  (618, 44.08263888888889),\n",
       "  (532, 44.097950677318515),\n",
       "  (337, 44.12860618700035),\n",
       "  (416, 44.17466945024356),\n",
       "  (711, 44.17466945024356),\n",
       "  (489, 44.19004524886878),\n",
       "  (166, 44.20543175487465),\n",
       "  (452, 44.20543175487465),\n",
       "  (329, 44.22082897944967),\n",
       "  (108, 44.29797627355199),\n",
       "  (421, 44.31343804537522),\n",
       "  (197, 44.359888190076866),\n",
       "  (515, 44.4219734079776),\n",
       "  (579, 44.577949438202246),\n",
       "  (593, 44.62495606326889),\n",
       "  (235, 44.65634892718959),\n",
       "  (450, 44.79816513761468),\n",
       "  (99, 44.86148409893993),\n",
       "  (317, 44.86148409893993),\n",
       "  (700, 44.86148409893993),\n",
       "  (68, 44.9727240524265),\n",
       "  (144, 44.9727240524265),\n",
       "  (271, 44.988660524450744),\n",
       "  (585, 45.05251951738822),\n",
       "  (70, 45.11656005685857),\n",
       "  (707, 45.148648648648646),\n",
       "  (49, 45.21296296296296),\n",
       "  (688, 45.229070181688634),\n",
       "  (346, 45.24518888096935),\n",
       "  (253, 45.261319073083776),\n",
       "  (691, 45.261319073083776),\n",
       "  (561, 45.277460770328105),\n",
       "  (490, 45.34214285714286),\n",
       "  (360, 45.40701001430615),\n",
       "  (521, 45.42325581395349),\n",
       "  (393, 45.43951324266285),\n",
       "  (750, 45.48835542816195),\n",
       "  (133, 45.684778697373154),\n",
       "  (437, 45.70122390208783),\n",
       "  (756, 45.70122390208783),\n",
       "  (304, 45.800144300144304),\n",
       "  (123, 45.83321299638989),\n",
       "  (718, 45.84976525821596),\n",
       "  (362, 45.86632947976879),\n",
       "  (107, 45.88290567401518),\n",
       "  (355, 45.88290567401518),\n",
       "  (296, 45.96596669080377),\n",
       "  (313, 45.96596669080377),\n",
       "  (69, 46.06603773584906),\n",
       "  (609, 46.08275862068965),\n",
       "  (451, 46.13299418604651),\n",
       "  (139, 46.149763722282806),\n",
       "  (211, 46.16654545454546),\n",
       "  (287, 46.16654545454546),\n",
       "  (262, 46.20014556040757),\n",
       "  (37, 46.21696396068438),\n",
       "  (276, 46.318132068588106),\n",
       "  (463, 46.368882395909424),\n",
       "  (588, 46.368882395909424),\n",
       "  (577, 46.3858238947753),\n",
       "  (149, 46.40277777777778),\n",
       "  (136, 46.43672275054865),\n",
       "  (683, 46.45371386754482),\n",
       "  (404, 46.48773343097766),\n",
       "  (472, 46.53885630498534),\n",
       "  (258, 46.55592225889256),\n",
       "  (285, 46.59009174311927),\n",
       "  (294, 46.59009174311927),\n",
       "  (759, 46.62431142122659),\n",
       "  (255, 46.67573529411764),\n",
       "  (539, 46.72727272727273),\n",
       "  (758, 46.778924097273396),\n",
       "  (222, 46.81342182890855),\n",
       "  (665, 46.86526393503138),\n",
       "  (79, 46.93456561922366),\n",
       "  (334, 47.0040725657164),\n",
       "  (698, 47.0040725657164),\n",
       "  (65, 47.0389032975176),\n",
       "  (243, 47.07378568780126),\n",
       "  (673, 47.09124629080119),\n",
       "  (52, 47.12620638455828),\n",
       "  (94, 47.19628252788104),\n",
       "  (219, 47.19628252788104),\n",
       "  (344, 47.23139880952381),\n",
       "  (704, 47.301788375558864),\n",
       "  (616, 47.37238805970149),\n",
       "  (214, 47.39007092198582),\n",
       "  (469, 47.42547627941726),\n",
       "  (709, 47.42547627941726),\n",
       "  (745, 47.42547627941726),\n",
       "  (659, 47.44319880418535),\n",
       "  (417, 47.49644594089038),\n",
       "  (589, 47.49644594089038),\n",
       "  (591, 47.49644594089038),\n",
       "  (118, 47.532010482965184),\n",
       "  (6, 47.549812734082394),\n",
       "  (570, 47.549812734082394),\n",
       "  (363, 47.58545727136432),\n",
       "  (526, 47.62115528882221),\n",
       "  (55, 47.67480285392415),\n",
       "  (75, 47.692712246431256),\n",
       "  (270, 47.782461422657136),\n",
       "  (315, 47.818455743879476),\n",
       "  (124, 47.854504334715415),\n",
       "  (302, 47.89060731799321),\n",
       "  (576, 47.962976955043445),\n",
       "  (342, 48.07194244604317),\n",
       "  (411, 48.07194244604317),\n",
       "  (24, 48.09015151515152),\n",
       "  (372, 48.12661106899166),\n",
       "  (424, 48.12661106899166),\n",
       "  (475, 48.12661106899166),\n",
       "  (655, 48.16312594840668),\n",
       "  (573, 48.18140417457305),\n",
       "  (145, 48.23632218844985),\n",
       "  (308, 48.23632218844985),\n",
       "  (621, 48.29136553822746),\n",
       "  (281, 48.30974124809741),\n",
       "  (225, 48.36495238095238),\n",
       "  (236, 48.401829965688144),\n",
       "  (507, 48.42028985507246),\n",
       "  (200, 48.45725190839695),\n",
       "  (781, 48.45725190839695),\n",
       "  (109, 48.549904397705546),\n",
       "  (679, 48.549904397705546),\n",
       "  (173, 48.56847742922724),\n",
       "  (179, 48.56847742922724),\n",
       "  (193, 48.58706467661692),\n",
       "  (509, 48.58706467661692),\n",
       "  (368, 48.64291187739464),\n",
       "  (386, 48.69888761028002),\n",
       "  (164, 48.754992319508446),\n",
       "  (102, 48.77372262773723),\n",
       "  (495, 48.792467332820905),\n",
       "  (500, 48.83),\n",
       "  (82, 48.84878799538284),\n",
       "  (592, 48.84878799538284),\n",
       "  (517, 48.92408477842004),\n",
       "  (387, 48.94294525828836),\n",
       "  (645, 48.94294525828836),\n",
       "  (569, 48.98070987654321),\n",
       "  (738, 48.99961404862987),\n",
       "  (120, 49.01853281853282),\n",
       "  (250, 49.03746620316725),\n",
       "  (537, 49.03746620316725),\n",
       "  (699, 49.189461449050754),\n",
       "  (477, 49.26581296080714),\n",
       "  (333, 49.284937888198755),\n",
       "  (112, 49.323232323232325),\n",
       "  (783, 49.38078568650331),\n",
       "  (684, 49.4),\n",
       "  (185, 49.419229272090305),\n",
       "  (182, 49.43847352024922),\n",
       "  (2, 49.47700701480904),\n",
       "  (326, 49.53492001560671),\n",
       "  (48, 49.5736040609137),\n",
       "  (747, 49.59296875),\n",
       "  (190, 49.65115369573719),\n",
       "  (488, 49.67057902973396),\n",
       "  (562, 49.67057902973396),\n",
       "  (690, 49.69001956947162),\n",
       "  (720, 49.69001956947162),\n",
       "  (779, 49.69001956947162),\n",
       "  (199, 49.74843260188088),\n",
       "  (209, 49.74843260188088),\n",
       "  (254, 49.74843260188088),\n",
       "  (492, 49.76793414347315),\n",
       "  (353, 49.78745098039216),\n",
       "  (735, 49.78745098039216),\n",
       "  (501, 49.826530612244895),\n",
       "  (661, 49.90487421383648),\n",
       "  (443, 49.9637937819756),\n",
       "  (364, 50.00315084679008),\n",
       "  (775, 50.00315084679008),\n",
       "  (269, 50.02285263987392),\n",
       "  (367, 50.04256996452503),\n",
       "  (146, 50.12159494670351),\n",
       "  (78, 50.141390205371245),\n",
       "  (782, 50.141390205371245),\n",
       "  (95, 50.200869909054965),\n",
       "  (435, 50.22072784810127),\n",
       "  (221, 50.28039603960396),\n",
       "  (127, 50.34020618556701),\n",
       "  (551, 50.34020618556701),\n",
       "  (636, 50.34020618556701),\n",
       "  (137, 50.42017474185862),\n",
       "  (232, 50.42017474185862),\n",
       "  (265, 50.46025437201908),\n",
       "  (777, 50.46025437201908),\n",
       "  (430, 50.56073277578654),\n",
       "  (676, 50.580876494023904),\n",
       "  (192, 50.601036269430054),\n",
       "  (148, 50.621212121212125),\n",
       "  (240, 50.621212121212125),\n",
       "  (595, 50.621212121212125),\n",
       "  (633, 50.621212121212125),\n",
       "  (598, 50.6414040686079),\n",
       "  (677, 50.70207667731629),\n",
       "  (640, 50.72233320015981),\n",
       "  (456, 50.82385908726982),\n",
       "  (491, 50.905372894947874),\n",
       "  (100, 50.98714859437751),\n",
       "  (439, 50.98714859437751),\n",
       "  (696, 50.98714859437751),\n",
       "  (268, 51.00763358778626),\n",
       "  (84, 51.06918744971842),\n",
       "  (104, 51.0897384305835),\n",
       "  (465, 51.13089005235602),\n",
       "  (563, 51.25474364150182),\n",
       "  (634, 51.25474364150182),\n",
       "  (105, 51.27544426494346),\n",
       "  (642, 51.27544426494346),\n",
       "  (610, 51.42081814499797),\n",
       "  (298, 51.50425963488844),\n",
       "  (715, 51.56701868399675),\n",
       "  (198, 51.65093572009764),\n",
       "  (584, 51.65093572009764),\n",
       "  (695, 51.65093572009764),\n",
       "  (498, 51.67195767195767),\n",
       "  (751, 51.67195767195767),\n",
       "  (484, 51.692996742671006),\n",
       "  (512, 51.692996742671006),\n",
       "  (649, 51.71405295315682),\n",
       "  (310, 51.75621687729311),\n",
       "  (473, 51.798449612403104),\n",
       "  (331, 51.88312219043727),\n",
       "  (316, 51.90433360588716),\n",
       "  (390, 51.9468085106383),\n",
       "  (680, 51.9468085106383),\n",
       "  (572, 51.96807204257061),\n",
       "  (98, 52.01065137238837),\n",
       "  (132, 52.01065137238837),\n",
       "  (180, 52.1816687217427),\n",
       "  (60, 52.224598930481285),\n",
       "  (671, 52.28912685337726),\n",
       "  (518, 52.332234130255564),\n",
       "  (440, 52.39702847709451),\n",
       "  (479, 52.39702847709451),\n",
       "  (519, 52.39702847709451),\n",
       "  (259, 52.418662262592896),\n",
       "  (638, 52.418662262592896),\n",
       "  (282, 52.440313919867826),\n",
       "  (81, 52.46198347107438),\n",
       "  (614, 52.46198347107438),\n",
       "  (151, 52.505376344086024),\n",
       "  (686, 52.570600414078676),\n",
       "  (434, 52.61417322834646),\n",
       "  (568, 52.7234219269103),\n",
       "  (379, 52.74532613211466),\n",
       "  (388, 52.76724854530341),\n",
       "  (742, 52.76724854530341),\n",
       "  (188, 52.85512073272273),\n",
       "  (785, 52.8771345272803),\n",
       "  (10, 52.899166666666666),\n",
       "  (400, 52.92121717382243),\n",
       "  (470, 52.965373383395914),\n",
       "  (184, 52.98747913188648),\n",
       "  (35, 53.03174603174603),\n",
       "  (297, 53.03174603174603),\n",
       "  (154, 53.05390722941914),\n",
       "  (224, 53.14273754709083),\n",
       "  (215, 53.164991624790616),\n",
       "  (266, 53.164991624790616),\n",
       "  (202, 53.18726434855467),\n",
       "  (248, 53.20955574182732),\n",
       "  (165, 53.29890848026868),\n",
       "  (574, 53.29890848026868),\n",
       "  (678, 53.32129357412852),\n",
       "  (657, 53.43350168350168),\n",
       "  (31, 53.456),\n",
       "  (119, 53.456),\n",
       "  (147, 53.5913887716336),\n",
       "  (663, 53.5913887716336),\n",
       "  (632, 53.65934065934066),\n",
       "  (131, 53.72746508675412),\n",
       "  (746, 53.7502116850127),\n",
       "  (169, 53.77297755188479),\n",
       "  (403, 53.77297755188479),\n",
       "  (27, 53.795762711864406),\n",
       "  (760, 53.795762711864406),\n",
       "  (712, 53.81856718948707),\n",
       "  (29, 53.88709677419355),\n",
       "  (207, 53.88709677419355),\n",
       "  (57, 53.93288020390824),\n",
       "  (257, 54.070698466780236),\n",
       "  (462, 54.11679454390452),\n",
       "  (662, 54.11679454390452),\n",
       "  (627, 54.186086214255226),\n",
       "  (196, 54.20922288642186),\n",
       "  (534, 54.23237932507475),\n",
       "  (697, 54.30196749358426),\n",
       "  (275, 54.34845890410959),\n",
       "  (571, 54.39502999143102),\n",
       "  (531, 54.511807642765135),\n",
       "  (736, 54.5352233676976),\n",
       "  (83, 54.55865921787709),\n",
       "  (774, 54.55865921787709),\n",
       "  (187, 54.65260439087387),\n",
       "  (72, 54.77049180327869),\n",
       "  (339, 54.794130340958134),\n",
       "  (381, 54.794130340958134),\n",
       "  (497, 54.794130340958134),\n",
       "  (624, 54.794130340958134),\n",
       "  (160, 54.888888888888886),\n",
       "  (716, 54.936391172652534),\n",
       "  (325, 54.983975747076656),\n",
       "  (373, 55.00779896013865),\n",
       "  (535, 55.03164282618119),\n",
       "  (505, 55.103298611111114),\n",
       "  (283, 55.1272253582284),\n",
       "  (612, 55.2712233347845),\n",
       "  (412, 55.295296167247386),\n",
       "  (544, 55.39179755671902),\n",
       "  (8, 55.440174672489086),\n",
       "  (654, 55.440174672489086),\n",
       "  (12, 55.65892152564665),\n",
       "  (22, 55.73222124670764),\n",
       "  (201, 55.78119507908612),\n",
       "  (167, 55.83025505716799),\n",
       "  (543, 55.83025505716799),\n",
       "  (205, 55.879401408450704),\n",
       "  (288, 55.95328338475099),\n",
       "  (453, 55.95328338475099),\n",
       "  (764, 56.00264666960741),\n",
       "  (541, 56.027360988526034),\n",
       "  (45, 56.05209713024283),\n",
       "  (286, 56.32564330079858),\n",
       "  (21, 56.501112594570536),\n",
       "  (397, 56.576648841354725),\n",
       "  (557, 56.576648841354725),\n",
       "  (460, 56.67767857142857),\n",
       "  (18, 56.82990152193375),\n",
       "  (351, 57.111111111111114),\n",
       "  (399, 57.18828828828829),\n",
       "  (536, 57.18828828828829),\n",
       "  (510, 57.291516245487365),\n",
       "  (687, 57.317381489841985),\n",
       "  (210, 57.62959600544712),\n",
       "  (47, 57.681962744207176),\n",
       "  (553, 57.998172681589764),\n",
       "  (650, 58.077767612076855),\n",
       "  (549, 58.15758131012368),\n",
       "  (307, 58.2910927456382),\n",
       "  (38, 58.39834406623735),\n",
       "  (20, 58.50599078341014),\n",
       "  (384, 58.58698661744347),\n",
       "  (449, 58.58698661744347),\n",
       "  (279, 59.46510538641686),\n",
       "  (619, 59.57672454246833),\n",
       "  (41, 59.9423984891407),\n",
       "  (33, 60.513822688274544),\n",
       "  (374, 61.332367149758454),\n",
       "  (763, 61.96095656417765),\n",
       "  (63, 82.70879478827362),\n",
       "  (454, 83.30577427821522),\n",
       "  (455, 100.20363062352013),\n",
       "  (542, 157.12623762376236),\n",
       "  (499, 179.8271954674221),\n",
       "  (586, 288.5409090909091),\n",
       "  (674, 313.4765432098765)],\n",
       " [(356, 1.0),\n",
       "  (90, 1.097090399875525),\n",
       "  (361, 1.340366885277065),\n",
       "  (546, 1.36488579171506),\n",
       "  (17, 1.5974173085636612),\n",
       "  (516, 1.7724987430869783),\n",
       "  (92, 1.8276308968377397),\n",
       "  (343, 2.4355785837651123),\n",
       "  (371, 3.1130242825607066),\n",
       "  (1, 3.739591620259878),\n",
       "  (309, 3.9380061435353255),\n",
       "  (89, 3.9690402476780187),\n",
       "  (150, 4.610003269042171),\n",
       "  (628, 4.814612495732332),\n",
       "  (350, 5.213308687615527),\n",
       "  (483, 5.219096965210955),\n",
       "  (53, 5.263904441955954),\n",
       "  (581, 5.523697610654132),\n",
       "  (459, 5.629540918163673),\n",
       "  (471, 5.856312292358804),\n",
       "  (552, 6.357980162308386),\n",
       "  (405, 6.456959706959707),\n",
       "  (183, 7.125821121778676),\n",
       "  (580, 7.541176470588235),\n",
       "  (629, 7.602156334231806),\n",
       "  (653, 7.701802293828509),\n",
       "  (51, 7.926925238898257),\n",
       "  (56, 8.058285714285715),\n",
       "  (34, 8.500301386377336),\n",
       "  (766, 8.557038834951456),\n",
       "  (378, 8.89155107187894),\n",
       "  (622, 9.186970684039087),\n",
       "  (753, 9.216993464052287),\n",
       "  (457, 9.567164179104477),\n",
       "  (590, 9.937984496124031),\n",
       "  (647, 10.001418439716312),\n",
       "  (773, 10.399705014749262),\n",
       "  (776, 10.659108087679517),\n",
       "  (706, 10.8727833461835),\n",
       "  (513, 11.051724137931034),\n",
       "  (76, 11.437145174371452),\n",
       "  (729, 11.559016393442622),\n",
       "  (566, 11.72236076475478),\n",
       "  (161, 12.337707786526684),\n",
       "  (13, 12.54626334519573),\n",
       "  (394, 12.704504504504504),\n",
       "  (204, 12.973321067157313),\n",
       "  (419, 13.167133520074696),\n",
       "  (11, 13.291234684260132),\n",
       "  (311, 13.366824644549762),\n",
       "  (272, 13.59884281581485),\n",
       "  (448, 13.638297872340425),\n",
       "  (227, 14.404494382022472),\n",
       "  (32, 14.751046025104603),\n",
       "  (558, 14.986184909670563),\n",
       "  (218, 15.034115138592751),\n",
       "  (408, 15.114683815648446),\n",
       "  (721, 15.179763186221743),\n",
       "  (548, 15.278439869989166),\n",
       "  (658, 15.378407851690294),\n",
       "  (494, 15.547960308710033),\n",
       "  (93, 16.025),\n",
       "  (402, 16.435897435897434),\n",
       "  (550, 16.888622754491017),\n",
       "  (16, 16.908872901678656),\n",
       "  (611, 17.05199516324063),\n",
       "  (312, 17.176613885505482),\n",
       "  (322, 17.239608801955992),\n",
       "  (40, 17.366995073891626),\n",
       "  (575, 17.518012422360247),\n",
       "  (502, 17.58354114713217),\n",
       "  (528, 17.94147582697201),\n",
       "  (600, 18.033248081841432),\n",
       "  (603, 18.172680412371133),\n",
       "  (264, 18.2905317769131),\n",
       "  (280, 18.2905317769131),\n",
       "  (719, 18.361979166666668),\n",
       "  (42, 18.67814569536424),\n",
       "  (191, 18.727755644090305),\n",
       "  (772, 18.802666666666667),\n",
       "  (260, 18.979811574697173),\n",
       "  (237, 19.29138166894665),\n",
       "  (152, 19.317808219178083),\n",
       "  (514, 19.451034482758622),\n",
       "  (765, 19.668061366806135),\n",
       "  (306, 19.695530726256983),\n",
       "  (547, 19.750700280112046),\n",
       "  (582, 19.834036568213783),\n",
       "  (429, 19.889985895627646),\n",
       "  (731, 20.03125),\n",
       "  (171, 20.29064748201439),\n",
       "  (234, 20.29064748201439),\n",
       "  (110, 20.43768115942029),\n",
       "  (426, 20.64714494875549),\n",
       "  (478, 20.799410029498524),\n",
       "  (485, 21.206015037593986),\n",
       "  (256, 21.399089529590288),\n",
       "  (693, 21.399089529590288),\n",
       "  (0, 21.496951219512194),\n",
       "  (669, 21.529770992366412),\n",
       "  (303, 21.56269113149847),\n",
       "  (639, 21.56269113149847),\n",
       "  (761, 21.695384615384615),\n",
       "  (666, 21.93157076205288),\n",
       "  (44, 22.103448275862068),\n",
       "  (768, 22.384126984126983),\n",
       "  (242, 22.455414012738853),\n",
       "  (717, 22.527156549520768),\n",
       "  (689, 22.672025723472668),\n",
       "  (708, 22.781906300484653),\n",
       "  (67, 22.818770226537218),\n",
       "  (174, 22.818770226537218),\n",
       "  (418, 22.818770226537218),\n",
       "  (267, 23.27062706270627),\n",
       "  (116, 23.425249169435215),\n",
       "  (168, 23.46422628951747),\n",
       "  (685, 23.503333333333334),\n",
       "  (739, 23.6214405360134),\n",
       "  (357, 23.820945945945947),\n",
       "  (620, 23.820945945945947),\n",
       "  (617, 24.18867924528302),\n",
       "  (744, 24.18867924528302),\n",
       "  (142, 24.230240549828178),\n",
       "  (635, 24.230240549828178),\n",
       "  (431, 24.313793103448276),\n",
       "  (62, 24.6108202443281),\n",
       "  (30, 24.653846153846153),\n",
       "  (376, 24.697022767075307),\n",
       "  (486, 24.697022767075307),\n",
       "  (170, 24.74035087719298),\n",
       "  (300, 24.95929203539823),\n",
       "  (212, 25.04795737122558),\n",
       "  (64, 25.137254901960784),\n",
       "  (228, 25.137254901960784),\n",
       "  (701, 25.182142857142857),\n",
       "  (425, 25.31777378815081),\n",
       "  (54, 25.409009009009008),\n",
       "  (415, 25.409009009009008),\n",
       "  (115, 25.454873646209386),\n",
       "  (525, 25.454873646209386),\n",
       "  (625, 25.780621572212066),\n",
       "  (163, 25.82783882783883),\n",
       "  (328, 25.82783882783883),\n",
       "  (694, 25.82783882783883),\n",
       "  (77, 25.875229357798165),\n",
       "  (529, 25.875229357798165),\n",
       "  (349, 26.163265306122447),\n",
       "  (631, 26.507518796992482),\n",
       "  (178, 26.60754716981132),\n",
       "  (229, 26.759013282732447),\n",
       "  (284, 26.86095238095238),\n",
       "  (391, 26.912213740458014),\n",
       "  (226, 27.27659574468085),\n",
       "  (181, 27.38252427184466),\n",
       "  (713, 27.54296875),\n",
       "  (560, 27.705304518664047),\n",
       "  (710, 27.705304518664047),\n",
       "  (58, 27.869565217391305),\n",
       "  (338, 27.869565217391305),\n",
       "  (493, 27.98015873015873),\n",
       "  (623, 28.03578528827038),\n",
       "  (274, 28.09163346613546),\n",
       "  (320, 28.09163346613546),\n",
       "  (73, 28.37424547283702),\n",
       "  (244, 28.37424547283702),\n",
       "  (289, 28.431451612903224),\n",
       "  (482, 28.431451612903224),\n",
       "  (323, 28.60446247464503),\n",
       "  (667, 28.60446247464503),\n",
       "  (433, 28.779591836734692),\n",
       "  (153, 28.897540983606557),\n",
       "  (413, 29.016460905349795),\n",
       "  (358, 29.136363636363637),\n",
       "  (380, 29.136363636363637),\n",
       "  (527, 29.136363636363637),\n",
       "  (604, 29.196687370600415),\n",
       "  (692, 29.196687370600415),\n",
       "  (231, 29.25726141078838),\n",
       "  (96, 29.31808731808732),\n",
       "  (506, 29.31808731808732),\n",
       "  (277, 29.379166666666666),\n",
       "  (366, 29.502092050209207),\n",
       "  (656, 29.563941299790358),\n",
       "  (728, 29.813953488372093),\n",
       "  (335, 29.877118644067796),\n",
       "  (159, 29.940552016985137),\n",
       "  (261, 30.068230277185503),\n",
       "  (370, 30.197002141327623),\n",
       "  (26, 30.261802575107296),\n",
       "  (608, 30.32688172043011),\n",
       "  (59, 30.457883369330453),\n",
       "  (407, 30.457883369330453),\n",
       "  (121, 30.523809523809526),\n",
       "  (714, 30.59002169197397),\n",
       "  (252, 30.656521739130437),\n",
       "  (599, 30.656521739130437),\n",
       "  (125, 30.92543859649123),\n",
       "  (752, 30.92543859649123),\n",
       "  (382, 30.993406593406593),\n",
       "  (605, 30.993406593406593),\n",
       "  (61, 31.061674008810574),\n",
       "  (36, 31.130242825607063),\n",
       "  (213, 31.199115044247787),\n",
       "  (444, 31.407572383073497),\n",
       "  (375, 31.689887640449438),\n",
       "  (613, 31.689887640449438),\n",
       "  (7, 31.76126126126126),\n",
       "  (134, 31.83295711060948),\n",
       "  (409, 31.83295711060948),\n",
       "  (607, 31.83295711060948),\n",
       "  (446, 31.90497737556561),\n",
       "  (216, 31.97732426303855),\n",
       "  (245, 32.05),\n",
       "  (652, 32.123006833712985),\n",
       "  (314, 32.19634703196347),\n",
       "  (466, 32.2700228832952),\n",
       "  (117, 32.34403669724771),\n",
       "  (681, 32.34403669724771),\n",
       "  (458, 32.56812933025404),\n",
       "  (723, 32.56812933025404),\n",
       "  (15, 32.64351851851852),\n",
       "  (23, 32.64351851851852),\n",
       "  (85, 32.71925754060325),\n",
       "  (177, 32.7953488372093),\n",
       "  (732, 32.87179487179487),\n",
       "  (87, 33.02576112412178),\n",
       "  (305, 33.10328638497653),\n",
       "  (565, 33.33806146572104),\n",
       "  (217, 33.41706161137441),\n",
       "  (447, 33.41706161137441),\n",
       "  (86, 33.496437054631826),\n",
       "  (233, 33.576190476190476),\n",
       "  (290, 33.656324582338904),\n",
       "  (468, 33.81774580335731),\n",
       "  (559, 33.89903846153846),\n",
       "  (737, 33.89903846153846),\n",
       "  (520, 33.980722891566266),\n",
       "  (705, 33.980722891566266),\n",
       "  (189, 34.14527845036319),\n",
       "  (427, 34.311435523114355),\n",
       "  (725, 34.311435523114355),\n",
       "  (511, 34.479217603911984),\n",
       "  (19, 34.56372549019608),\n",
       "  (602, 34.648648648648646),\n",
       "  (155, 34.73399014778325),\n",
       "  (295, 34.73399014778325),\n",
       "  (637, 34.73399014778325),\n",
       "  (643, 34.73399014778325),\n",
       "  (71, 34.81975308641975),\n",
       "  (396, 34.90594059405941),\n",
       "  (442, 35.07960199004975),\n",
       "  (341, 35.255),\n",
       "  (141, 35.4321608040201),\n",
       "  (220, 35.4321608040201),\n",
       "  (601, 35.4321608040201),\n",
       "  (554, 35.611111111111114),\n",
       "  (420, 35.70126582278481),\n",
       "  (428, 35.70126582278481),\n",
       "  (273, 35.88295165394402),\n",
       "  (730, 35.88295165394402),\n",
       "  (524, 35.974489795918366),\n",
       "  (111, 36.066496163682864),\n",
       "  (330, 36.066496163682864),\n",
       "  (14, 36.15897435897436),\n",
       "  (467, 36.15897435897436),\n",
       "  (530, 36.15897435897436),\n",
       "  (564, 36.15897435897436),\n",
       "  (741, 36.15897435897436),\n",
       "  (377, 36.25192802056555),\n",
       "  (734, 36.25192802056555),\n",
       "  (740, 36.25192802056555),\n",
       "  (128, 36.345360824742265),\n",
       "  (522, 36.345360824742265),\n",
       "  (538, 36.345360824742265),\n",
       "  (91, 36.43927648578811),\n",
       "  (724, 36.43927648578811),\n",
       "  (186, 36.53367875647668),\n",
       "  (195, 36.819843342036556),\n",
       "  (301, 36.916230366492144),\n",
       "  (664, 37.01312335958005),\n",
       "  (172, 37.11052631578947),\n",
       "  (327, 37.11052631578947),\n",
       "  (503, 37.11052631578947),\n",
       "  (555, 37.20844327176781),\n",
       "  (496, 37.3068783068783),\n",
       "  (103, 37.40583554376658),\n",
       "  (480, 37.40583554376658),\n",
       "  (670, 37.40583554376658),\n",
       "  (727, 37.40583554376658),\n",
       "  (158, 37.505319148936174),\n",
       "  (660, 37.505319148936174),\n",
       "  (432, 37.605333333333334),\n",
       "  (481, 37.605333333333334),\n",
       "  (406, 37.705882352941174),\n",
       "  (487, 37.705882352941174),\n",
       "  (203, 37.806970509383376),\n",
       "  (318, 37.806970509383376),\n",
       "  (702, 37.806970509383376),\n",
       "  (129, 37.90860215053763),\n",
       "  (508, 37.90860215053763),\n",
       "  (385, 38.01078167115903),\n",
       "  (46, 38.32065217391305),\n",
       "  (755, 38.32065217391305),\n",
       "  (504, 38.42506811989101),\n",
       "  (138, 38.53005464480874),\n",
       "  (369, 38.53005464480874),\n",
       "  (672, 38.53005464480874),\n",
       "  (9, 38.74175824175824),\n",
       "  (438, 38.74175824175824),\n",
       "  (757, 38.74175824175824),\n",
       "  (206, 38.84848484848485),\n",
       "  (66, 38.95580110497237),\n",
       "  (126, 38.95580110497237),\n",
       "  (651, 38.95580110497237),\n",
       "  (770, 38.95580110497237),\n",
       "  (194, 39.06371191135734),\n",
       "  (743, 39.06371191135734),\n",
       "  (365, 39.172222222222224),\n",
       "  (771, 39.172222222222224),\n",
       "  (135, 39.28133704735376),\n",
       "  (401, 39.28133704735376),\n",
       "  (726, 39.28133704735376),\n",
       "  (39, 39.391061452513966),\n",
       "  (157, 39.391061452513966),\n",
       "  (336, 39.391061452513966),\n",
       "  (641, 39.391061452513966),\n",
       "  (767, 39.391061452513966),\n",
       "  (114, 39.50140056022409),\n",
       "  (176, 39.50140056022409),\n",
       "  (556, 39.50140056022409),\n",
       "  (615, 39.50140056022409),\n",
       "  (630, 39.50140056022409),\n",
       "  (398, 39.6123595505618),\n",
       "  (567, 39.6123595505618),\n",
       "  (648, 39.6123595505618),\n",
       "  (324, 39.72394366197183),\n",
       "  (759, 39.72394366197183),\n",
       "  (101, 39.836158192090394),\n",
       "  (241, 39.836158192090394),\n",
       "  (156, 39.94900849858357),\n",
       "  (319, 39.94900849858357),\n",
       "  (461, 39.94900849858357),\n",
       "  (130, 40.0625),\n",
       "  (4, 40.17663817663818),\n",
       "  (239, 40.17663817663818),\n",
       "  (682, 40.17663817663818),\n",
       "  (733, 40.17663817663818),\n",
       "  (348, 40.29142857142857),\n",
       "  (703, 40.29142857142857),\n",
       "  (778, 40.29142857142857),\n",
       "  (25, 40.40687679083094),\n",
       "  (247, 40.40687679083094),\n",
       "  (540, 40.40687679083094),\n",
       "  (80, 40.52298850574713),\n",
       "  (243, 40.52298850574713),\n",
       "  (359, 40.52298850574713),\n",
       "  (392, 40.52298850574713),\n",
       "  (278, 40.75722543352601),\n",
       "  (410, 40.75722543352601),\n",
       "  (583, 40.75722543352601),\n",
       "  (292, 40.87536231884058),\n",
       "  (294, 40.87536231884058),\n",
       "  (533, 40.99418604651163),\n",
       "  (340, 41.113702623906704),\n",
       "  (441, 41.113702623906704),\n",
       "  (532, 41.113702623906704),\n",
       "  (675, 41.113702623906704),\n",
       "  (140, 41.23391812865497),\n",
       "  (436, 41.23391812865497),\n",
       "  (175, 41.354838709677416),\n",
       "  (769, 41.354838709677416),\n",
       "  (355, 41.476470588235294),\n",
       "  (452, 41.476470588235294),\n",
       "  (748, 41.476470588235294),\n",
       "  (113, 41.59882005899705),\n",
       "  (422, 41.59882005899705),\n",
       "  (474, 41.59882005899705),\n",
       "  (347, 41.72189349112426),\n",
       "  (50, 41.845697329376854),\n",
       "  (143, 41.845697329376854),\n",
       "  (354, 41.845697329376854),\n",
       "  (28, 41.970238095238095),\n",
       "  (43, 41.970238095238095),\n",
       "  (106, 41.970238095238095),\n",
       "  (249, 41.970238095238095),\n",
       "  (597, 41.970238095238095),\n",
       "  (211, 42.0955223880597),\n",
       "  (230, 42.0955223880597),\n",
       "  (291, 42.0955223880597),\n",
       "  (445, 42.0955223880597),\n",
       "  (606, 42.0955223880597),\n",
       "  (332, 42.221556886227546),\n",
       "  (451, 42.221556886227546),\n",
       "  (68, 42.348348348348345),\n",
       "  (235, 42.348348348348345),\n",
       "  (285, 42.348348348348345),\n",
       "  (337, 42.348348348348345),\n",
       "  (389, 42.348348348348345),\n",
       "  (246, 42.475903614457835),\n",
       "  (345, 42.475903614457835),\n",
       "  (476, 42.475903614457835),\n",
       "  (99, 42.60422960725076),\n",
       "  (208, 42.733333333333334),\n",
       "  (780, 42.733333333333334),\n",
       "  (784, 42.733333333333334),\n",
       "  (197, 42.86322188449848),\n",
       "  (450, 42.86322188449848),\n",
       "  (668, 42.86322188449848),\n",
       "  (88, 42.99390243902439),\n",
       "  (626, 42.99390243902439),\n",
       "  (707, 42.99390243902439),\n",
       "  (587, 43.12538226299694),\n",
       "  (352, 43.39076923076923),\n",
       "  (722, 43.39076923076923),\n",
       "  (3, 43.52469135802469),\n",
       "  (5, 43.52469135802469),\n",
       "  (37, 43.52469135802469),\n",
       "  (97, 43.52469135802469),\n",
       "  (133, 43.52469135802469),\n",
       "  (263, 43.52469135802469),\n",
       "  (395, 43.52469135802469),\n",
       "  (102, 43.6594427244582),\n",
       "  (238, 43.6594427244582),\n",
       "  (304, 43.6594427244582),\n",
       "  (576, 43.79503105590062),\n",
       "  (313, 43.93146417445483),\n",
       "  (596, 43.93146417445483),\n",
       "  (749, 43.93146417445483),\n",
       "  (299, 44.06875),\n",
       "  (414, 44.06875),\n",
       "  (464, 44.06875),\n",
       "  (519, 44.06875),\n",
       "  (539, 44.06875),\n",
       "  (644, 44.06875),\n",
       "  (49, 44.34591194968554),\n",
       "  (469, 44.34591194968554),\n",
       "  (223, 44.48580441640379),\n",
       "  (151, 44.62658227848101),\n",
       "  (754, 44.62658227848101),\n",
       "  (65, 44.768253968253966),\n",
       "  (79, 44.768253968253966),\n",
       "  (360, 44.768253968253966),\n",
       "  (521, 44.768253968253966),\n",
       "  (688, 44.768253968253966),\n",
       "  (750, 44.768253968253966),\n",
       "  (758, 44.768253968253966),\n",
       "  (383, 44.910828025477706),\n",
       "  (517, 44.910828025477706),\n",
       "  (646, 44.910828025477706),\n",
       "  (344, 45.054313099041536),\n",
       "  (673, 45.054313099041536),\n",
       "  (74, 45.19871794871795),\n",
       "  (162, 45.19871794871795),\n",
       "  (214, 45.19871794871795),\n",
       "  (270, 45.19871794871795),\n",
       "  (609, 45.19871794871795),\n",
       "  (621, 45.19871794871795),\n",
       "  (700, 45.19871794871795),\n",
       "  (180, 45.344051446945336),\n",
       "  (404, 45.344051446945336),\n",
       "  (585, 45.344051446945336),\n",
       "  (640, 45.344051446945336),\n",
       "  (782, 45.344051446945336),\n",
       "  (122, 45.49032258064516),\n",
       "  (329, 45.49032258064516),\n",
       "  (443, 45.49032258064516),\n",
       "  (475, 45.49032258064516),\n",
       "  (108, 45.637540453074436),\n",
       "  (139, 45.785714285714285),\n",
       "  (411, 45.785714285714285),\n",
       "  (745, 45.785714285714285),\n",
       "  (146, 45.93485342019544),\n",
       "  (190, 45.93485342019544),\n",
       "  (321, 45.93485342019544),\n",
       "  (477, 45.93485342019544),\n",
       "  (636, 45.93485342019544),\n",
       "  (690, 45.93485342019544),\n",
       "  (173, 46.08496732026144),\n",
       "  (293, 46.08496732026144),\n",
       "  (756, 46.08496732026144),\n",
       "  (123, 46.23606557377049),\n",
       "  (185, 46.23606557377049),\n",
       "  (490, 46.23606557377049),\n",
       "  (655, 46.23606557377049),\n",
       "  (251, 46.38815789473684),\n",
       "  (372, 46.38815789473684),\n",
       "  (509, 46.38815789473684),\n",
       "  (537, 46.38815789473684),\n",
       "  (545, 46.38815789473684),\n",
       "  (709, 46.38815789473684),\n",
       "  (735, 46.38815789473684),\n",
       "  (762, 46.38815789473684),\n",
       "  (196, 46.54125412541254),\n",
       "  (317, 46.54125412541254),\n",
       "  (334, 46.54125412541254),\n",
       "  (594, 46.54125412541254),\n",
       "  (24, 46.6953642384106),\n",
       "  (118, 46.6953642384106),\n",
       "  (124, 46.6953642384106),\n",
       "  (137, 46.6953642384106),\n",
       "  (258, 46.6953642384106),\n",
       "  (302, 46.6953642384106),\n",
       "  (507, 46.6953642384106),\n",
       "  (584, 46.6953642384106),\n",
       "  (616, 46.6953642384106),\n",
       "  (691, 46.6953642384106),\n",
       "  (393, 46.85049833887043),\n",
       "  (570, 46.85049833887043),\n",
       "  (255, 47.00666666666667),\n",
       "  (342, 47.00666666666667),\n",
       "  (472, 47.00666666666667),\n",
       "  (747, 47.00666666666667),\n",
       "  (250, 47.163879598662206),\n",
       "  (253, 47.163879598662206),\n",
       "  (276, 47.163879598662206),\n",
       "  (287, 47.163879598662206),\n",
       "  (346, 47.163879598662206),\n",
       "  (367, 47.163879598662206),\n",
       "  (473, 47.163879598662206),\n",
       "  (526, 47.163879598662206),\n",
       "  (642, 47.163879598662206),\n",
       "  (718, 47.163879598662206),\n",
       "  (70, 47.32214765100671),\n",
       "  (144, 47.32214765100671),\n",
       "  (271, 47.32214765100671),\n",
       "  (388, 47.32214765100671),\n",
       "  (495, 47.32214765100671),\n",
       "  (698, 47.32214765100671),\n",
       "  (363, 47.48148148148148),\n",
       "  (479, 47.48148148148148),\n",
       "  (488, 47.48148148148148),\n",
       "  (593, 47.48148148148148),\n",
       "  (296, 47.641891891891895),\n",
       "  (618, 47.641891891891895),\n",
       "  (100, 47.80338983050847),\n",
       "  (308, 47.80338983050847),\n",
       "  (416, 47.80338983050847),\n",
       "  (561, 47.80338983050847),\n",
       "  (592, 47.80338983050847),\n",
       "  (665, 47.80338983050847),\n",
       "  (333, 47.965986394557824),\n",
       "  (423, 47.965986394557824),\n",
       "  (683, 47.965986394557824),\n",
       "  (127, 48.129692832764505),\n",
       "  (269, 48.129692832764505),\n",
       "  (562, 48.129692832764505),\n",
       "  (715, 48.129692832764505),\n",
       "  (240, 48.294520547945204),\n",
       "  (412, 48.294520547945204),\n",
       "  (48, 48.460481099656356),\n",
       "  (145, 48.460481099656356),\n",
       "  (236, 48.460481099656356),\n",
       "  (551, 48.460481099656356),\n",
       "  (588, 48.460481099656356),\n",
       "  (221, 48.62758620689655),\n",
       "  (283, 48.62758620689655),\n",
       "  (390, 48.62758620689655),\n",
       "  (591, 48.62758620689655),\n",
       "  (659, 48.62758620689655),\n",
       "  (57, 48.79584775086505),\n",
       "  (120, 48.79584775086505),\n",
       "  (182, 48.79584775086505),\n",
       "  (188, 48.79584775086505),\n",
       "  (192, 48.79584775086505),\n",
       "  (362, 48.79584775086505),\n",
       "  (437, 48.79584775086505),\n",
       "  (614, 48.79584775086505),\n",
       "  (109, 48.96527777777778),\n",
       "  (386, 48.96527777777778),\n",
       "  (387, 48.96527777777778),\n",
       "  (563, 48.96527777777778),\n",
       "  (781, 48.96527777777778),\n",
       "  (69, 49.13588850174216),\n",
       "  (200, 49.13588850174216),\n",
       "  (573, 49.13588850174216),\n",
       "  (577, 49.13588850174216),\n",
       "  (579, 49.13588850174216),\n",
       "  (193, 49.30769230769231),\n",
       "  (353, 49.30769230769231),\n",
       "  (52, 49.48070175438596),\n",
       "  (331, 49.48070175438596),\n",
       "  (484, 49.48070175438596),\n",
       "  (574, 49.48070175438596),\n",
       "  (578, 49.48070175438596),\n",
       "  (598, 49.48070175438596),\n",
       "  (78, 49.65492957746479),\n",
       "  (148, 49.65492957746479),\n",
       "  (149, 49.65492957746479),\n",
       "  (282, 49.65492957746479),\n",
       "  (421, 49.65492957746479),\n",
       "  (463, 49.65492957746479),\n",
       "  (512, 49.65492957746479),\n",
       "  (209, 49.830388692579504),\n",
       "  (219, 49.830388692579504),\n",
       "  (430, 49.830388692579504),\n",
       "  (489, 49.830388692579504),\n",
       "  (633, 49.830388692579504),\n",
       "  (679, 49.830388692579504),\n",
       "  (742, 49.830388692579504),\n",
       "  (75, 50.00709219858156),\n",
       "  (184, 50.00709219858156),\n",
       "  (589, 50.00709219858156),\n",
       "  (179, 50.18505338078292),\n",
       "  (222, 50.18505338078292),\n",
       "  (225, 50.18505338078292),\n",
       "  (326, 50.18505338078292),\n",
       "  (492, 50.18505338078292),\n",
       "  (501, 50.18505338078292),\n",
       "  (764, 50.18505338078292),\n",
       "  (60, 50.364285714285714),\n",
       "  (95, 50.364285714285714),\n",
       "  (569, 50.364285714285714),\n",
       "  (738, 50.364285714285714),\n",
       "  (751, 50.364285714285714),\n",
       "  (27, 50.54480286738351),\n",
       "  (254, 50.54480286738351),\n",
       "  (259, 50.54480286738351),\n",
       "  (265, 50.54480286738351),\n",
       "  (29, 50.726618705035975),\n",
       "  (297, 50.726618705035975),\n",
       "  (373, 50.726618705035975),\n",
       "  (440, 50.726618705035975),\n",
       "  (677, 50.726618705035975),\n",
       "  (82, 50.90974729241877),\n",
       "  (500, 50.90974729241877),\n",
       "  (678, 50.90974729241877),\n",
       "  (777, 50.90974729241877),\n",
       "  (84, 51.094202898550726),\n",
       "  (94, 51.094202898550726),\n",
       "  (105, 51.094202898550726),\n",
       "  (470, 51.094202898550726),\n",
       "  (515, 51.094202898550726),\n",
       "  (523, 51.094202898550726),\n",
       "  (704, 51.094202898550726),\n",
       "  (711, 51.094202898550726),\n",
       "  (6, 51.28),\n",
       "  (232, 51.28),\n",
       "  (298, 51.28),\n",
       "  (364, 51.28),\n",
       "  (281, 51.46715328467153),\n",
       "  (315, 51.46715328467153),\n",
       "  (634, 51.46715328467153),\n",
       "  (785, 51.46715328467153),\n",
       "  (107, 51.65567765567766),\n",
       "  (595, 51.65567765567766),\n",
       "  (645, 51.65567765567766),\n",
       "  (661, 51.65567765567766),\n",
       "  (424, 51.845588235294116),\n",
       "  (632, 51.845588235294116),\n",
       "  (775, 51.845588235294116),\n",
       "  (104, 52.03690036900369),\n",
       "  (262, 52.03690036900369),\n",
       "  (268, 52.03690036900369),\n",
       "  (417, 52.22962962962963),\n",
       "  (783, 52.22962962962963),\n",
       "  (55, 52.42379182156134),\n",
       "  (132, 52.42379182156134),\n",
       "  (248, 52.42379182156134),\n",
       "  (505, 52.42379182156134),\n",
       "  (531, 52.42379182156134),\n",
       "  (549, 52.42379182156134),\n",
       "  (686, 52.42379182156134),\n",
       "  (699, 52.42379182156134),\n",
       "  (136, 52.61940298507463),\n",
       "  (687, 52.61940298507463),\n",
       "  (716, 52.61940298507463),\n",
       "  (760, 52.61940298507463),\n",
       "  (167, 53.015037593984964),\n",
       "  (169, 53.015037593984964),\n",
       "  (439, 53.015037593984964),\n",
       "  (638, 53.015037593984964),\n",
       "  (650, 53.015037593984964),\n",
       "  (31, 53.21509433962264),\n",
       "  (160, 53.21509433962264),\n",
       "  (307, 53.21509433962264),\n",
       "  (498, 53.21509433962264),\n",
       "  (568, 53.21509433962264),\n",
       "  (662, 53.21509433962264),\n",
       "  (712, 53.21509433962264),\n",
       "  (21, 53.416666666666664),\n",
       "  (98, 53.416666666666664),\n",
       "  (368, 53.416666666666664),\n",
       "  (434, 53.416666666666664),\n",
       "  (465, 53.416666666666664),\n",
       "  (779, 53.416666666666664),\n",
       "  (131, 53.61977186311787),\n",
       "  (680, 53.61977186311787),\n",
       "  (339, 53.82442748091603),\n",
       "  (571, 53.82442748091603),\n",
       "  (207, 54.030651340996165),\n",
       "  (266, 54.030651340996165),\n",
       "  (534, 54.030651340996165),\n",
       "  (624, 54.030651340996165),\n",
       "  (684, 54.030651340996165),\n",
       "  (695, 54.030651340996165),\n",
       "  (697, 54.030651340996165),\n",
       "  (654, 54.238461538461536),\n",
       "  (746, 54.238461538461536),\n",
       "  (22, 54.447876447876446),\n",
       "  (202, 54.447876447876446),\n",
       "  (456, 54.447876447876446),\n",
       "  (627, 54.447876447876446),\n",
       "  (649, 54.447876447876446),\n",
       "  (671, 54.447876447876446),\n",
       "  (198, 54.65891472868217),\n",
       "  (20, 54.8715953307393),\n",
       "  (166, 54.8715953307393),\n",
       "  (288, 54.8715953307393),\n",
       "  (510, 54.8715953307393),\n",
       "  (10, 55.0859375),\n",
       "  (12, 55.0859375),\n",
       "  (379, 55.0859375),\n",
       "  (610, 55.0859375),\n",
       "  (657, 55.0859375),\n",
       "  (164, 55.30196078431373),\n",
       "  (663, 55.30196078431373),\n",
       "  (199, 55.51968503937008),\n",
       "  (205, 55.51968503937008),\n",
       "  (257, 55.51968503937008),\n",
       "  (286, 55.51968503937008),\n",
       "  (316, 55.51968503937008),\n",
       "  (557, 55.51968503937008),\n",
       "  (696, 55.51968503937008),\n",
       "  (310, 55.73913043478261),\n",
       "  (147, 55.96031746031746),\n",
       "  (210, 55.96031746031746),\n",
       "  (2, 56.18326693227092),\n",
       "  (45, 56.18326693227092),\n",
       "  (83, 56.18326693227092),\n",
       "  (154, 56.18326693227092),\n",
       "  (275, 56.18326693227092),\n",
       "  (491, 56.18326693227092),\n",
       "  (774, 56.18326693227092),\n",
       "  (35, 56.408),\n",
       "  (112, 56.408),\n",
       "  (224, 56.408),\n",
       "  (720, 56.408),\n",
       "  (81, 56.63453815261044),\n",
       "  (397, 56.63453815261044),\n",
       "  (399, 56.63453815261044),\n",
       "  (543, 56.63453815261044),\n",
       "  (763, 56.63453815261044),\n",
       "  (119, 56.86290322580645),\n",
       "  (544, 56.86290322580645),\n",
       "  (72, 57.093117408906885),\n",
       "  (165, 57.093117408906885),\n",
       "  (403, 57.093117408906885),\n",
       "  (33, 57.32520325203252),\n",
       "  (384, 57.32520325203252),\n",
       "  (676, 57.32520325203252),\n",
       "  (572, 57.559183673469384),\n",
       "  (215, 58.03292181069959),\n",
       "  (553, 58.03292181069959),\n",
       "  (201, 58.27272727272727),\n",
       "  (381, 58.51452282157676),\n",
       "  (460, 58.51452282157676),\n",
       "  (536, 58.75833333333333),\n",
       "  (400, 59.00418410041841),\n",
       "  (497, 59.00418410041841),\n",
       "  (38, 59.252100840336134),\n",
       "  (435, 59.252100840336134),\n",
       "  (8, 59.50210970464135),\n",
       "  (535, 59.50210970464135),\n",
       "  (541, 59.50210970464135),\n",
       "  (518, 59.75423728813559),\n",
       "  (325, 60.00851063829787),\n",
       "  (612, 60.00851063829787),\n",
       "  (187, 60.26495726495727),\n",
       "  (619, 60.26495726495727),\n",
       "  (736, 60.78448275862069),\n",
       "  (449, 61.580786026200876),\n",
       "  (462, 61.580786026200876),\n",
       "  (453, 62.12334801762115),\n",
       "  (18, 62.39823008849557),\n",
       "  (41, 62.675555555555555),\n",
       "  (374, 63.237668161434975),\n",
       "  (279, 63.52252252252252),\n",
       "  (351, 63.52252252252252),\n",
       "  (47, 65.5906976744186),\n",
       "  (454, 81.04597701149426),\n",
       "  (63, 81.98837209302326),\n",
       "  (455, 102.18840579710145),\n",
       "  (542, 153.2826086956522),\n",
       "  (499, 171.97560975609755),\n",
       "  (674, 251.82142857142858),\n",
       "  (586, 276.5098039215686)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Performance Log for the Model\n",
    "\n",
    "We will log the original performance as a reference point as well as the performance of the latest model to the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    simple_log\n",
    "except NameError:\n",
    "    simple_log = []\n",
    "\n",
    "with open('data/simple_log.jsonl') as f:\n",
    "    for line in f:\n",
    "        simple_log.append(json.loads(line))\n",
    "\n",
    "SEQUENCE = simple_log[-1]['sequence'] if len(simple_log) > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a Simple CNN Model to Classify Questions to their Corresponding Tags\n",
    "\n",
    "We start with a simple model with one `Conv1D`/`GlobalMaxPool1D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.025429034848639277,\n",
       "  'categorical_accuracy': 0.4202008843421936,\n",
       "  'precision': 0.7850497364997864,\n",
       "  'recall': 0.561117947101593,\n",
       "  'binary_accuracy': 0.9912359118461609,\n",
       "  'hinge': 0.9985249042510986,\n",
       "  'auc': 0.977318286895752,\n",
       "  'accuracy': 0.028561104089021683,\n",
       "  'mean_absolute_error': 0.013315621763467789,\n",
       "  'mean_absolute_percentage_error': 6739443.0,\n",
       "  'true_positives': 178463.0,\n",
       "  'false_positives': 48864.0,\n",
       "  'true_negatives': 21137114.0,\n",
       "  'false_negatives': 139586.0,\n",
       "  'name': 'kim_cnn_complex',\n",
       "  'sequence': 35,\n",
       "  'f1': 0.32722929088516656},\n",
       " {'loss': 0.028156400370478098,\n",
       "  'categorical_accuracy': 0.4214495122432709,\n",
       "  'precision': 0.6774758100509644,\n",
       "  'recall': 0.6599706411361694,\n",
       "  'binary_accuracy': 0.9903233051300049,\n",
       "  'hinge': 1.0019246339797974,\n",
       "  'auc': 0.9820438027381897,\n",
       "  'accuracy': 0.0005144624155946076,\n",
       "  'mean_absolute_error': 0.01671472191810608,\n",
       "  'mean_absolute_percentage_error': 11342052.0,\n",
       "  'true_positives': 209903.0,\n",
       "  'false_positives': 99928.0,\n",
       "  'true_negatives': 21086010.0,\n",
       "  'false_negatives': 108146.0,\n",
       "  'name': 'basic_cnn',\n",
       "  'sequence': 36,\n",
       "  'f1': 0.33430433369255025},\n",
       " {'loss': 0.01694748630270624,\n",
       "  'categorical_accuracy': 0.3807617127895355,\n",
       "  'precision': 0.6619125604629517,\n",
       "  'recall': 0.5946587920188904,\n",
       "  'binary_accuracy': 0.9945917129516602,\n",
       "  'hinge': 1.002063512802124,\n",
       "  'auc': 0.9780333042144775,\n",
       "  'accuracy': 0.0007571718888357282,\n",
       "  'mean_absolute_error': 0.00968870334327221,\n",
       "  'mean_absolute_percentage_error': 6508402.5,\n",
       "  'true_positives': 242908.0,\n",
       "  'false_positives': 124071.0,\n",
       "  'true_negatives': 53043088.0,\n",
       "  'false_negatives': 165575.0,\n",
       "  'name': 'basic_cnn_5000_limit',\n",
       "  'sequence': 37,\n",
       "  'f1': 0.3132429550057862},\n",
       " {'loss': 0.017176187729819502,\n",
       "  'categorical_accuracy': 0.381195068359375,\n",
       "  'precision': 0.6433826088905334,\n",
       "  'recall': 0.611038863658905,\n",
       "  'binary_accuracy': 0.9944475293159485,\n",
       "  'hinge': 1.0023584365844727,\n",
       "  'auc': 0.9792392253875732,\n",
       "  'accuracy': 0.0009749013115651906,\n",
       "  'mean_absolute_error': 0.009982646442949772,\n",
       "  'mean_absolute_percentage_error': 6896639.0,\n",
       "  'true_positives': 249599.0,\n",
       "  'false_positives': 138349.0,\n",
       "  'true_negatives': 53028884.0,\n",
       "  'false_negatives': 158884.0,\n",
       "  'name': 'basic_cnn_5000_limit_10_epochs',\n",
       "  'sequence': 38,\n",
       "  'f1': 0.3133968820187582},\n",
       " {'loss': 0.013641377153544455,\n",
       "  'categorical_accuracy': 0.3978332579135895,\n",
       "  'precision': 0.7862475514411926,\n",
       "  'recall': 0.548923671245575,\n",
       "  'binary_accuracy': 0.9954243898391724,\n",
       "  'hinge': 0.9992377161979675,\n",
       "  'auc': 0.9775896072387695,\n",
       "  'accuracy': 0.09095804393291473,\n",
       "  'mean_absolute_error': 0.006862356327474117,\n",
       "  'mean_absolute_percentage_error': 3360255.5,\n",
       "  'true_positives': 224226.0,\n",
       "  'false_positives': 60959.0,\n",
       "  'true_negatives': 53106280.0,\n",
       "  'false_negatives': 184257.0,\n",
       "  'name': 'kim_cnn_5000_limit_10_epochs',\n",
       "  'sequence': 40,\n",
       "  'f1': 0.323246850375081},\n",
       " {'loss': 0.014030436042412475,\n",
       "  'categorical_accuracy': 0.3990112245082855,\n",
       "  'precision': 0.7948366403579712,\n",
       "  'recall': 0.5220902562141418,\n",
       "  'binary_accuracy': 0.9953285455703735,\n",
       "  'hinge': 0.9993232488632202,\n",
       "  'auc': 0.9759425520896912,\n",
       "  'accuracy': 0.07194202393293381,\n",
       "  'mean_absolute_error': 0.006946335546672344,\n",
       "  'mean_absolute_percentage_error': 3280998.75,\n",
       "  'true_positives': 213265.0,\n",
       "  'false_positives': 55048.0,\n",
       "  'true_negatives': 53112236.0,\n",
       "  'false_negatives': 195218.0,\n",
       "  'name': 'kim_cnn_5000_limit_10_epochs_save_best',\n",
       "  'sequence': 41,\n",
       "  'f1': 0.31510971967619567},\n",
       " {'loss': 0.024671485709404806,\n",
       "  'categorical_accuracy': 0.3803955018520355,\n",
       "  'precision': 0.4904581904411316,\n",
       "  'recall': 0.7498427033424377,\n",
       "  'binary_accuracy': 0.9921571016311646,\n",
       "  'hinge': 1.0084298849105835,\n",
       "  'auc': 0.9857017993927002,\n",
       "  'accuracy': 0.0006525535718537867,\n",
       "  'mean_absolute_error': 0.01605524867773056,\n",
       "  'mean_absolute_percentage_error': 13890733.0,\n",
       "  'true_positives': 306298.0,\n",
       "  'false_positives': 318216.0,\n",
       "  'true_negatives': 52849048.0,\n",
       "  'false_negatives': 102185.0,\n",
       "  'name': 'simple_5000_limit_10_epochs_save_best',\n",
       "  'sequence': 42,\n",
       "  'f1': 0.29651393241759044},\n",
       " {'loss': 0.02332179286331666,\n",
       "  'categorical_accuracy': 0.3853210508823395,\n",
       "  'precision': 0.5106842517852783,\n",
       "  'recall': 0.7252468466758728,\n",
       "  'binary_accuracy': 0.9926097989082336,\n",
       "  'hinge': 1.0073416233062744,\n",
       "  'auc': 0.9842875003814697,\n",
       "  'accuracy': 0.0008280996116809547,\n",
       "  'mean_absolute_error': 0.014967193827033043,\n",
       "  'mean_absolute_percentage_error': 12642280.0,\n",
       "  'true_positives': 296251.0,\n",
       "  'false_positives': 283855.0,\n",
       "  'true_negatives': 52883420.0,\n",
       "  'false_negatives': 112232.0,\n",
       "  'name': 'kim_cnn_limit_10_epochs_save_best',\n",
       "  'sequence': 44,\n",
       "  'f1': 0.2996705428930854},\n",
       " {'loss': 0.02494977526657749,\n",
       "  'categorical_accuracy': 0.38348388671875,\n",
       "  'precision': 0.48939934372901917,\n",
       "  'recall': 0.7291515469551086,\n",
       "  'binary_accuracy': 0.9921374320983887,\n",
       "  'hinge': 1.0085182189941406,\n",
       "  'auc': 0.9841805100440979,\n",
       "  'accuracy': 0.0005458820378407836,\n",
       "  'mean_absolute_error': 0.016143226996064186,\n",
       "  'mean_absolute_percentage_error': 13851891.0,\n",
       "  'true_positives': 297846.0,\n",
       "  'false_positives': 310749.0,\n",
       "  'true_negatives': 52856432.0,\n",
       "  'false_negatives': 110637.0,\n",
       "  'name': 'kim_cnn_limit_10_epochs_save_best_2',\n",
       "  'sequence': 45,\n",
       "  'f1': 0.2928447972808801},\n",
       " {'loss': 0.017647538313734508,\n",
       "  'categorical_accuracy': 0.31393980979919434,\n",
       "  'precision': 0.3907146155834198,\n",
       "  'recall': 0.718410074710846,\n",
       "  'binary_accuracy': 0.9949800968170166,\n",
       "  'hinge': 1.0085320472717285,\n",
       "  'auc': 0.9840664267539978,\n",
       "  'accuracy': 0.0018648107070475817,\n",
       "  'mean_absolute_error': 0.012112497352063656,\n",
       "  'mean_absolute_percentage_error': 11001994.0,\n",
       "  'true_positives': 313114.0,\n",
       "  'false_positives': 488274.0,\n",
       "  'true_negatives': 120811464.0,\n",
       "  'false_negatives': 122729.0,\n",
       "  'name': 'kim_cnn_2000',\n",
       "  'sequence': 46,\n",
       "  'f1': 0.25307642921323154}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = 3\n",
    "EPOCHS              = 5\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "STRIDES             = 1\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "EXPERIMENT_NAME = 'simple_cnn'\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'embedding': 'own',\n",
    "        'architecture': 'Kim CNN',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'filter_count': FILTER_COUNT,\n",
    "        'filter_size': FILTER_SIZE,\n",
    "        'activation': ACTIVATION,\n",
    "        'conv_padding': CONV_PADDING,\n",
    "        'sequence': SEQUENCE\n",
    "    }\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        TOKEN_COUNT,\n",
    "        EMBED_SIZE, \n",
    "        input_length=X_train.shape[1],\n",
    "        embeddings_initializer=RandomUniform(),\n",
    "    )\n",
    ")\n",
    "# model.add(\n",
    "#     Embedding(\n",
    "#         TOKEN_COUNT,\n",
    "#         EMBED_SIZE,\n",
    "#         weights=[embedding_matrix],\n",
    "#         input_length=MAX_LEN,\n",
    "#         trainable=True\n",
    "#     )\n",
    "# )\n",
    "model.add(Dropout(0.1))\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        FILTER_COUNT, \n",
    "        FILTER_SIZE, \n",
    "        padding=CONV_PADDING, \n",
    "        activation=ACTIVATION, \n",
    "        strides=1\n",
    "    )\n",
    ")\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(\n",
    "    Dense(\n",
    "        y_train.shape[1],\n",
    "        activation='sigmoid',\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'categorical_accuracy',\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        patience=2\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=TEST_SPLIT,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Kim-CNN Model to Classify Questions to their Corresponding Tags\n",
    "\n",
    "We will train a model to classify questions into categories corresponding to tags based on [Kim-CNN](https://arxiv.org/abs/1408.5882)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simple_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb30cadc9bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mCONV_DROPOUT_RATIO\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mEXPERIMENT_NAME\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msimple_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RENAME YOUR EXPERIMENT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RENAME YOUR EXPERIMENT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_log' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Embedding, Flatten, MaxPool1D, GlobalMaxPool1D, Dropout, Conv1D, Input, concatenate\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy, kld\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "EXPERIMENT_NAME = 'kim_cnn_2000_embed_50_drop'\n",
    "\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZE         = [3, 4, 5]\n",
    "EPOCHS              = 5\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "\n",
    "if EXPERIMENT_NAME == simple_log[-1]['name']:\n",
    "    print('RENAME YOUR EXPERIMENT')\n",
    "    raise Exception('RENAME YOUR EXPERIMENT')\n",
    "\n",
    "SEQUENCE += 1\n",
    "\n",
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "config.update(\n",
    "    {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'embedding': 'own',\n",
    "        'architecture': 'Kim CNN',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'filter_count': FILTER_COUNT,\n",
    "        'filter_size': FILTER_SIZE,\n",
    "        'activation': ACTIVATION,\n",
    "        'conv_padding': CONV_PADDING,\n",
    "        'sequence': SEQUENCE\n",
    "    }\n",
    ")\n",
    "\n",
    "padded_input = Input(\n",
    "    shape=(X_train.shape[1],),\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "emb = Embedding(\n",
    "    TOKEN_COUNT, \n",
    "    EMBED_SIZE,\n",
    "    embeddings_initializer=RandomUniform(),\n",
    "    input_length=X_train.shape[1]\n",
    ")(padded_input)\n",
    "# emb = Embedding(\n",
    "#     TOKEN_COUNT,\n",
    "#     EMBED_SIZE,\n",
    "#     weights=[embedding_matrix],\n",
    "#     input_length=MAX_LEN,\n",
    "#     trainable=True,\n",
    "# )(padded_input)\n",
    "drp = Dropout(0.1)(emb)\n",
    "\n",
    "# Create convlutions of different sizes\n",
    "convs = []\n",
    "for filter_size in FILTER_SIZE:\n",
    "    f_conv = Conv1D(\n",
    "        filters=FILTER_COUNT,\n",
    "        kernel_size=filter_size,\n",
    "        padding=CONV_PADDING,\n",
    "        activation=ACTIVATION\n",
    "    )(drp)\n",
    "    f_pool = MaxPool1D(filter_size)(f_conv)\n",
    "    convs.append(f_pool)\n",
    "\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "l_conv = Conv1D(\n",
    "    128,\n",
    "    5,\n",
    "    activation=ACTIVATION\n",
    ")(l_merge)\n",
    "l_pool = GlobalMaxPool1D()(l_conv)\n",
    "l_flat = Flatten()(l_pool)\n",
    "l_drp  = Dropout(CONV_DROPOUT_RATIO)(l_flat)\n",
    "l_dense = Dense(\n",
    "    128,\n",
    "    activation=ACTIVATION\n",
    ")(l_drp)\n",
    "out_dense = Dense(\n",
    "    y_train.shape[1],\n",
    "    activation='sigmoid'\n",
    ")(l_dense)\n",
    "\n",
    "model = Model(inputs=padded_input, outputs=out_dense)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.Hinge(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        patience=3\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/cnn_tagger.weights.hdf5')\n",
    "metrics = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.018\n",
      "categorical_accuracy: 0.314\n",
      "precision: 0.391\n",
      "recall: 0.718\n",
      "binary_accuracy: 0.995\n",
      "hinge: 1.009\n",
      "auc: 0.984\n",
      "accuracy: 0.002\n",
      "mean_absolute_error: 0.012\n",
      "mean_absolute_percentage_error: 11001994.000\n",
      "true_positives: 313114.000\n",
      "false_positives: 488274.000\n",
      "true_negatives: 120811464.000\n",
      "false_negatives: 122729.000\n",
      "f1: 0.253\n"
     ]
    }
   ],
   "source": [
    "log = {}\n",
    "for name, val in zip(model.metrics_names, metrics):\n",
    "    \n",
    "    if name[-1].isdigit():\n",
    "        repeat_name = '_'.join(name.split('_')[:-1])\n",
    "    else:\n",
    "        repeat_name = name\n",
    "    py_val = val.item() if isinstance(val, np.float32) else val\n",
    "    \n",
    "    log[repeat_name] = py_val\n",
    "    \n",
    "log.update({'name': EXPERIMENT_NAME})\n",
    "log.update({'sequence': SEQUENCE})\n",
    "\n",
    "log.update({'f1': (log['precision'] * log['recall']) / (log['precision'] + log['recall'])})\n",
    "\n",
    "simple_log.append(log)\n",
    "\n",
    "with open('data/simple_log.jsonl', 'w') as f:\n",
    "    [f.write(json.dumps(l) + '\\n') for l in simple_log]\n",
    "\n",
    "for key, val in log.items():\n",
    "    if key not in ['name', 'sequence']:\n",
    "        print(f'{key}: {val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous</th>\n",
       "      <th>current</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>hinge</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>true_positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>-0.026696</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.106261</td>\n",
       "      <td>-0.074153</td>\n",
       "      <td>-16857.0</td>\n",
       "      <td>439410.0</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>-0.007781</td>\n",
       "      <td>-0.001203</td>\n",
       "      <td>4262551.0</td>\n",
       "      <td>-0.394335</td>\n",
       "      <td>0.157292</td>\n",
       "      <td>99674350.0</td>\n",
       "      <td>134651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best_2</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>-0.069544</td>\n",
       "      <td>-0.039768</td>\n",
       "      <td>12092.0</td>\n",
       "      <td>177525.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007302</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-2849897.0</td>\n",
       "      <td>-0.098685</td>\n",
       "      <td>-0.010741</td>\n",
       "      <td>67955032.0</td>\n",
       "      <td>15268.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              previous       current  accuracy       auc  \\\n",
       "0                      kim_cnn_complex  kim_cnn_2000 -0.026696  0.006748   \n",
       "1  kim_cnn_limit_10_epochs_save_best_2  kim_cnn_2000  0.001319 -0.000114   \n",
       "\n",
       "   binary_accuracy  categorical_accuracy        f1  false_negatives  \\\n",
       "0         0.003744             -0.106261 -0.074153         -16857.0   \n",
       "1         0.002843             -0.069544 -0.039768          12092.0   \n",
       "\n",
       "   false_positives     hinge      loss  mean_absolute_error  \\\n",
       "0         439410.0  0.010007 -0.007781            -0.001203   \n",
       "1         177525.0  0.000014 -0.007302            -0.004031   \n",
       "\n",
       "   mean_absolute_percentage_error  precision    recall  true_negatives  \\\n",
       "0                       4262551.0  -0.394335  0.157292      99674350.0   \n",
       "1                      -2849897.0  -0.098685 -0.010741      67955032.0   \n",
       "\n",
       "   true_positives  \n",
       "0        134651.0  \n",
       "1         15268.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to original\n",
    "if len(simple_log) > 1:\n",
    "    d2 = simple_log[-1]\n",
    "    d1 = simple_log[0]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "log_diff_1 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_1['current'] = d2['name']\n",
    "log_diff_1['previous'] = d1['name']\n",
    "\n",
    "# Compare to last run\n",
    "if len(simple_log) > 1:\n",
    "    d1 = simple_log[-2]\n",
    "    d2 = simple_log[-1]\n",
    "else:\n",
    "    d1 = simple_log[0]\n",
    "    d2 = simple_log[0]\n",
    "    \n",
    "log_diff_2 = {key: d2.get(key, 0) - d1.get(key, 0) for key in d1.keys() if key not in ['name', 'sequence']}\n",
    "log_diff_2['current'] = d2['name']\n",
    "log_diff_2['previous'] = d1['name']\n",
    "\n",
    "df = pd.DataFrame.from_dict([log_diff_1, log_diff_2])\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('previous')\n",
    "cols.remove('current')\n",
    "show_cols = ['previous', 'current'] + cols\n",
    "df[show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>hinge</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>kim_cnn_complex</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.420201</td>\n",
       "      <td>0.785050</td>\n",
       "      <td>0.561118</td>\n",
       "      <td>0.327229</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>178463.0</td>\n",
       "      <td>48864.0</td>\n",
       "      <td>21137114.0</td>\n",
       "      <td>139586.0</td>\n",
       "      <td>0.998525</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>basic_cnn</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.421450</td>\n",
       "      <td>0.677476</td>\n",
       "      <td>0.659971</td>\n",
       "      <td>0.334304</td>\n",
       "      <td>0.982044</td>\n",
       "      <td>209903.0</td>\n",
       "      <td>99928.0</td>\n",
       "      <td>21086010.0</td>\n",
       "      <td>108146.0</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>0.016715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>basic_cnn_5000_limit</td>\n",
       "      <td>0.016947</td>\n",
       "      <td>0.380762</td>\n",
       "      <td>0.661913</td>\n",
       "      <td>0.594659</td>\n",
       "      <td>0.313243</td>\n",
       "      <td>0.978033</td>\n",
       "      <td>242908.0</td>\n",
       "      <td>124071.0</td>\n",
       "      <td>53043088.0</td>\n",
       "      <td>165575.0</td>\n",
       "      <td>1.002064</td>\n",
       "      <td>0.009689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>basic_cnn_5000_limit_10_epochs</td>\n",
       "      <td>0.017176</td>\n",
       "      <td>0.381195</td>\n",
       "      <td>0.643383</td>\n",
       "      <td>0.611039</td>\n",
       "      <td>0.313397</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>249599.0</td>\n",
       "      <td>138349.0</td>\n",
       "      <td>53028884.0</td>\n",
       "      <td>158884.0</td>\n",
       "      <td>1.002358</td>\n",
       "      <td>0.009983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>kim_cnn_5000_limit_10_epochs</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.397833</td>\n",
       "      <td>0.786248</td>\n",
       "      <td>0.548924</td>\n",
       "      <td>0.323247</td>\n",
       "      <td>0.977590</td>\n",
       "      <td>224226.0</td>\n",
       "      <td>60959.0</td>\n",
       "      <td>53106280.0</td>\n",
       "      <td>184257.0</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>0.006862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>kim_cnn_5000_limit_10_epochs_save_best</td>\n",
       "      <td>0.014030</td>\n",
       "      <td>0.399011</td>\n",
       "      <td>0.794837</td>\n",
       "      <td>0.522090</td>\n",
       "      <td>0.315110</td>\n",
       "      <td>0.975943</td>\n",
       "      <td>213265.0</td>\n",
       "      <td>55048.0</td>\n",
       "      <td>53112236.0</td>\n",
       "      <td>195218.0</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.006946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>simple_5000_limit_10_epochs_save_best</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>0.380396</td>\n",
       "      <td>0.490458</td>\n",
       "      <td>0.749843</td>\n",
       "      <td>0.296514</td>\n",
       "      <td>0.985702</td>\n",
       "      <td>306298.0</td>\n",
       "      <td>318216.0</td>\n",
       "      <td>52849048.0</td>\n",
       "      <td>102185.0</td>\n",
       "      <td>1.008430</td>\n",
       "      <td>0.016055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.510684</td>\n",
       "      <td>0.725247</td>\n",
       "      <td>0.299671</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>296251.0</td>\n",
       "      <td>283855.0</td>\n",
       "      <td>52883420.0</td>\n",
       "      <td>112232.0</td>\n",
       "      <td>1.007342</td>\n",
       "      <td>0.014967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "      <td>kim_cnn_limit_10_epochs_save_best_2</td>\n",
       "      <td>0.024950</td>\n",
       "      <td>0.383484</td>\n",
       "      <td>0.489399</td>\n",
       "      <td>0.729152</td>\n",
       "      <td>0.292845</td>\n",
       "      <td>0.984181</td>\n",
       "      <td>297846.0</td>\n",
       "      <td>310749.0</td>\n",
       "      <td>52856432.0</td>\n",
       "      <td>110637.0</td>\n",
       "      <td>1.008518</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46</td>\n",
       "      <td>kim_cnn_2000</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.313940</td>\n",
       "      <td>0.390715</td>\n",
       "      <td>0.718410</td>\n",
       "      <td>0.253076</td>\n",
       "      <td>0.984066</td>\n",
       "      <td>313114.0</td>\n",
       "      <td>488274.0</td>\n",
       "      <td>120811464.0</td>\n",
       "      <td>122729.0</td>\n",
       "      <td>1.008532</td>\n",
       "      <td>0.012112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence                                    name      loss  \\\n",
       "0         35                         kim_cnn_complex  0.025429   \n",
       "1         35                         kim_cnn_complex  0.025429   \n",
       "2         35                         kim_cnn_complex  0.025429   \n",
       "3         36                               basic_cnn  0.028156   \n",
       "4         37                    basic_cnn_5000_limit  0.016947   \n",
       "5         38          basic_cnn_5000_limit_10_epochs  0.017176   \n",
       "6         40            kim_cnn_5000_limit_10_epochs  0.013641   \n",
       "7         41  kim_cnn_5000_limit_10_epochs_save_best  0.014030   \n",
       "8         42   simple_5000_limit_10_epochs_save_best  0.024671   \n",
       "9         44       kim_cnn_limit_10_epochs_save_best  0.023322   \n",
       "10        45     kim_cnn_limit_10_epochs_save_best_2  0.024950   \n",
       "11        46                            kim_cnn_2000  0.017648   \n",
       "\n",
       "    categorical_accuracy  precision    recall        f1       auc  \\\n",
       "0               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "1               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "2               0.420201   0.785050  0.561118  0.327229  0.977318   \n",
       "3               0.421450   0.677476  0.659971  0.334304  0.982044   \n",
       "4               0.380762   0.661913  0.594659  0.313243  0.978033   \n",
       "5               0.381195   0.643383  0.611039  0.313397  0.979239   \n",
       "6               0.397833   0.786248  0.548924  0.323247  0.977590   \n",
       "7               0.399011   0.794837  0.522090  0.315110  0.975943   \n",
       "8               0.380396   0.490458  0.749843  0.296514  0.985702   \n",
       "9               0.385321   0.510684  0.725247  0.299671  0.984288   \n",
       "10              0.383484   0.489399  0.729152  0.292845  0.984181   \n",
       "11              0.313940   0.390715  0.718410  0.253076  0.984066   \n",
       "\n",
       "    true_positives  false_positives  true_negatives  false_negatives  \\\n",
       "0         178463.0          48864.0      21137114.0         139586.0   \n",
       "1         178463.0          48864.0      21137114.0         139586.0   \n",
       "2         178463.0          48864.0      21137114.0         139586.0   \n",
       "3         209903.0          99928.0      21086010.0         108146.0   \n",
       "4         242908.0         124071.0      53043088.0         165575.0   \n",
       "5         249599.0         138349.0      53028884.0         158884.0   \n",
       "6         224226.0          60959.0      53106280.0         184257.0   \n",
       "7         213265.0          55048.0      53112236.0         195218.0   \n",
       "8         306298.0         318216.0      52849048.0         102185.0   \n",
       "9         296251.0         283855.0      52883420.0         112232.0   \n",
       "10        297846.0         310749.0      52856432.0         110637.0   \n",
       "11        313114.0         488274.0     120811464.0         122729.0   \n",
       "\n",
       "       hinge  mean_absolute_error  \n",
       "0   0.998525             0.013316  \n",
       "1   0.998525             0.013316  \n",
       "2   0.998525             0.013316  \n",
       "3   1.001925             0.016715  \n",
       "4   1.002064             0.009689  \n",
       "5   1.002358             0.009983  \n",
       "6   0.999238             0.006862  \n",
       "7   0.999323             0.006946  \n",
       "8   1.008430             0.016055  \n",
       "9   1.007342             0.014967  \n",
       "10  1.008518             0.016143  \n",
       "11  1.008532             0.012112  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = pd.DataFrame(simple_log)\n",
    "log_df['f1'] = (log_df['precision'] * log_df['recall']) / (log_df['precision'] + log_df['recall'])\n",
    "\n",
    "log_df[[\n",
    "    'sequence',\n",
    "    'name',\n",
    "    'loss',\n",
    "    'categorical_accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'f1',\n",
    "    'auc',\n",
    "    'true_positives',\n",
    "    'false_positives',\n",
    "    'true_negatives',\n",
    "    'false_negatives',\n",
    "    'hinge',\n",
    "    'mean_absolute_error',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = tokenizer.sequences_to_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tags = []\n",
    "for row in y_test.tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col == 1]\n",
    "    y_test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the threshold for classification\n",
    "\n",
    "This lets us see how well the model generalizes to labeling more classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_THRESHOLD = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > CLASSIFY_THRESHOLD) * 1\n",
    "\n",
    "y_pred_tags = []\n",
    "for row in y_pred.tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col > CLASSIFY_THRESHOLD]\n",
    "    y_pred_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  190, -3649,   310,   129,   228,    84,   155,   127,    92,\n",
       "         233,   503,  -351,    81,  -153,    55,    99,   -24, -2473,\n",
       "         793,   827,   173,   422,   738,  1243,   261,   948,  1331,\n",
       "         629,   474,   153,   531,   585,    59,   624,  -124,   146,\n",
       "         218,    62,    36,    80,     8,    20,     2,   100,  -446,\n",
       "         444,  -251,    84,   460,   172,   567,  -286,   178, -1255,\n",
       "         115,   108,  -647,   396,   162,    88,   134,    64,   333,\n",
       "         240,  -166,   117,   162,  -398,    88,   800,  -106,   233,\n",
       "           8,    51,    23,   281,  -178,   154,   296,    46,   -94,\n",
       "         293,   101,    28,   263,   517,     0,  -320,   384,   266,\n",
       "       -6893,  -364, -2516,    78,   252,   198,  -107,    -1,   146,\n",
       "          10,   172,   149,     9,   154,   216,   321,   171,   -12,\n",
       "         198,   187,   -65,     8,    63,    48,   346,   120,   717,\n",
       "         -25,  -149,     3,   -26,   191,   219,    41,    82,   -60,\n",
       "         140,   -85,  -312,  -139,  -276,   135,    58,   317,     0,\n",
       "        -311,   170,    86,   313,   -32,     2,    85,    38,   156,\n",
       "          58,   167,   -29,    45,   584,   157,  -251,   534,   616,\n",
       "          12,    73,    53,    57,    19,  -151,     9,   232, -1078,\n",
       "          22,   268,    39,    37,    13,   119,   198,    68,   107,\n",
       "        -393,   226,   105,    63,   -63,    15,  -189,  -176,    84,\n",
       "         130,     0,    69,  -112,   216,   235,   271,    80,   160,\n",
       "         -49,    60,   -17,    42,   187,   151,   155,   -12,    77,\n",
       "         227,   128,   270,    -4,   173,  -199,   201,   167,    47,\n",
       "         278,    16,   179,     9,   178,    87,   116,    27,     7,\n",
       "           8,    64,   -74,   228,    66,    46,    -4,  -179,   176,\n",
       "         -75,  -268,    -2,   144,  -258,     8,   204,    73,    24,\n",
       "         171,   178,    61,  -591,  -198,   112,   -93,   -30,    -1,\n",
       "          80,   217,   100,    51,    97,   488,   166,   148,    58,\n",
       "          64,    40,   -53,   -27,    55,   100,   -64,    42,  -252,\n",
       "        -135,   456,    38,   156,    90,   237,   127,   109,   136,\n",
       "          61,   137,   -49,    12,   401,    25,    -4,   -12,   128,\n",
       "          33,   159,   132,    -9,    36,   106,   184,   180,    36,\n",
       "          49,   168,    22,   -73,  -205,   173,    19,    39,   183,\n",
       "          39,    13,     5,    50,    39,   175,    -4,   -35,    41,\n",
       "         237,    45,    36, -1039,   181,  -285,  -289,   101,    85,\n",
       "         -22,   253,   -82,    92,    84,  -123,   -27,  -265,   106,\n",
       "           6,    82,     1,    39,     1,    78,   -53,    65,  -110,\n",
       "         944,   177,   -11,    64,    85,    20,    82,  -183,    91,\n",
       "          93,  -665,   399,   292,   635,   195,   206,   -87,   391,\n",
       "          35,    32,   272,   263,   159, -1006,   -22,  -171,   211,\n",
       "         -12, -2257,    90,   188,    55,   -39,    28,    77,    78,\n",
       "         188,   113,  -471,   104,   -91,   378,   215,   118,   228,\n",
       "        -211,   115,   144,    19,   185,   -24,     2,    58,   214,\n",
       "          76,    19,   153,    13,  -133,  -273,   -39,    63,   109,\n",
       "          60,    83,   179,    78,   120,   -20,  -223,   184,   249,\n",
       "        -964,   237,   210,   -69,   169,    -7,   -10,    60,   -94,\n",
       "          76,    -6,    -7,    71,    38,  -248,    37,    19,     7,\n",
       "         -45,    30,    93,    47,    67,   140,    91,   146,  -408,\n",
       "        -101,  -202,    87,    51,    86,    44,  -209,   183,    12,\n",
       "         -56,    96,   142,   218,  -117,  -218,    -7,   -51,   122,\n",
       "          48,   -69,   148,   132,   115,    17,    91,  -647,    10,\n",
       "        -458,   116,    34,    -1,  -116,    48,    39,  -147,   187,\n",
       "          59,     5,     5,  -268,   -52,    26,    29,   126,    -5,\n",
       "          87,    40,    58,   -82,   -93,  -311,   531,    70,  -159,\n",
       "          83,   119,    21,    62,   119,    69,   114,  -371,   -43,\n",
       "          16,  -128,    24,   100,   -73,    90,   -30,    73,   -96,\n",
       "        -129,    75,  -265,   159,   -56,   -33,    42,   124,     3,\n",
       "       -1151,   -29,   -56,   398,    73,   212,    79,    69,   130,\n",
       "          77,   143,  -267,   345,   -80,   -55,  -103,   -27,   156,\n",
       "         114,   104,   -34,   -62,    53,   123,    32,  -183,    30,\n",
       "         -21,   192,   144,     2,   382,    53,  -480,  -674,  -746,\n",
       "         124,   -45,    43,   319,     0,     6,    92,    67,   144,\n",
       "          88,   -39,    80,    18,    13,  -113,  -372,  -137,  -193,\n",
       "          27,    82,  -144,  -141,   -10,    40,  -171,    89,   -81,\n",
       "         104,    76,    13,    88,   -16,   163,  -127,   -94,    22,\n",
       "         589,    46,    40,    20,    53,    52,   120,   -19,    65,\n",
       "         -74,    12,  -176,   149,  -158,  -307,  -520,   179,   -30,\n",
       "          28,    43,  -202,  -237,    52,   -20,  -280,    27,  -114,\n",
       "           5,   -28,     5,    16,    50,    -7,    -5,     4,   -17,\n",
       "           3,   -90,   -60,    18,   101,    23,   495,  -115,  -327,\n",
       "           1,   -31,    23,   -27,    -3,   -51,    26,     0,    29,\n",
       "         152,     3,   109,  -247,   390,   102,   190,  -184,   -50,\n",
       "         137,    57,    23,    65,    54,  -331,   191,    -4,   322,\n",
       "          62,    74,  -130,  -302,    36,   -37,   181,    17,   -90,\n",
       "        -214,    -9,    64,  -521,  -245,    68,     8,   -93,   118,\n",
       "         -67,   -95,   -26,    14,    30,    26,   163,   -39,     3,\n",
       "          78,   344,    66,    15,    26,    98,   145,  -205,  -158,\n",
       "          43,  -178,   134,   158,   383,    54,   348,   194,   176,\n",
       "         624,   425,   139,   -47,    -4,   -15,    33,   -71,  -133,\n",
       "         225,    67,  -339,    48,    26,    10,  -107,  -192,  -227,\n",
       "         121,   222,    85,   -32,    62,    42,  -226,    22,   410,\n",
       "        -424,    15,     4,   -83,   -50,    -8,    42,    85,   -12,\n",
       "          69,    21,  -385,  -367,    42,    11,  -214,   129,   -88,\n",
       "         -37,   -23,   -21,    71,    99,   169,  -405,    42,    11,\n",
       "          88,    94,   151,   -49,   135,    55,    99,    95,    48,\n",
       "          73,   264,   355,    64,   239,   121,    44,    93,  -273,\n",
       "          16,    99,  -423,   -19,   133,    81,    65,     3,    85,\n",
       "         106,    19,    -5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(y_pred, 0).sum(axis=0) - y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('using variables gulp destination file name i new gulp i wondering i want achieve practical possible my projects structure now i want gulp stores compiled css files according css folder public assets uses name folder found scss file is possible do i need pipe plugin thanks ps realize i could achieve renaming scss i like avoid pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['gulp', 'sass'],\n",
       "  ['css', 'gulp', 'javascript', 'sass']),\n",
       " ('woocommerce get custom select fields value thank page i using woocommerce plugin want add custom select field checkout page i added function hook function php page it displaying checkout page but get value field thankyou php page i tried get points to go field value way but work i values derive side value pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['php', 'woocommerce', 'wordpress'],\n",
       "  ['php', 'woocommerce', 'wordpress']),\n",
       " ('where i store photos file system database possible duplicate storing uploaded photos documents filesystem vs database blob i starting develop web app primary purpose display photos the users able upload photos well the first question came store photos file system database i using windows box host site the database mysql backend code c utilizing asp net mvc pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['asp.net-mvc', 'asp.net-mvc-2', 'asp.net-mvc-3', 'c#'],\n",
       "  ['architecture']),\n",
       " ('trying get aes encryption string node js match encrypted value net i trying encrypt value node js i decrypt net i given code use net side things encrypting value trying achieve encrypted value node js script i definitely encryption please help figure going wrong my node js encrypted value matching net encrypted value node js encrypted value actually returning value every time i run script either here net encryption logic my node js script when running encryption piece c code value looks like slightly modified security purposes how i change node js code output matched encrypted value output node js script pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['cryptography', 'encryption', 'javascript', 'node.js'],\n",
       "  ['c#', 'cryptography', 'encryption', 'javascript', 'node.js']),\n",
       " ('why need using also difference appbar layout collapsing toolbar layout thanks lot pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['android-layout', 'autolayout', 'dart', 'flutter', 'layout'],\n",
       "  ['android', 'android-layout']),\n",
       " ('is way detect users facebook api a site i working significant problems fake users facebook openid is api call reports internal facebook calculations user account pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['facebook', 'facebook-graph-api'],\n",
       "  ['facebook', 'facebook-graph-api']),\n",
       " ('how i handle window close event tkinter how i handle window close event user clicking x button python tkinter program pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['button', 'python', 'tkinter'],\n",
       "  ['events', 'python', 'tkinter']),\n",
       " ('argument type unsigned int incompatible parameter type size t i code cuda c i got error compiling line memory free memory total the errors line i clue error idea pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['c', 'cuda'],\n",
       "  ['c++', 'cuda']),\n",
       " ('visual studio running test different projects amd modules i two projects solution one main web project say serves testing purposes say i want javascript headless tests running second one on first project javascript files directory like at test project i json file root the file configuration but i try run spec file i get error spec file the error the thing tries find jquery module test project rather main project resides why i getting kind behavior i solve please i trying hours tackle luck far note the names used clarity rather using real names pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  [],\n",
       "  ['javascript', 'requirejs', 'visual-studio-2013']),\n",
       " ('cant access specific memory address c program i working book hacking art of exercise c page 61 here code below question gdb output i posted gdb output sure much would need my problem lies bottom gdb output run x gdb get error on book jon able access error part memory what missing why access three addresses pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['c', 'gdb'],\n",
       "  ['c', 'gcc', 'gdb', 'linux', 'memory']),\n",
       " ('openssl verify peer client certificate c i working application ssl connection server the server uses self signed certificate client loads certificate authority chain tell server ok trust i code like client and server i trying modify code server also verifies client peer certificate self signed using issuer server bit trouble i found good conceptual overview documentation anywhere seems typical openssl libraries on client i added ssl ctx load verify locations call on server i added ssl ctx use file call the connection fails certificate validate the client seems load certificate fine i comment ssl ctx set verify line client connects without trouble certificate never verified it seems server think client certificate authority chain good what i missing from commandline i run openssl verify ca crt generic client pem and passes i right certificate data available i must using wrong somehow pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['certificate', 'openssl', 'ssl'],\n",
       "  ['c++', 'certificate', 'openssl', 'ssl']),\n",
       " ('the replacement token description value trying create publish nuget package project csproj via nuget exe i got following warning the replacement token description value how i get rid warning pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['msbuild', 'nuget'],\n",
       "  ['.net', 'c#', 'nuget']),\n",
       " ('how replace mysql mariadb wamp i wamp server installed windows works perfectly this article inspired migrate mysql mariadb question states i uninstall mysql install mariadb work fine however windows uninstall programs section control panel list something like mysql 2 2 consequently replace mysql windows install php apache mariadb seperately pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['mysql', 'php'],\n",
       "  ['database', 'php']),\n",
       " ('what preferred way use auto declare lambda variable c does anyone insight pros cons various ways use auto declare lambda variable for e g can anyone think reason always prefer 1 i imagine template function preferred way declare callable parameter use universal reference or guidelines prefer declare non reference parameter thank pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['c#', 'c++', 'c++11', 'lambda'],\n",
       "  ['c++', 'c++11', 'lambda']),\n",
       " ('spring data neo4j 5 includes companion objects using kotlin i using spring data 5 via spring boot 2 0 0 m4 i noticed using providing package includes scanning inner classes generated kotlin in example i one property i also tried still errors is anyway filter internal classes kotlin generates here exception i getting full stacktrace i created sample project help pom xml model store domain web application yml pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['kotlin', 'neo4j', 'spring-boot', 'spring-data'],\n",
       "  ['kotlin', 'neo4j', 'spring-data']),\n",
       " ('draw bitmap using opengl es android i android application drawing 800 480 image screen my phone also 800 480 px the phone able drawing 25 ms every seconds want transition another image also 800 480 fading alpha first image increasing alpha second image while drawable draw canvas call takes 75 ms what reduce drawing time transition phase i considered using opengl cannot figure pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['android', 'bitmap', 'canvas', 'opengl-es'],\n",
       "  ['android', 'opengl-es']),\n",
       " ('how run selenium system tests without requiring open browser i test method created using selenium something similar when test runs opens ie carries test imagine 200 test methods like spread across multiple test fixtures means ie opened closed many times many test fixtures since 1 browser opened per test fixture how run selenium system tests without requiring open browser i mean example i thinking might possible develop windows service run selenium tests winforms web browser control case browser opened time tests run automatically not sure implement though or better known way thanks pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['internet-explorer', 'nunit', 'selenium', 'selenium-webdriver'],\n",
       "  ['selenium', 'testing', 'windows-services']),\n",
       " ('access db table endpoint using odata i learning db walking following tutorial microsoft db documentation site unfortunately i stuck point i trying access entities i created anytime i run get request receive odata response i receive following error message i endpoint looks like note i replace actual account i also tried using start odata queries according tutorial i create database table add three entities i successfully using c table api method since i successfully create table entities using table api i know endpoint i using accurate made sure i copied pasted element connection string i know matches database azure here database looks like azure next tutorial run query table endpoint here i start getting confused i cannot find documentation explains get table endpoint here example tutorial shows so i figure try that work i figure maybe i also need specify database see picture that work either as mentioned earlier post i try appending end test queries error response still right i trying postman gets also i try getting token like postman and sending get request using parameter passing token still get error listed do know i properly construct table endpoint successfully send request receive odata pad pad pad pad pad',\n",
       "  ['azure'],\n",
       "  ['azure', 'rest']),\n",
       " ('comparing incrementing series values with php i series numbers always increment logically numbers actually representations version numbers database so example sample data 1 0 1 0 1 2 0 0 2 0 1 using php i know i compare values determine 2 0 1 less 2 0 3 example way pr determining jumping 2 0 1 2 0 3 breaks series skips 2 0 2 pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['comparison', 'numbers', 'php'],\n",
       "  ['comparison', 'mysql', 'php']),\n",
       " ('what algorithm implemented opencv contour detection does anyone know algorithm implemented opencv library perform contour detection can possibly give reference thanks pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad',\n",
       "  ['computer-vision', 'image-processing', 'opencv'],\n",
       "  ['computer-vision', 'image-processing', 'opencv'])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "viz_keys = ['val_categorical_accuracy', 'val_precision', 'val_recall']\n",
    "# summarize history for accuracy\n",
    "for key in viz_keys:\n",
    "    plt.plot(history.history[key])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(viz_keys, loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## Model imports\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import ( Input, Embedding, GlobalMaxPooling1D, Conv1D, Dense, Activation, \n",
    "                                      Dropout, Lambda, BatchNormalization, concatenate )\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Fit imports\n",
    "from tensorflow.keras.losses import hinge, mae, binary_crossentropy, kld, Huber, squared_hinge\n",
    "\n",
    "# Hyperparameter/method search space\n",
    "import itertools\n",
    "\n",
    "# For 4 GPUs\n",
    "DIST_BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "print('Starting experiment loop...')\n",
    "\n",
    "EXPERIMENT_NAME = 'kld dense dims x filter_lengths x adam/sgd'\n",
    "learning_rates = [None]\n",
    "losses = [binary_crossentropy]#, kld, hinge, mae]\n",
    "activations = ['selu'] # 'selu'\n",
    "optimizers = ['adam']\n",
    "dropout_ratios = [0.2]\n",
    "filter_lengths = [300]\n",
    "class_weight_set = [train_class_weights]\n",
    "sample_weight_set = [None], #, train_sample_weights]\n",
    "test_sample_weight_set = [None] #, test_sample_weights]\n",
    "dense_dims = [32]\n",
    "\n",
    "args = list(itertools.product(\n",
    "    learning_rates,\n",
    "    losses,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    dropout_ratios,\n",
    "    filter_lengths,\n",
    "    class_weight_set,\n",
    "    sample_weight_set,\n",
    "    test_sample_weight_set,\n",
    "    dense_dims\n",
    "))\n",
    "print()\n",
    "print(f'{len(args):,} total iterations...')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases Monitoring\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"weakly-supervised-learning\", name=EXPERIMENT_NAME)\n",
    "config = wandb.config\n",
    "\n",
    "# tqdm_notebook\n",
    "for learning_rate, loss_function, activation, optimizer, dropout_ratio, filter_length, class_weights, \\\n",
    "    sample_weights, test_sample_weights, dense_dim in args:\n",
    "    \n",
    "    cw_label  = 'class_weights' if isinstance(class_weights, dict) else 'no_class_weights'\n",
    "    sw_label  = 'sample_weights' if isinstance(sample_weights, np.ndarray) else 'no_sample_weights'\n",
    "    tsw_label = 'test_sample_weights' if isinstance(test_sample_weights, list) else 'no_test_sample_weights'\n",
    "    \n",
    "    model_name = str(loss_function.__name__) + ' ' + str(learning_rate) + ' ' + str(optimizer) + ' ' + \\\n",
    "                 str(activation) + ' ' + str(EPOCHS) + ' ' + cw_label + ' ' + sw_label + ' ' + tsw_label + ' ' + \\\n",
    "                 str(dense_dim)\n",
    "    \n",
    "    # Log wandb config\n",
    "    config.update(\n",
    "        {\n",
    "            'class_weights': cw_label,\n",
    "            'sample_weights': sw_label,\n",
    "            'test_sample_weights': tsw_label,\n",
    "        },\n",
    "        allow_val_change=True\n",
    "    )\n",
    "    \n",
    "    print(model_name)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #\n",
    "    # Build ze model...\n",
    "    #\n",
    "    def build_model(\n",
    "        token_count=None,\n",
    "        max_words=None,\n",
    "        embedding_dim=None,\n",
    "        label_count=None,\n",
    "        dropout_ratio=None,\n",
    "        filter_length=None,\n",
    "        loss_function=None,\n",
    "        learning_rate=None,\n",
    "        optimizer=None,\n",
    "        activation=None,\n",
    "        dense_dim=None,\n",
    "    ):\n",
    "        \"\"\"Build the model using this experiment's parameters\"\"\"\n",
    "        \n",
    "        # Store config in wandb\n",
    "        config.update(\n",
    "            {\n",
    "                'token_count': token_count,\n",
    "                'max_words': max_words,\n",
    "                'embedding_dim': embedding_dim,\n",
    "                'label_count': label_count,\n",
    "                'dropout_ratio': dropout_ratio,\n",
    "                'filter_length': filter_length,\n",
    "                'loss_function': loss_function.__name__,\n",
    "                'learning_rate': learning_rate,\n",
    "                'optimizer': optimizer,\n",
    "                'activation': activation,\n",
    "                'dense_dim': dense_dim,\n",
    "            },\n",
    "            allow_val_change=True\n",
    "        )\n",
    "        \n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])#, \"/gpu:1\", \"/gpu:2\", \"/gpu:3\"])\n",
    "        with mirrored_strategy.scope():\n",
    "            \n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        \n",
    "            hashed_input = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "\n",
    "            # emb = Embedding(token_count, embedding_dim, weights=[embedding_matrix])(hashed_input)\n",
    "            emb = Embedding(max_words, 20, input_length=maxlen)(hashed_input)\n",
    "\n",
    "#             # Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "#             conv1_1 = Conv1D(filters=filter_length, kernel_size=3)(emb)\n",
    "#             btch1_1 = BatchNormalization()(conv1_1)\n",
    "#             drp1_1  = Dropout(dropout_ratio)(btch1_1)\n",
    "#             actv1_1 = Activation(activation)(drp1_1)\n",
    "#             glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "#             conv1_2 = Conv1D(filters=filter_length, kernel_size=4)(emb)\n",
    "#             btch1_2 = BatchNormalization()(conv1_2)\n",
    "#             drp1_2  = Dropout(dropout_ratio)(btch1_2)\n",
    "#             actv1_2 = Activation(activation)(drp1_2)\n",
    "#             glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "#             conv1_3 = Conv1D(filters=filter_length, kernel_size=5)(emb)\n",
    "#             btch1_3 = BatchNormalization()(conv1_3)\n",
    "#             drp1_3  = Dropout(dropout_ratio)(btch1_3)\n",
    "#             actv1_3 = Activation(activation)(drp1_3)\n",
    "#             glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "#             conv1_4 = Conv1D(filters=filter_length, kernel_size=6)(emb)\n",
    "#             btch1_4 = BatchNormalization()(conv1_4)\n",
    "#             drp1_4  = Dropout(dropout_ratio)(btch1_4)\n",
    "#             actv1_4 = Activation(activation)(drp1_4)\n",
    "#             glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "#             # Gather all convolution layers\n",
    "#             cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "#             drp1 = Dropout(dropout_ratio)(cnct)\n",
    "\n",
    "#             dns1  = Dense(dense_dim, activation=activation)(drp1)\n",
    "#             btch1 = BatchNormalization()(dns1)\n",
    "#             drp2  = Dropout(dropout_ratio)(btch1)\n",
    "            \n",
    "            drp1 = Dropout(0.1)(emb)\n",
    "            conv1 = Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1)(drp1)\n",
    "            gmp1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "            out = Dense(y_train.shape[1], activation='sigmoid')(gmp1)\n",
    "\n",
    "            text_model = Model(\n",
    "                inputs=hashed_input, \n",
    "                outputs=out\n",
    "            )\n",
    "\n",
    "            if activation == 'adam':\n",
    "                activation = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "            if activation == 'sgd':\n",
    "                activation = SGD(lr=learning_rate)\n",
    "\n",
    "            text_model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function,\n",
    "                metrics=[\n",
    "                    'categorical_accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    tf.keras.metrics.BinaryAccuracy(),\n",
    "                    tf.keras.metrics.Hinge(),\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.Accuracy(),\n",
    "                    tf.keras.metrics.MeanAbsoluteError(),\n",
    "                    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                    tf.keras.metrics.TruePositives(),\n",
    "                    tf.keras.metrics.FalsePositives(),\n",
    "                    tf.keras.metrics.TrueNegatives(),\n",
    "                    tf.keras.metrics.FalseNegatives()\n",
    "                ]\n",
    "            )\n",
    "            #text_model.summary()\n",
    "\n",
    "            return text_model\n",
    "\n",
    "    #\n",
    "    # Train ze model...\n",
    "    #\n",
    "    def train_model(\n",
    "        model=None,\n",
    "        X_train=None,\n",
    "        X_test=None,\n",
    "        learning_rate=None,\n",
    "        loss_function=None,\n",
    "        optimizer=None,\n",
    "        activation=None,\n",
    "        epochs=None,\n",
    "        class_weights=None,\n",
    "        sample_weights=None,\n",
    "        test_sample_weights=None,\n",
    "    ):\n",
    "        \"\"\"Train the model using the current parameters and evaluate performance\"\"\"\n",
    "        \n",
    "        # Log wandb config\n",
    "        config.update(\n",
    "            { 'epochs': epochs },\n",
    "            allow_val_change=True,\n",
    "        )\n",
    "        \n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(\n",
    "                patience=1,\n",
    "                verbose=1,\n",
    "                min_delta=0.001,\n",
    "                min_lr=0.0005,\n",
    "            ), \n",
    "            EarlyStopping(\n",
    "                patience=2,\n",
    "                min_delta=0.001,\n",
    "                verbose=1,\n",
    "                restore_best_weights=True\n",
    "            ), \n",
    "            WandbCallback()\n",
    "            #ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = text_model.fit(\n",
    "            X_train, \n",
    "            y_train,\n",
    "            class_weight=class_weights,\n",
    "            sample_weight=sample_weights,\n",
    "            epochs=epochs,\n",
    "            batch_size=DIST_BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "        # Evaluate to our log and return a description key and a list of metrics\n",
    "        accr = text_model.evaluate(X_test, y_test, sample_weight=test_sample_weights)\n",
    "        f1_score = 2.0 * (accr[1] * accr[2]) / \\\n",
    "                         (accr[1] + accr[2])\n",
    "        return_val = [i for i in zip([j.item() for j in accr + [f1_score]], text_model.metrics_names + ['val_f1_score'])]\n",
    "\n",
    "        return return_val\n",
    "\n",
    "    #\n",
    "    # main()\n",
    "    #\n",
    "    text_model = build_model(\n",
    "        token_count=TOKEN_COUNT,\n",
    "        max_words=100,\n",
    "        embedding_dim=50,\n",
    "        label_count=y_train.shape[1],\n",
    "        filter_length=128,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        dropout_ratio=dropout_ratio,\n",
    "        dense_dim=dense_dim,\n",
    "    )\n",
    "    try:\n",
    "        accuracies = train_model(\n",
    "            model=text_model,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            activation=activation,\n",
    "            epochs=EPOCHS,\n",
    "            class_weights=class_weights,\n",
    "            sample_weights=sample_weights,\n",
    "            test_sample_weights=test_sample_weights,\n",
    "        )\n",
    "\n",
    "        log_record = (model_name, accuracies)\n",
    "        performance_log.append(log_record)\n",
    "\n",
    "        with open('data/performance_log.jsonl', 'w') as f:\n",
    "            for record in performance_log:\n",
    "                f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "        print(log_record)\n",
    "        sys.stdout.flush()\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Aboring training run!')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print('Completed experiment loop!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EPOCHS = 4\n",
    "\n",
    "# history = text_model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='loss', patience=1, min_delta=0.0001),\n",
    "#         EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001),\n",
    "#     ],\n",
    "#     class_weight=class_weights,\n",
    "#     # sample_weight=train_sample_weights,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = text_model.evaluate(X_test, y_test) #, sample_weight=test_sample_weights)\n",
    "[i for i in zip(accr, text_model.metrics_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(history.history)\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_loss'])\n",
    "#plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['precision'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_loss', 'f1', 'categorical accuracy', 'MAE', 'precision'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, statistics\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "trim_size = math.floor(X_test.shape[0] / 64) * 64\n",
    "print(f'trim size: {trim_size:,}')\n",
    "\n",
    "X_test_trim = X_test[:trim_size]\n",
    "y_test_trim = y_test[:trim_size]\n",
    "\n",
    "y_pred = model.predict(X_test_trim)\n",
    "\n",
    "best_cutoff = 0\n",
    "max_score = 0\n",
    "for cutoff in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    y_pred_bin = np.greater(y_pred, cutoff)\n",
    "    print('Cutoff: {:,}'.format(cutoff))\n",
    "    print('Hamming loss: {:,}'.format(\n",
    "        hamming_loss(y_test_trim, y_pred_bin)\n",
    "    ))\n",
    "    scores = []\n",
    "    for j_type in ['micro', 'macro', 'weighted']:\n",
    "        j_score = jaccard_score(y_test_trim, y_pred_bin, average=j_type)\n",
    "        print('Jaccard {} score: {:,}'.format(\n",
    "            j_type,\n",
    "            j_score\n",
    "        ))\n",
    "        scores.append(j_score)\n",
    "    print('')\n",
    "    mean_score = statistics.mean(scores)\n",
    "    if mean_score > max_score:\n",
    "        best_cutoff = cutoff\n",
    "        max_score = mean_score\n",
    "\n",
    "print('Best cutoff was: {:,} with mean jaccard score of {:,}'.format(best_cutoff, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "y_pred = text_model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred_bool = np.where(y_pred > best_cutoff, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "print(multilabel_confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "\n",
    "Now lets map from the one-hot-encoded tags back to the text tags and view them alongside the text of the original posts to sanity check the model and see if it really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "for test, pred in zip(y_test, y_pred_bool):\n",
    "    tags = []\n",
    "    for i, val in enumerate(test):\n",
    "        if pred[i] == 1.0:\n",
    "            tags.append(sorted_all_tags[i])\n",
    "    predicted_tags.append(tags)\n",
    "\n",
    "for text, tags in zip(X_test, predicted_tags):\n",
    "    print(' '.join(text), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
