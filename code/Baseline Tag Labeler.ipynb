{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Baseline Tag Labeler\n",
    "\n",
    "Here we will use [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html) and [scikit-learn](https://scikit-learn.org/stable/) to create a baseline multi-class, multi-label classifier that will label our sample of [Stack Overflow](http://stackoverflow.com) posts (questions and their answers), two thirds of which lack labels. This will serve as a basis of comparison for the deep network we will train therafter. We will create separate [embeddings](https://keras.io/layers/embeddings/) of their language and code and use these as the signal for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our sample of 100K questions/answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original posts: 273,876\n"
     ]
    }
   ],
   "source": [
    "posts_df = pd.read_json('data/stackoverflow/sample/CombinedDocs.100K.Questions.jsonl.gz', lines=True)\n",
    "print('Original posts: {:,}'.format(len(posts_df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop posts without tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts w/ tags: 98,278\n"
     ]
    }
   ],
   "source": [
    "tag_posts = posts_df.dropna(axis=0, subset=['_Tags'])\n",
    "print('Posts w/ tags: {:,}'.format(len(tag_posts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip tag names from their XML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tag_posts['_Tags'].apply(lambda x: re.findall('\\<(.+?)\\>', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which tags to predict\n",
    "\n",
    "We choose to limit tags to those with at least 50 occurrences, which means 709 labels to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18,549 tags with more than 0 count\n",
      "There are 2,730 tags with more than 10 count\n",
      "There are 1,589 tags with more than 20 count\n",
      "There are 709 tags with more than 50 count\n",
      "There are 367 tags with more than 100 count\n",
      "There are 33 tags with more than 1,000 count\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# import seaborn as sns\n",
    "\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for row in tags:\n",
    "    for tag in row:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "for i in [0, 10, 20, 50, 100, 1000]:\n",
    "    filtered_tags = list(filter(lambda x: x > i, tag_counts.values()))\n",
    "    print('There are {:,} tags with more than {:,} count'.format(len(filtered_tags), i))\n",
    "    # t = pd.Series(filtered_tags, name=\"Tag Count\")\n",
    "    # ax = sns.distplot(t)\n",
    "\n",
    "MIN_TAGS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute maps between tag ID and labels, and a list of labels with > 50 instances\n",
    "\n",
    "These will be used when presenting results and their tag-wise performance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags with 50 occurrences: 709\n"
     ]
    }
   ],
   "source": [
    "all_tags = set()\n",
    "for row in tags:\n",
    "    for tag in row:\n",
    "        if tag_counts[tag] > MIN_TAGS:\n",
    "            all_tags.add(tag)\n",
    "print('Total unique tags with {:,} occurrences: {:,}'.format(MIN_TAGS, len(all_tags)))\n",
    "sorted_all_tags = sorted(all_tags)\n",
    "\n",
    "tag_to_id = {val:i for i, val in enumerate(sorted_all_tags)}\n",
    "id_to_tag = {i:val for i, val in enumerate(sorted_all_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matrix of tags 709 elements wide, one for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98278, 709)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "tag_list = tags.tolist()\n",
    "# Loop through every post...\n",
    "for i, tag_set in enumerate(tags.tolist()):\n",
    "    # Then build a 709 element wide list for each tag present\n",
    "    label_row = []\n",
    "    for tag in sorted_all_tags:\n",
    "        if tag in tag_list[i]:\n",
    "            label_row.append(1)\n",
    "        else:\n",
    "            label_row.append(0)\n",
    "    labels.append(label_row)\n",
    "    \n",
    "tag_labels = [id_to_tag[key_id] for key_id in sorted(id_to_tag.keys()) if tag_counts[id_to_tag[key_id]] > MIN_TAGS]\n",
    "\n",
    "len(labels), len(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `BeautifulSoup` to extract language and code from posts separately\n",
    "\n",
    "Note that what we really need are character level embeddings for code... but we'll start with word embeddings.\n",
    "\n",
    "Once we remove infrequent words we need to filter empty posts and create a new set of labels matching the indexes of the unfiltered posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "MIN_CODE = 20\n",
    "\n",
    "def extract_code(x):\n",
    "    doc = BeautifulSoup(x)\n",
    "    return '\\n'.join([d.text for d in doc.find_all('code')])\n",
    "    \n",
    "post_code_text = tag_posts._Body.apply(extract_code)\n",
    "post_code_words = [x.split() for x in post_code_text.tolist()]\n",
    "\n",
    "# Count the words for testing\n",
    "code_word_counts = defaultdict(int)\n",
    "for post in post_code_words:\n",
    "    for word in post:\n",
    "        code_word_counts[word] += 1\n",
    "\n",
    "# Take words with > MIN_CODE (20) instances\n",
    "post_code_words = [[y for y in x if code_word_counts[y] > MIN_CODE] for x in post_code_words]\n",
    "\n",
    "# Create a new list of labels to match the new non-empty lists of words\n",
    "code_post_ids = defaultdict(bool)\n",
    "filtered_code_words = []\n",
    "for i, post in enumerate(post_code_words):\n",
    "    if len(post) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        code_post_ids[i] = True\n",
    "        filtered_code_words.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "for post_id in code_post_ids.keys():\n",
    "    label_set = labels[post_id]\n",
    "    if code_post_ids[post_id]:\n",
    "        new_labels.append(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 71,803 example posts\n"
     ]
    }
   ],
   "source": [
    "# Validate the posts match the labels\n",
    "assert(len(filtered_code_words) == len(new_labels))\n",
    "print('We are left with {:,} example posts'.format(len(filtered_code_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMINDER: When we add text words we must combine the two valid label lists and then create a new list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_TEXT = 20\n",
    "\n",
    "# def extract_text(x):\n",
    "#     doc = BeautifulSoup(x)\n",
    "#     codes = doc.find_all('code')\n",
    "#     [code.extract() if code else None for code in codes]\n",
    "#     return doc.text\n",
    "\n",
    "# post_text = tag_posts._Body.apply(extract_text)\n",
    "# post_text_words = [x.split() for x in post_text.tolist()]\n",
    "\n",
    "# # Take words with > MIN_TEXT (20) instances\n",
    "# post_text_words = [[y for y in x if tag_counts[y] > MIN_TEXT] for x in post_text_words]\n",
    "\n",
    "# # Create a new list of labels to match the new non-empty lists of words\n",
    "# text_post_ids = defaultdict(bool)\n",
    "# text_post_id_list = []\n",
    "# for i, post in enumerate(post_text_words):\n",
    "#     if len(post) == 0:\n",
    "#         pass\n",
    "#     else:\n",
    "#         text_post_ids[i] = True\n",
    "#         text_post_id_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the tags, replacing their string form with their respective IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_tags = []\n",
    "# raw_tags = []\n",
    "# for tagset in coded_tags:\n",
    "#    encoded_tags.append([1 if id in tagset else 0 for id in id_to_tag.keys()])\n",
    "\n",
    "# labels = np.array(encoded_tags)\n",
    "\n",
    "# encoded_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Baseline Gradient Boosted Decision Tree Model\n",
    "\n",
    "It is useful to have a decision tree model to use as a baseline for comparison with our deep network model. XGBoost's implementation of gradient boosted decision trees is state of the art for this kind of application, but it can't do multi-class, multi-label classification. Therefore we use an [`xgboost.XGBClassifier`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier) with an [`sklearn.multiclass.OneVsRestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) on top of the XGBoost classifier to train one classifier model per label and then apply them to each label to compute the output for each.\n",
    "\n",
    "We define `VOCAB_SIZE`, `MAX_LENGTH` and `TEST_SPLIT` to define the number of unique words as input into our embedding, the sequence length for each input, and the test/train split for our performance testing.\n",
    "\n",
    "#### Encode the data using Gensim and Word2Vec\n",
    "\n",
    "For the network, we'll create our own embeddings. For the baseline model we'll use Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    filtered_code_words,\n",
    "    size=100,\n",
    "    min_count=1,\n",
    "    window=10,\n",
    "    workers=10,\n",
    "    iter=10,\n",
    "    seed=33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caught', 0.6866112947463989),\n",
       " ('threw', 0.6854922771453857),\n",
       " ('exception:', 0.6644082069396973),\n",
       " ('thrown', 0.6516616344451904),\n",
       " ('nested', 0.6475620269775391),\n",
       " ('uncaught', 0.6054445505142212),\n",
       " ('An', 0.5725916028022766),\n",
       " ('occurred', 0.568584144115448),\n",
       " ('occurred:', 0.5625081062316895),\n",
       " ('HRESULT:', 0.5587760210037231)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive='exception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_posts = [[w2v_model.wv[word] for word in post] for post in filtered_code_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_posts), len(encoded_posts[0]), len(encoded_posts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad and limit the posts to MAX_LENGTH (20) words using the average of all words in the corpus\n",
    "\n",
    "We will now compute a position-wise maximum and minimum, concatenate these values, and use them to pad any documents with less than 20 words. We will simultaneously truncate any documents with more than 20 words. If we were creating our own embeddings using keras we would use [`keras.preprocessing.pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences), but using [`gensim.models.word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) we pad them on our own.\n",
    "\n",
    "See [Representation learning for very short texts using weighted word embedding aggregation](https://arxiv.org/pdf/1607.00570.pdf) referenced from [Stack Overflow](https://datascience.stackexchange.com/a/17348/59975)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "padded_posts = []\n",
    "for post in encoded_posts:\n",
    "    short = 0\n",
    "    # Pad short posts with alternating min/max\n",
    "    if len(post) < MAX_LENGTH:\n",
    "        pointwise_min = np.minimum.reduce(post)\n",
    "        pointwise_max = np.maximum.reduce(post)\n",
    "        padding = [pointwise_max, pointwise_min]\n",
    "        \n",
    "        post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
    "        short = 1\n",
    "        \n",
    "    # Shorten long posts or those odd number length posts we padded to 51\n",
    "    if len(post) > MAX_LENGTH:\n",
    "        post = post[:50]\n",
    "      \n",
    "    padded_posts.append(post)\n",
    "\n",
    "# Verify their lengths\n",
    "assert(max([len(post) for post in padded_posts]) == 50)\n",
    "assert(min([len(post) for post in padded_posts]) == 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the 3D feature array into a wider 2D array\n",
    "\n",
    "The classifier requires 2D data, so we need to convert our 3D feature array into a wider 2D feature array. We will do this by iterating through the 50 padded elements of Word2Vec vectors for each post and appending them to a long list for each post.\n",
    "\n",
    "Note that the type of `padded_posts` is `list(list(np.array))`, an artifact of the Word2Vec mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.array(encoded_posts),\n",
    "    np.array(new_labels),\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input numpy.ndarray must be 2 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-4b7c1d4cb6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md_test\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m params = {\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'booster'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'gbtree'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/deep/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/deep/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing, nthread)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input numpy.ndarray must be 2 dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;31m# flatten the array by rows and ensure it is float32.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input numpy.ndarray must be 2 dimensional"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test =  xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'silent': 0,\n",
    "}\n",
    "\n",
    "clf = OneVsRestClassifier(\n",
    "    xgb.XGBClassifier(\n",
    "        learning_rate=0.2, n_estimators=600, objective='binary:logistic', nthread=2\n",
    "    ), \n",
    "    n_jobs=20,\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(clf, X_train, y_train, cv=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=False, target_names=tag_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, \n",
    "    precision_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "xgb_params = {\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, lower=True)\n",
    "tokenizer.fit_on_texts(post_code)\n",
    "sequences = tokenizer.texts_to_sequences(post_code)\n",
    "X = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "X.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=MAX_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep] *",
   "language": "python",
   "name": "conda-env-deep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
