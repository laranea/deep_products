{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Baseline Tag Labeler\n",
    "\n",
    "Here we will use [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html) and [scikit-learn](https://scikit-learn.org/stable/) to create a baseline multi-class, multi-label classifier that will label our sample of [Stack Overflow](http://stackoverflow.com) posts (questions and their answers), two thirds of which lack labels. This will serve as a basis of comparison for the deep network we will train therafter. We will create separate [embeddings](https://keras.io/layers/embeddings/) of their language and code and use these as the signal for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm working!\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"I'm working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our sample of questions/answers with at least 1 vote and 1 answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_tags = json.load(open('data/stackoverflow/08-05-2019/sorted_all_tags.50000.json'))\n",
    "max_index = sorted_all_tags[-1][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>...</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>label_20</th>\n",
       "      <th>label_21</th>\n",
       "      <th>label_22</th>\n",
       "      <th>label_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C, Mono, Winforms, MessageBox, problem, I, fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Are, NET, data, providers, Oracle, require, O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[How, I, focus, foreign, window, I, applicatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Default, button, hit, windows, forms, trying,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Can, I, avoid, JIT, net, Say, code, always, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  label_0  label_1  \\\n",
       "0  [C, Mono, Winforms, MessageBox, problem, I, fi...        1        0   \n",
       "1  [Are, NET, data, providers, Oracle, require, O...        1        0   \n",
       "2  [How, I, focus, foreign, window, I, applicatio...        1        0   \n",
       "3  [Default, button, hit, windows, forms, trying,...        1        0   \n",
       "4  [Can, I, avoid, JIT, net, Say, code, always, g...        1        0   \n",
       "\n",
       "   label_2  label_3  label_4  label_5  label_6  label_7  label_8  ...  \\\n",
       "0        0        0        0        1        0        0        0  ...   \n",
       "1        0        0        0        0        0        0        0  ...   \n",
       "2        0        0        0        1        0        0        0  ...   \n",
       "3        0        0        0        1        0        0        0  ...   \n",
       "4        0        0        0        0        0        0        0  ...   \n",
       "\n",
       "   label_14  label_15  label_16  label_17  label_18  label_19  label_20  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   label_21  label_22  label_23  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         0         0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "posts_df = pd.read_parquet(\n",
    "    'data/stackoverflow/08-05-2019/Questions.Stratified.Final.50000.parquet',\n",
    "    columns=['_Body'] + ['label_{}'.format(i) for i in range(0, max_index)],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,293,018 answered questions with at least 1 upvote\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '{:,} answered questions with at least 1 upvote'.format(len(posts_df.index))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from Tags to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = json.load(open('data/stackoverflow/08-05-2019/tag_index.50000.json'))\n",
    "index_tag = json.load(open('data/stackoverflow/08-05-2019/index_tag.50000.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = json.load(open('data/stackoverflow/08-05-2019/label_counts.50000.json'))\n",
    "\n",
    "# Sanity check the difference files\n",
    "assert(len(label_counts.keys()) == len(tag_index.keys()) == len(index_tag.keys()) == len(sorted_all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Be Consistent: Make Record Count a Multiple of the Batch Size and Post Sequence Length\n",
    "\n",
    "Although it is not necessary in our baseline labeler, the Elmo embedding in the network model requires that the number of records be a multiple of the batch size times the number of tokens in the padded posts. We do the same thing to keep the data consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Factor: 404 Training Count: 1,292,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print('Highest Factor: {:,} Training Count: {:,}'.format(highest_factor, training_count))\n",
    "\n",
    "# Remove stopwords - now done in Spark, so can remove once that runs\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(words)\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample the data to speed development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(33)\n",
    "\n",
    "# SAMPLE_SIZE = 10000\n",
    "# id_list = list(range(0, len(filtered_code_words)))\n",
    "# idx = random.sample(id_list, SAMPLE_SIZE)\n",
    "\n",
    "# # idx = np.random.choice(np.arange(len(matrix_posts)), SAMPLE_SIZE, replace=False)\n",
    "\n",
    "# sampled_posts = [x for i, x in enumerate(filtered_code_words) if i in idx]\n",
    "# sampled_labels = [x for i, x in enumerate(new_labels) if i in idx]\n",
    "\n",
    "# del filtered_code_words\n",
    "# del new_labels\n",
    "\n",
    "# len(sampled_posts), len(sampled_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMINDER: When we add text words we must combine the two valid label lists and then create a new list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_TEXT = 20\n",
    "\n",
    "# def extract_text(x):\n",
    "#     doc = BeautifulSoup(x)\n",
    "#     codes = doc.find_all('code')\n",
    "#     [code.extract() if code else None for code in codes]\n",
    "#     return doc.text\n",
    "\n",
    "# post_text = tag_posts._Body.apply(extract_text)\n",
    "# post_text_words = [x.split() for x in post_text.tolist()]\n",
    "\n",
    "# # Take words with > MIN_TEXT (20) instances\n",
    "# post_text_words = [[y for y in x if tag_counts[y] > MIN_TEXT] for x in post_text_words]\n",
    "\n",
    "# # Create a new list of labels to match the new non-empty lists of words\n",
    "# text_post_ids = defaultdict(bool)\n",
    "# text_post_id_list = []\n",
    "# for i, post in enumerate(post_text_words):\n",
    "#     if len(post) == 0:\n",
    "#         pass\n",
    "#     else:\n",
    "#         text_post_ids[i] = True\n",
    "#         text_post_id_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the tags, replacing their string form with their respective IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_tags = []\n",
    "# raw_tags = []\n",
    "# for tagset in coded_tags:\n",
    "#    encoded_tags.append([1 if id in tagset else 0 for id in id_to_tag.keys()])\n",
    "\n",
    "# labels = np.array(encoded_tags)\n",
    "\n",
    "# encoded_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Baseline Gradient Boosted Decision Tree Model\n",
    "\n",
    "It is useful to have a decision tree model to use as a baseline for comparison with our deep network model. XGBoost's implementation of gradient boosted decision trees is state of the art for this kind of application, but it can't do multi-class, multi-label classification. Therefore we use an [`xgboost.XGBClassifier`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier) with an [`sklearn.multiclass.OneVsRestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) on top of the XGBoost classifier to train one classifier model per label and then apply them to each label to compute the output for each.\n",
    "\n",
    "We define `VOCAB_SIZE`, `MAX_LENGTH` and `TEST_SPLIT` to define the number of unique words as input into our embedding, the sequence length for each input, and the test/train split for our performance testing.\n",
    "\n",
    "#### Encode the data using Gensim and Word2Vec\n",
    "\n",
    "For the network, we'll create our own embeddings. For the baseline model we'll use Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model built!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LENGTH = 100\n",
    "EMBEDDING_SIZE = 50\n",
    "NUM_CORES = 12\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "from os import path\n",
    "\n",
    "w2v_model  = None\n",
    "model_path = \"data/stackoverflow/08-05-2019/word2vec.50000.model\"\n",
    "\n",
    "# Load the Word2Vec model if it exists\n",
    "if path.exists(model_path):\n",
    "    w2v_model = Word2Vec.load(model_path)\n",
    "else:\n",
    "    w2v_model = Word2Vec(\n",
    "        documents,\n",
    "        size=EMBEDDING_SIZE,\n",
    "        min_count=1,\n",
    "        window=10,\n",
    "        workers=NUM_CORES,\n",
    "        iter=10,\n",
    "        seed=33\n",
    "    )\n",
    "    w2v_model.save(model_path)\n",
    "\n",
    "print('Word2Vec model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('program,', 0.8130234479904175),\n",
       " ('programm', 0.7869037985801697),\n",
       " ('programme', 0.77663254737854),\n",
       " ('script', 0.7494806051254272),\n",
       " ('process', 0.7310086488723755),\n",
       " ('programs', 0.705868661403656),\n",
       " ('code', 0.6764416694641113),\n",
       " ('routine', 0.6757915019989014),\n",
       " ('computer', 0.6742575168609619),\n",
       " ('program.', 0.6669228076934814)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive='program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_docs = [[w2v_model.wv[word] for word in post] for post in documents]\n",
    "len(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292800,\n",
       " 100,\n",
       " array([ 5.487012  , -5.1054535 , -1.8988078 , -7.685227  ,  4.207665  ,\n",
       "        -1.1219728 ,  1.1265227 , -0.25528577, -9.583929  , -1.1784164 ,\n",
       "        -6.200084  ,  2.2063937 ,  0.5968637 ,  0.12365432, -4.4148273 ,\n",
       "         8.716778  ,  6.9193225 ,  2.5406606 , -3.1150146 ,  3.4230134 ,\n",
       "         5.548197  , -6.9893117 ,  1.3929974 , -3.8196619 ,  3.193098  ,\n",
       "         8.028727  ,  6.06637   ,  6.2713885 , 12.862909  , -0.11642915,\n",
       "         1.8495387 ,  6.1364207 , -4.3824267 , -2.5337682 ,  2.9778636 ,\n",
       "         5.652717  ,  6.90736   , -0.9474404 , -1.989368  , -2.7301998 ,\n",
       "        -4.755108  , -0.313335  ,  2.453273  ,  6.1209016 ,  0.86407304,\n",
       "         1.9723939 , -3.104216  , -5.3178406 ,  4.0534744 ,  2.2914498 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_docs), len(encoded_docs[0]), encoded_docs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad and limit the posts to MAX_LENGTH (100) words using the average of all words in the corpus\n",
    "\n",
    "We will now compute a position-wise maximum and minimum, concatenate these values, and use them to pad any documents with less than 20 words. We will simultaneously truncate any documents with more than 20 words. If we were creating our own embeddings using keras we would use [`keras.preprocessing.pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences), but using [`gensim.models.word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) we pad them on our own.\n",
    "\n",
    "See [Representation learning for very short texts using weighted word embedding aggregation](https://arxiv.org/pdf/1607.00570.pdf) referenced from [Stack Overflow](https://datascience.stackexchange.com/a/17348/59975)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292800,\n",
       " 100,\n",
       " array([ 5.487012  , -5.1054535 , -1.8988078 , -7.685227  ,  4.207665  ,\n",
       "        -1.1219728 ,  1.1265227 , -0.25528577, -9.583929  , -1.1784164 ,\n",
       "        -6.200084  ,  2.2063937 ,  0.5968637 ,  0.12365432, -4.4148273 ,\n",
       "         8.716778  ,  6.9193225 ,  2.5406606 , -3.1150146 ,  3.4230134 ,\n",
       "         5.548197  , -6.9893117 ,  1.3929974 , -3.8196619 ,  3.193098  ,\n",
       "         8.028727  ,  6.06637   ,  6.2713885 , 12.862909  , -0.11642915,\n",
       "         1.8495387 ,  6.1364207 , -4.3824267 , -2.5337682 ,  2.9778636 ,\n",
       "         5.652717  ,  6.90736   , -0.9474404 , -1.989368  , -2.7301998 ,\n",
       "        -4.755108  , -0.313335  ,  2.453273  ,  6.1209016 ,  0.86407304,\n",
       "         1.9723939 , -3.104216  , -5.3178406 ,  4.0534744 ,  2.2914498 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "padded_posts = []\n",
    "for post in encoded_docs:\n",
    "    # Pad short posts with alternating min/max\n",
    "    if len(post) < MAX_LENGTH:\n",
    "        pointwise_min = np.minimum.reduce(post)\n",
    "        pointwise_max = np.maximum.reduce(post)\n",
    "        padding = [pointwise_max, pointwise_min]\n",
    "        \n",
    "        post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
    "        \n",
    "    # Shorten long posts or those odd number length posts we padded to 51\n",
    "    if len(post) > MAX_LENGTH:\n",
    "        post = post[:MAX_LENGTH]\n",
    "      \n",
    "    padded_posts.append(post)\n",
    "\n",
    "# Verify their lengths\n",
    "assert(min([len(post) for post in padded_posts]) == MAX_LENGTH)\n",
    "assert(max([len(post) for post in padded_posts]) == MAX_LENGTH)\n",
    "\n",
    "# Free up the RAM, since we copied the data\n",
    "# del encoded_docs\n",
    "len(padded_posts), len(padded_posts[0]), padded_posts[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the 3D feature array into a wider 2D array\n",
    "\n",
    "The classifier requires 2D data, so we need to convert our 3D feature array into a wider 2D feature array. We will do this by iterating through the 50 padded elements of Word2Vec vectors for each post and appending them to a long list for each post.\n",
    "\n",
    "Note that the type of `padded_posts` is `list(list(np.array))`, an artifact of the Word2Vec mapping.\n",
    "\n",
    "#### Create one Row per Label Column\n",
    "\n",
    "Training a `sklearn.multiclass.OneVsRestClassifier` with one `xgboost.XGBClassifier` per label exceeded 64GB of RAM and so we are remapping the data to have one instance of the row for each label column in a given row.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "# Input\n",
    "rows, labels = [0.1, 0.3, 0.4, ...],[0,1,0,1]\n",
    "\n",
    "# Output\n",
    "rows_w_labels = [\n",
    "    ([0.1, 0.3, 0.4, ...], 0),\n",
    "    ([0.1, 0.3, 0.4, ...], 1),\n",
    "    ([0.1, 0.3, 0.4, ...], 0),\n",
    "    ([0.1, 0.3, 0.4, ...], 1)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,292,800 padded_posts, (1292800, 24) labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "out of memory to allocate 20480 bytes (total 89846784 bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-99e27ed6669e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpost_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_row.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.7/site-packages/cupy/manipulation/join.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tup, axis)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_manipulation.concatenate_method\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_manipulation.concatenate_method\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_manipulation._concatenate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_manipulation._concatenate_single_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: out of memory to allocate 20480 bytes (total 89846784 bytes)"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import sys\n",
    "\n",
    "# cupy memory management\n",
    "mempool = cp.get_default_memory_pool()\n",
    "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "\n",
    "row_length = MAX_LENGTH * EMBEDDING_SIZE\n",
    "\n",
    "matrix_posts = []\n",
    "flat_labels = []\n",
    "print_shape = False\n",
    "print_dot = True\n",
    "\n",
    "print(f'{len(padded_posts):,} padded_posts, {labels.shape} labels')\n",
    "\n",
    "cpu_posts = []\n",
    "five_k_count = 0\n",
    "for i, post in enumerate(padded_posts):\n",
    "    # Starting with an empty array and append the entire list of embedded words to it, \n",
    "    # expanding it's shape to (5000,)\n",
    "    post = cp.array(post)\n",
    "    if print_shape and i % 5000 == 0:\n",
    "        print('post.shape', post.shape)\n",
    "    \n",
    "    post_row = cp.concatenate(post, axis=0)\n",
    "    if print_shape and i % 5000 == 0:\n",
    "        print('post_row.shape', post_row.shape)\n",
    "    \n",
    "    assert(post_row.shape == (row_length,))\n",
    "    \n",
    "    # Now add a downward dimension to the data, expanding its dimensions to (1,5000)\n",
    "    post_row = cp.expand_dims(post_row, axis=0)\n",
    "    if print_shape and i % 5000 == 0:\n",
    "        print('post_row.shape', post_row.shape)\n",
    "    \n",
    "    assert(post_row.shape == (1,row_length))\n",
    "    \n",
    "    if print_shape and i % 5000 == 0:\n",
    "        print(len(post_row[0]))\n",
    "    \n",
    "    matrix_posts.append(post_row)\n",
    "    \n",
    "    if print_dot and i % 5000 == 0:\n",
    "        sys.stderr.write('.')\n",
    "    \n",
    "    # Every 25 * 5000 rows, send GPU RAM to main RAM or it will crash with 7618GB cards (g3.4xlarge)\n",
    "    if i % 5000 == 0:\n",
    "        five_k_count += 1\n",
    "        first_five_k = True\n",
    "        \n",
    "    if five_k_count % 25 == 0 and first_five_k is True:\n",
    "        print(f'\\ni:{i:,} 5k:{five_k_count:,} GPU RAM --> CPU RAM')\n",
    "        cpu_posts.append(\n",
    "            cp.asnumpy(cp.concatenate(matrix_posts, axis=0))\n",
    "        )\n",
    "        del matrix_posts\n",
    "        matrix_posts = []\n",
    "        mempool.free_all_blocks()\n",
    "        pinned_mempool.free_all_blocks()\n",
    "        print('Completed!')\n",
    "        \n",
    "        first_five_k = False\n",
    "\n",
    "# Do the final concat for the remainder over 5K and clear GPU RAM\n",
    "cpu_posts.append(\n",
    "    cp.asnumpy(cp.concatenate(matrix_posts, axis=0))\n",
    ")\n",
    "mempool.free_all_blocks()\n",
    "pinned_mempool.free_all_blocks()\n",
    "\n",
    "# Memory conservation is critical\n",
    "# del padded_posts\n",
    "del matrix_posts\n",
    "len(cpu_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_posts = np.concatenate(cpu_posts, axis=0)\n",
    "cpu_posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample the Data Once Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_SIZE = 1000\n",
    "# id_list = list(range(0, len(matrix_posts)))\n",
    "# idx = random.sample(id_list, SAMPLE_SIZE)\n",
    "# sampled_posts = [post for i, post in enumerate(matrix_posts) if i in idx]\n",
    "# sampled_labels = [label for i, label in enumerate(flat_labels) if i in idx]\n",
    "\n",
    "# del matrix_posts\n",
    "# del flat_labels\n",
    "# len(sampled_posts), len(sampled_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from GPU `cupy.ndarray` to main memory `numpy.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cpu_posts,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "# del matrix_posts\n",
    "# del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train_sample_weights = compute_sample_weight('balanced', y_train).tolist()\n",
    "test_sample_weights = compute_sample_weight('balanced', y_test).tolist()\n",
    "\n",
    "train_sample_weights, test_sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "test_weight_vec = list(np.max(np.sum(y_test, axis=0))/np.sum(y_test, axis=0))\n",
    "test_class_weights = {i: test_weight_vec[i] for i in range(y_test.shape[1])}\n",
    "\n",
    "# sorted(list(train_class_weights.items()), key=lambda x: x[1]), sorted(list(test_class_weights.items()), key=lambda x: x[1])\n",
    "train_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     'balanced',\n",
    "#      np.unique(y_train.tolist()),\n",
    "#      y_train.tolist()\n",
    "# )\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_leaf=4,\n",
    "    max_depth=10,\n",
    "    n_jobs=12,\n",
    "    class_weight='balanced',\n",
    "    random_state=1337,\n",
    "    verbose=1,\n",
    ")\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train, sample_weight=train_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse\n",
    "\n",
    "# X_train = sparse.csr_matrix(X_train)\n",
    "\n",
    "# params = {\n",
    "#     'booster': 'gbtree',\n",
    "#     'silent': 0,\n",
    "# }\n",
    "\n",
    "# clf = OneVsRestClassifier(\n",
    "#     xgb.XGBClassifier(\n",
    "#         learning_rate=0.2,\n",
    "#         n_estimators=100,\n",
    "#         objective='binary:logistic',\n",
    "#         nthread=1,\n",
    "#         tree_method='gpu_hist'\n",
    "#     ), \n",
    "#     n_jobs=1,\n",
    "# )\n",
    "# %timeit clf.fit(X_train[:200000], y_train[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = RandomForestClassifier(\n",
    "#     n_estimators=100,\n",
    "#     max_depth=3,\n",
    "#     random_state=33,\n",
    "#     n_jobs=12\n",
    "# )\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo 'Fitting done!' | ~/bin/twilio-sms 404-317-3620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "# d_test =  xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_score(clf, X_train, y_train, cv=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \n",
    "    accuracy_score, jaccard_score, precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "tag_labels = [tag[1] for tag in sorted_all_tags]\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# report = classification_report(y_test, y_pred, output_dict=True, target_names=tag_labels)\n",
    "y_pred.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "jaccard_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "xgb_params = {\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, lower=True)\n",
    "tokenizer.fit_on_texts(post_code)\n",
    "sequences = tokenizer.texts_to_sequences(post_code)\n",
    "X = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "X.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=MAX_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
